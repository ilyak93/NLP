{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "deletable": false, "editable": false, "jupyter": {"outputs_hidden": true, "source_hidden": true}}, "outputs": [], "source": ["# Please do not change this cell because some hidden tests might depend on it.\n", "import os\n", "\n", "# Otter grader does not handle ! commands well, so we define and use our\n", "# own function to execute shell commands.\n", "def shell(commands, warn=True):\n", "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n", "     \n", "       Prints the result to stdout and returns the exit status. \n", "       Provides a printed warning on non-zero exit status unless `warn` \n", "       flag is unset.\n", "    \"\"\"\n", "    file = os.popen(commands)\n", "    print (file.read().rstrip('\\n'))\n", "    exit_status = file.close()\n", "    if warn and exit_status != None:\n", "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n", "    return exit_status\n", "\n", "shell(\"\"\"\n", "ls requirements.txt >/dev/null 2>&1\n", "if [ ! $? = 0 ]; then\n", " rm -rf .tmp\n", " git clone https://github.com/cs236299-2020/project2.git .tmp\n", " mv .tmp/requirements.txt ./\n", " rm -rf .tmp\n", "fi\n", "pip install -q -r requirements.txt\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook()"]}, {"cell_type": "raw", "metadata": {}, "source": ["%%latex\n", "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\newcommand{\\softmax}{\\operatorname{softmax}}\n", "\\newcommand{\\Prob}{\\Pr}\n", "\\newcommand{\\given}{\\,|\\,}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["$$\n", "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n", "\\renewcommand{\\Prob}{\\Pr}\n", "\\renewcommand{\\given}{\\,|\\,}\n", "$$"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "zXQlsJiAboxU"}, "source": ["# Project 2: Sequence labeling \u2013 The slot filling task\n", "\n", "The second segment of the project involves a sequence labeling task, in which the goal is to label the tokens in a text. Many NLP tasks have this general form. Most famously is the task of _part-of-speech labeling_ as you explored in lab 2-4, where the tokens in a text are to be labeled with their part of speech (noun, verb, preposition, etc.). In this project segment, however, you'll use sequence labeling to implement a system for filling the slots in a template that is intended to describe the meaning of an ATIS query. For instance, the sentence \n", "\n", "    What's the earliest arriving flight between Boston and Washington DC?\n", "    \n", "might be associated with the following slot-filled template: \n", "\n", "    flight_id\n", "        fromloc.cityname: boston\n", "        toloc.cityname: washington\n", "        toloc.state: dc\n", "        flight_mod: earliest arriving\n", "    \n", "You may wonder how this task is a sequence labeling task. We label each word in the source sentence with a tag taken from a set of tags that correspond to the slot-labels. For each slot-label, say `flight_mod`, there are two tags: `B-flight_mod` and `I-flight_mod`. These are used to mark the beginning (B) or interior (I) of a phrase that fills the given slot. In addition, there is a tag for other (O) words that are not used to fill any slot. (This technique is thus known as IOB encoding.) Thus the sample sentence would be labeled as follows:\n", "\n", "| Token   | Label    |\n", "| ------- | ------ | \n", "| BOS | O |\n", "| what's | O |\n", "| the | O |\n", "| earliest | B-flight_mod |\n", "| arriving | I-flight_mod |\n", "| flight | O |\n", "| between | O |\n", "| boston | B-fromloc.city_name |\n", "| and | O |\n", "| washington | B-toloc.city_name |\n", "| dc | B-toloc.state_code |\n", "| EOS | O |\n", "\n", "> See below for information about the `BOS` and `EOS` tokens. \n", "\n", "The template itself is associated with the question type for the sentence, perhaps as recovered from the sentence in the last project segment.\n", "\n", "In this segment, you'll implement two methods for sequence labeling: a hidden Markov model (HMM) and two recurrent neural networks, a simple RNN and a long short-term memory network (LSTM). By the end of this homework, you should have grasped the pros and cons of both approaches.\n", "\n", "## Goals\n", "\n", "1. Implement an HMM-based approach to sequence labeling.\n", "2. Implement an RNN-based approach to sequence labeling.\n", "3. Implement an LSTM-based approach to sequence labeling.\n", "4. (Optional) Compare the performances of HMM and RNN/LSTM under different amount of training data. Discuss the pros and cons of the HMM approach and the neural approach."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "L3MDY8fqdYQg"}, "source": ["## Setup"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "mgumU2YoUtQn"}, "outputs": [], "source": ["import copy\n", "import math\n", "import matplotlib.pyplot as plt\n", "import random\n", "import warnings\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torchtext as tt\n", "\n", "from tqdm import tqdm\n", "\n", "# Set random seeds\n", "seed = 1234\n", "random.seed(seed)\n", "torch.manual_seed(seed)\n", "\n", "# GPU check, sets runtime type to \"GPU\" where available\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(device)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "RN3cNFRmvRgS"}, "source": ["### Load Data\n", "\n", "First, we download the ATIS dataset, already presplit into training, validation (dev), and test sets."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "tr1l3mMyfZ4u"}, "outputs": [], "source": ["shell(\"\"\"\n", "  wget -nv -N -P data https://raw.githubusercontent.com/nlp-236299/data/master/ATIS/atis.train.txt\n", "  wget -nv -N -P data https://raw.githubusercontent.com/nlp-236299/data/master/ATIS/atis.dev.txt\n", "  wget -nv -N -P data https://raw.githubusercontent.com/nlp-236299/data/master/ATIS/atis.test.txt\n", "\"\"\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "PvzFJtVlf7R-"}, "source": ["### Data preprocessing\n", "\n", "We again use `torchtext` to load data and convert words to indices in the vocabulary. We use one field `TEXT` for processing the question, and another field `TAG` for processing the sequence labels.\n", "\n", "We treat words occurring fewer than three times in the training data as _unknown words_. They'll be replaced by the unknown word type `<unk>`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## Turn off annoying torchtext warnings about pending deprecations\n", "warnings.filterwarnings(\"ignore\", module=\"torchtext\", category=UserWarning)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "RHSSr75vgFR4"}, "outputs": [], "source": ["MIN_FREQ = 3\n", "\n", "TEXT = tt.data.Field(init_token=\"<bos>\")\n", "TAG = tt.data.Field(init_token=\"<bos>\")\n", "fields=(('text', TEXT), ('tag', TAG))\n", "\n", "train, val, test = tt.datasets.SequenceTaggingDataset.splits(\n", "  fields=fields, \n", "  path='./data/', \n", "  train='atis.train.txt',\n", "  validation='atis.dev.txt',\n", "  test='atis.test.txt'\n", ")\n", "\n", "TEXT.build_vocab(train.text, min_freq=MIN_FREQ)\n", "TAG.build_vocab(train.tag)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can get some sense of the datasets by looking at the size and some elements of the text and tag vocabularies."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "RHSSr75vgFR4"}, "outputs": [], "source": ["print(f\"Size of English vocabulary: {len(TEXT.vocab)}\")\n", "print(f\"Most common English words: {TEXT.vocab.freqs.most_common(10)}\\n\")\n", "\n", "print(f\"Number of tags: {len(TAG.vocab)}\")\n", "print(f\"Most common tags: {TAG.vocab.freqs.most_common(10)}\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Wb5BbrHf86SO"}, "source": ["### Special tokens and tags\n", "\n", "You'll have already noticed the `BOS` and `EOS`, special tokens that the dataset developers used to indicate the beginning and end of the sentence; we'll leave them in the data.\n", "\n", "We've also passed in `init_token=\"<bos>\"` for both torchtext fields. Torchtext will prepend these to the sequence of words and tags. This relieves us from estimating the initial distribution of tags and tokens in HMMs, since we always start with a token `<bos>` whose tag is also `<bos>`. We'll be able to refer to these tags as exemplified here:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "HEuhnBXE8Xrf"}, "outputs": [], "source": ["print(f\"\"\"\n", "Initial tag string: {TAG.init_token}\n", "Initial tag id:     {TAG.vocab.stoi[TAG.init_token]}\n", "\"\"\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Wb5BbrHf86SO"}, "source": ["Finally, since torchtext will be providing the sentences in the training corpus in \"batches\", torchtext will force the sentences within a batch to be the same length by padding them with a special token. Again, we can access that token as shown here:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "HEuhnBXE8Xrf"}, "outputs": [], "source": ["print(f\"\"\"\n", "Pad tag string: {TAG.pad_token}\n", "Pad tag id:     {TAG.vocab.stoi[TAG.pad_token]}\n", "\"\"\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "BbJ1HfJWgHJk"}, "source": ["Now, we can iterate over the dataset using `torchtext`'s iterator. We'll use a non-trivial batch size to gain the benefit of training on multiple sentences at a shot. This is different from how you've previously used torch, with a batch size of 1, and you'll need to be careful about the shapes of the various tensors that are being manipulated."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "MAIdJbAfgO4E"}, "outputs": [], "source": ["BATCH_SIZE = 20\n", "\n", "train_iter, val_iter, test_iter = tt.data.BucketIterator.splits(\n", "    (train, val, test), \n", "    batch_size=BATCH_SIZE, \n", "    repeat=False, \n", "    device=device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Each batch will be a tensor of size `max_length x batch_size`. Let's examine a batch."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "MAIdJbAfgO4E"}, "outputs": [], "source": ["# Get the first batch\n", "batch = next(iter(train_iter))\n", "\n", "# What's its shape? Should be max_length x batch_size.\n", "print(f'Shape of batch text tensor: {batch.text.shape}\\n')\n", "\n", "# Extract the first sentence in the batch, both text and tags\n", "first_sentence = batch.text[:, 0]\n", "first_tags = batch.tag[:, 0]\n", "\n", "# Print out the first sentence, as token ids and as text\n", "print(\"First sentence in batch\")\n", "print(f\"{first_sentence}\")\n", "print(f\"{' '.join([TEXT.vocab.itos[i] for i in first_sentence])}\\n\")\n", "\n", "print(\"First tags in batch\")\n", "print(f\"{first_tags}\")\n", "print(f\"{[TAG.vocab.itos[i] for i in first_tags]}\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "rrbmrJwiEszW"}, "source": ["The goal of this project is to predict the sequence of tags `batch.tag` given a sequence of words `batch.text`."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Majority class labeling\n", "\n", "As usual, we can get a sense of the difficulty of the task by looking at a simple baseline, tagging every token with the majority tag. Here's a table of tag frequencies for the most frequent tags:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tag_counts = torch.zeros(len(TAG.vocab.itos), device=device)\n", "\n", "for batch in train_iter:              # for each batch\n", "  for sent_id in range(len(batch)):   # ... each sentence in the batch\n", "    for tag in batch.tag[:, sent_id]: # ... each tag in the sentence\n", "      tag_counts[tag] += 1            # bump the tag count\n", "\n", "for tag_id in range(len(TAG.vocab.itos)):\n", "  print(f'{tag_id:3}  {TAG.vocab.itos[tag_id]:30}{tag_counts[tag_id].item():3.0f}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It looks like the `'O'` (other) tag is, unsurprisingly, the most frequent tag (except for the padding tag). The proportion of tokens labeled with that tag (ignoring the padding tag) gives us a good baseline accuracy for this sequence labeling task. If we were to just label every token with the `'O'` tag, we'd expect to be right on this proportion of the tokens."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["majority_baseline_accuracy = (\n", "  tag_counts[TAG.vocab.stoi['O']] \n", "  / (sum(tag_counts) \n", "     - tag_counts[TAG.vocab.stoi[TAG.pad_token]])\n", ")\n", "print(f'Baseline accuracy: {majority_baseline_accuracy:.3f}')"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "OECrLzLtgf5o"}, "source": ["## HMM for sequence labeling\n", "\n", "Having established the baseline to beat, we turn to implementing an HMM model.\n", "\n", "### Notation\n", "\n", "First, let's start with some notation. We use $Q = \\langle{Q_1, Q_2, \\ldots, Q_N} \\rangle$ to denote the possible tags, which is the state space of the HMM, and $\\mathcal{V} = \\langle v_1, v_2, \\ldots v_V \\rangle$ to denote the vocabulary of word types.  We use $q_{t}\\in Q$ to denote the state at time step $t$ (where $t$ varies from $1$ to $T$), and $o_t\\in \\mathcal{V}$ to denote the observation (word) at time step $t$."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "XZL1KkJ2iO0p"}, "source": ["### Training an HMM by counting\n", "\n", "Recall that an HMM is defined via a transition matrix $A$ which stores the probability of moving from one state $Q_i$ to another $Q_j$, that is, \n", "\n", "$$A_{ij}=P(q_{t+1}=Q_j  \\given  q_t=Q_i)$$\n", "\n", "and an emission matrix $B$ which stores the probability of generating word $\\mathcal{V}_j$ given state $Q_i$, that is, \n", "\n", "$$B_{ij}= P(o_t=\\mathcal{V}_j  \\given q_t= Q_i)$$\n", "\n", "> As is typical in notating probabilities, we'll use abbreviations\n", ">\n", "\\begin{align}\n", "P(q_{t+1} \\given  q_t) &\\equiv P(q_{t+1}=Q_j  \\given  q_t=Q_i) \\\\\n", "P(o_t  \\given q_t) &\\equiv P(o_t=\\mathcal{V}_j  \\given q_t= Q_i)\n", "\\end{align}\n", ">\n", "> where the $i$ and $j$ are clear from context.\n", "\n", "In our case, since the labels are observed in the training data, we can directly use counting to determine (maximum likelihood) estimates of $A$ and $B$."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "FZfPy07kg8Bn"}, "source": ["#### Goal 1(a): Find the transition matrix\n", "\n", "The matrix $A$ contains the transition probabilities: $A_{ij}$ is the probability of moving from state $Q_i$ to state $Q_j$ in the training data, so that $\\sum^{N}_{j = 1 } A_{ij} = 1$ for all $i$. \n", "\n", "We find these probabilities by counting the number of times state $Q_j$ appears right after state $Q_i$, as a proportion of all of the transitions from $Q_i$.\n", "\n", "$$\n", "A_{ij} = \\frac{\\cnt{Q_i, Q_j} + \\delta}{\\cnt{Q_i} + \\delta N}\n", "$$\n", "\n", "(In the above formula, we also used add-$\\delta$ smoothing.)\n", "\n", "Using the above definition, implement the method `train_A` in the `HMM` class, which calculates and returns the $A$ matrix as a tensor of size $N \\times N$.\n", "\n", "> You'll want to go ahead and implement this part now, and test it below, before moving on to the next goal.\n", "\n", "> Remember that the training data is being delivered to you batched."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "YKkywJsCY5PU"}, "source": ["#### Goal 1(b): Find the emission matrix $B$\n", "\n", "Similar to the transition matrix, the emission matrix contains the emission probabilities such that $B_{ij}$ is probability of word $o_t=\\mathcal{V}_j$ conditioned on state $q_t=Q_i$.\n", "\n", "We can find this by counting as well.\n", "$$\n", "B_{ij} = \\frac{\\cnt{Q_i, \\mathcal{V}_j} + \\delta}{\\cnt{Q_i} + \\delta V}\n", "$$\n", "\n", "Using the above definitions, implement the `train_B` method in the `HMM` class, which calculates and returns the $B$ matrix as a tensor of size $N \\times V$.\n", "\n", "> You'll want to go ahead and implement this part now, and test it below, before moving on to the next goal."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "O9M5u0CpjkI0"}, "source": ["### Sequence labeling with a trained HMM\n", "\n", "Now that you're able to train an HMM by estimating the transition matrix $A$ and the emission matrix $B$, you can apply it to the task of sequence labeling. Our goal is to find the most probable sequence of tags $\\hat q \\in Q^T$ given a sequence of words $o \\in \\mathcal{V}^T$.\n", "\n", "\\begin{align*}\n", "\\hat q &= \\operatorname*{argmax}\\limits_{q \\in Q^T}(P(q \\given o)) \\\\\n", "& = \\operatorname*{argmax}_{q \\in Q^T}(P(q,o)) \\\\\n", "& = \\operatorname*{argmax}_{q \\in Q^T}\\left(\\Pi^{T}_{t = 1} P(o_{t+1} \\given q_{t+1})P(q_{t+1} \\given q_t)\\right)\n", "\\end{align*}\n", "\n", "where $P(o_{t+1}=\\mathcal{V}_j \\given q_{t+1}=Q_i) = B_{ij}$, $P(q_{t+1}=Q_j \\given q_t=Q_{i})=A_{ij}$."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "INnW0jImbK7N"}, "source": ["#### Goal 1(c): Viterbi algorithm\n", "\n", "Implement the `predict` method, which should use the Viterbi algorithm to find the most likely sequence of tags for a sequence of `words`.\n", "\n", "> You'll want to go ahead and implement this part now, and test it below, before moving on to the next goal.\n", "\n", "> Warning: It may take up to 30 minutes to tag the entire test set depending on your implementation. We highly recommend that you begin by experimenting with your code using a _very small subset_ of the dataset, say two or three sentences, ramping up from there.\n", "\n", "> Hint: Consider how to use vectorized computations where possible for speed."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Evaluation\n", "\n", "We've provided you with the `evaluate` function, which takes a dataset iterator and uses `predict` on each sentence in each batch, comparing against the gold tags, to determine the accuracy of the model on the test set."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "PxMiTaNilc7K"}, "outputs": [], "source": ["class HMMTagger():\n", "  def __init__ (self, text, tag):\n", "    self.text = text\n", "    self.tag = tag\n", "    self.V = len(text.vocab.itos)    # vocabulary size\n", "    self.N = len(tag.vocab.itos)     # state space size\n", "    self.initial_state_id = tag.vocab.stoi[tag.init_token]\n", "    self.pad_state_id = tag.vocab.stoi[tag.pad_token]\n", "  \n", "  def train_A(self, iterator, delta):\n", "    \"\"\"Stores A for training dataset `iterator` using add-`delta` smoothing.\"\"\"\n", "    # Create A table\n", "    A = torch.zeros(self.N, self.N, device=device)\n", "\n", "    #TODO: Add your solution from Goal 1(a) here.\n", "    #      The returned value should be a tensor for the A matrix\n", "    #      of size N x N.\n", "    \n", "    ...\n", "    \n", "    return A\n", "\n", "  def train_B(self, iterator, delta):\n", "    \"\"\"Stores B for training dataset `iterator` using add-`delta` smoothing.\"\"\"\n", "    # Create B\n", "    B = torch.zeros(self.N, self.V, device=device)\n", "    \n", "    #TODO: Add your solution from Goal 1 (b) here.\n", "    #      The returned value should be a tensor for the $B$ matrix\n", "    #      of size N x V.\n", "    \n", "    ...\n", "\n", "    return B\n", "\n", "  def train_all(self, iterator, delta=0.01):\n", "    \"\"\"Stores A and B for training dataset `iterator`.\"\"\"\n", "    self.log_A = self.train_A(iterator, delta).log()\n", "    self.log_B = self.train_B(iterator, delta).log()\n", "    \n", "  def predict(self, words):\n", "    \"\"\"Returns the most likely sequence of tags for a sequence of `words`.\n", "    Arguments: \n", "      words: a tensor of size (seq_len,) \n", "    Returns:\n", "      a list of tag ids\n", "    \"\"\"\n", "    #TODO: Add your solution from Goal 1 (c) here.\n", "    #      The returned value should be a list of tag ids.\n", "\n", "    ...\n", "\n", "    return bestpath\n", "    \n", "  def evaluate(self, iterator):\n", "    \"\"\"Returns the model's performance on a given dataset `iterator`.\"\"\"\n", "    correct = 0\n", "    total = 0\n", "    for batch in tqdm(iterator):\n", "      for sent_id in range(len(batch)):\n", "        words = batch.text[:, sent_id]\n", "        tags_gold = batch.tag[:, sent_id]\n", "        tags_pred = self.predict(words)\n", "        for tag_gold, tag_pred in zip(tags_gold, tags_pred):\n", "          if tag_gold == self.pad_state_id:  # stop once we hit padding\n", "            break\n", "          else:\n", "            total += 1\n", "            if tag_pred == tag_gold:\n", "              correct += 1\n", "    return correct/total"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "O19h1vI0nTyF"}, "source": ["Putting everything together, you can expect a correct implementation to reach about **90% test set accuracy** after running the following cell."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "Tq4SqhvN8m1r"}, "outputs": [], "source": ["# Instantiate and train classifier\n", "hmm_tagger = HMMTagger(TEXT, TAG)\n", "hmm_tagger.train_all(train_iter)\n", "\n", "# Evaluate model performance\n", "print(f'Training accuracy: {hmm_tagger.evaluate(train_iter):.3f}\\n'\n", "      f'Test accuracy:     {hmm_tagger.evaluate(test_iter):.3f}')"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Qmi4Kl-woT9x"}, "source": ["### RNN for Sequence Labeling\n", "\n", "HMMs work quite well for this sequence labeling task. Now let's take an alternative (and more trendy) approach: RNN/LSTM-based sequence labeling. Similar to the HMM part of this project, you will also need to train a model on the training data, and then use the trained model to decode and evaluate some testing data.\n", "\n", "\n", "\n", "<img src=\"https://github.com/nlp-236299/data/raw/feature/project2figs/Resources/rnn-unfolded-figure.png\" width=600 align=right />\n", "\n", "After unfolding an RNN, the cell at time $t$ generates the observed output $o_t$ based on the input $x_t$ and the hidden state of the previous cell $h_{t-1}$, according to the following equations.\n", "\n", "\\begin{align*}\n", "h_t &=  \\sigma(U x_t + V h_{t-1}) \\\\\n", "o_t &= \\softmax(W h_t)\n", "\\end{align*}\n", "\n", "The parameters here are the elements of the matrices $U$, $V$, and $W$, and the bias terms $b_h$ and $b_y$ which we omitted for simplicity. Similar to the last project segment, we will perform the forward computation, calculate the loss, and then perform the backward computation to compute the gradients with respect to these model parameters. Finally, we will adjust the parameters opposite the direction of the gradients to minimize the loss, repeating until convergence.\n", "\n", "You've seen these kinds of neural network models before, for language modeling in lab 2-3 and sequence labeling in lab 2-5. The code there should be very helpful in implementing an `RNNTagger` class below. Consequently, we've provided very little guidance on the implementation. We do recommend you follow the steps below however."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "GcbFYlvWoaR_"}, "source": ["#### Goal 2(a): RNN training\n", "\n", "Implement the forward pass of the RNN tagger and the loss function. A reasonable way to proceed is to implement the following methods:\n", "\n", "1. `forward(self, text_batch, hidden_0)`: Performs the RNN forward computation over a whole `text_batch` (`batch.text` in the above data loading example) starting with the starting hidden state `hidden_0` of size, say, `1 x batch_size x hidden_size`. The `text_batch` will be of shape `max_length x batch_size`. You might run it through the following layers: an embedding layer, which maps each token index to an embedding of size `embedding_size` (so that the size of the mapped batch becomes `max_length x batch_size x embedding_size`); then an RNN, which maps each token embedding to a vector of `hidden_size` (the size of all outputs is `max_length x batch_size x hidden_size`); then a linear layer, which maps each RNN output element to a vector of size $N$ (which is commonly referred to as \"logits\", recall that $N=|Q|$, the size of the tag set).\n", "\n", "This function is expected to return `logits`, which provides a logit for each tag of each word of each sentence in the batch (which is a tensor of size `max_length x batch_size x N`). \n", "\n", "> You might find the following functions useful: \n", ">\n", "> * [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n", "> * [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n", "> * [`nn.RNN`](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n", "\n", "2. `compute_loss(self, logits, ground_truth)`: Computes the loss for a batch by comparing `logits` of a batch returned by `forward` to `ground_truth`, which stores the true tag ids for the batch. Thus `logits` is a tensor of size `max_length x batch_size x N`, and `ground_truth` is a tensor of size `max_length x batch_size`. Note that the criterion functions in `torch` expect outputs of a certain shape, so you might need to perform some shape conversions.\n", "\n", "> You might find [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) from the last project segment useful. Note that if you use `nn.CrossEntropyLoss` then you should not use a softmax layer at the end since that's already absorbed into the loss function. Alternatively, you can use [`nn.LogSoftmax`](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html) as the final sublayer in the forward pass, but then you need to use [`nn.NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html), which does not contain its own softmax. We recommend the former, since working in log space is usually more numerically stable.\n", "> For reshaping tensors, check out the [`torch.Tensor.view`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) method.\n", "\n", "3. `train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001)`: Trains the model on training data generated by the iterator `train_iter` and validation data `val_iter`.The `epochs` and `learning_rate` variables are the number of epochs (number of times to run through the training data) to run for and the learning rate for the optimizer, respectively. You can use the validation data to determine which model was the best one as the epocks go by. Notice that our code below assumes that during training the best model is stored so that `rnn_tagger.load_state_dict(rnn_tagger.best_model)` restores the parameters of the best model."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "y7SHhMr5pi_l"}, "source": ["#### Goal 2(b) RNN decoding\n", "\n", "Implement a method to predict the tag sequence associated with a sequence of words:\n", "\n", "1. `predict(self, text_batch)`: Returns the batched predicted tag sequences associated with a batch of sentences.\n", "2. `def evaluate(self, iterator)`: Returns the accuracy of the trained tagger on a dataset provided by `iterator`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "FPQ_Gb1NsUzL"}, "outputs": [], "source": ["class RNNTagger(nn.Module):\n", "    \n", "  ..."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Q3ubVqd8EdoY"}, "source": ["Now train your tagger on the training and validation set.\n", "Run the below cell to train an RNN, and evaluate it. A proper implementation should reach about **95%+ accuracy**."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "uIhOSOtU8m2d"}, "outputs": [], "source": ["# Instantiate and train classifier\n", "rnn_tagger = RNNTagger(TEXT, TAG, embedding_size=36, hidden_size=36).to(device)\n", "rnn_tagger.train_all(train_iter, val_iter, epochs=10, learning_rate=0.001)\n", "rnn_tagger.load_state_dict(rnn_tagger.best_model)\n", "\n", "# Evaluate model performance\n", "print(f'Training accuracy: {rnn_tagger.evaluate(train_iter):.3f}\\n'\n", "      f'Test accuracy:     {rnn_tagger.evaluate(test_iter):.3f}')"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "YCpYjn9W4yNK"}, "source": ["## LSTM for slot filling\n", "\n", "Did your RNN perform better than HMM? How much better was it? Was that expected? \n", "\n", "RNNs tend to exhibit the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). To remedy this, the Long-Short Term Memory (LSTM) model was introduced. In PyTorch, we can simply use [`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). \n", "\n", "In this section, you'll implement an LSTM model for slot filling. If you've got the RNN model well implemented, this should be extremely straightforward. Just copy and paste your solution, change the call to `nn.RNN` to a call to `nn.LSTM`, and make any other minor adjustments that are necessary. In particular, LSTMs have _two_ recurrent bits, `h` and `c`. You'll thus need to initialize both of these when performing forward computations."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "FPQ_Gb1NsUzL"}, "outputs": [], "source": ["class LSTMTagger(nn.Module):\n", "    \n", "  ..."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "b8c69PaVHymd"}, "source": ["Run the cell below to train an LSTM, and evaluate it. A proper implementation should reach about **95%+ accuracy**."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "Rg49kkS5G9GR"}, "outputs": [], "source": ["# Instantiate and train classifier\n", "lstm_tagger = LSTMTagger(TEXT, TAG, embedding_size=36, hidden_size=36).to(device)\n", "lstm_tagger.train_all(train_iter, val_iter, epochs=10, learning_rate=0.001)\n", "lstm_tagger.load_state_dict(lstm_tagger.best_model)\n", "\n", "# Evaluate model performance\n", "print(f'Training accuracy: {lstm_tagger.evaluate(train_iter):.3f}\\n'\n", "      f'Test accuracy:     {lstm_tagger.evaluate(test_iter):.3f}')"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "D43ge-OUrEw5"}, "source": ["## (Optional) Goal 4: Compare HMM to RNN/LSTM with different amounts of training data\n", "\n", "Vary the amount of training data and compare the performance of HMM to RNN or LSTM (Since RNN is similar to LSTM, picking one of them is enough.) Discuss the pros and cons of HMM and RNN/LSTM based on your experiments.\n", "\n", "> This part is more open-ended. We're looking for thoughtful experiments and analysis of the results, not any particular result or conclusion."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "eCpu5MOorTD8"}, "source": ["The below code shows how to subsample the training set with downsample ratio `ratio`. To speedup evaluation we only use 50 test samples."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "MHZq99iZlflx"}, "outputs": [], "source": ["ratio = 0.1\n", "test_size = 50\n", "\n", "# Set random seeds to make sure subsampling is the same for HMM and RNN\n", "random.seed(seed)\n", "torch.manual_seed(seed)\n", "\n", "train, val, test = tt.datasets.SequenceTaggingDataset.splits(\n", "            fields=fields, \n", "            path='./data/', \n", "            train='atis.train.txt', \n", "            validation='atis.dev.txt',\n", "            test='atis.test.txt')\n", "\n", "# Subsample\n", "random.shuffle(train.examples)\n", "train.examples = train.examples[:int(math.floor(len(train.examples)*ratio))]\n", "random.shuffle(test.examples)\n", "test.examples = test.examples[:test_size]\n", "\n", "# Rebuild vocabulary\n", "TEXT.build_vocab(train.text, min_freq=MIN_FREQ)\n", "TAG.build_vocab(train.tag)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "PD6uJ1GslmBq"}, "outputs": [], "source": ["..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "ARilKe5yrikx"}, "outputs": [], "source": ["..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "Ew04jov0tu3a"}, "outputs": [], "source": ["..."]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "## Debrief\n", "\n", "**Question:** We're interested in any thoughts you have about this project segment so that we can improve it for later years, and to inform later segments for this year. Please list any issues that arose or comments you have to improve the project segment. Useful things to comment on include the following: \n", "\n", "* Was the project segment clear or unclear? Which portions?\n", "* Were the readings appropriate background for the project segment? \n", "* Are there additions or changes you think would make the project segment better?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_debrief\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "## Instructions for submission of the project segment\n", "\n", "This project segment should be submitted to Gradescope, which will be made available some time before the due date.\n", "\n", "Project segment notebooks are manually graded, not autograded using otter as labs are. (Otter is used within project segment notebooks to synchronize distribution and solution code however.) **We will not run your notebook before grading it.** Instead, we ask that you submit the already freshly run notebook. The best method is to \"restart kernel and run all cells\", allowing time for all cells to be run to completion.\n", "\n", "We also request that you **submit a PDF of the freshly run notebook**. The simplest method is to use \"Export notebook to PDF\", which will render the notebook to PDF via LaTeX. If that doesn't work, the method that seems to be most reliable is to export the notebook as HTML (if you are using Jupyter Notebook, you can do so using `File -> Print Preview`), open the HTML in a browser, and print it to a file. Then make sure to add the file to your git commit. Please name the file the same name as this notebook, but with a `.pdf` extension. (Conveniently, the methods just described will use that name by default.) You can then perform a git commit and push and submit the commit to Gradescope."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# End of project segment 2"]}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": [], "include_colab_link": true, "name": "project2_sequence.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}, "title": "236299 Project Segment 2: Sequence labeling \u2013 The slot filling task"}, "nbformat": 4, "nbformat_minor": 4}