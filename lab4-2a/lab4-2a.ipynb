{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "jupyter": {"outputs_hidden": true, "source_hidden": true}}, "outputs": [], "source": ["# Please do not change this cell because some hidden tests might depend on it.\n", "import os\n", "\n", "# Otter grader does not handle ! commands well, so we define and use our\n", "# own function to execute shell commands.\n", "def shell(commands, warn=True):\n", "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n", "     \n", "       Prints the result to stdout and returns the exit status. \n", "       Provides a printed warning on non-zero exit status unless `warn` \n", "       flag is unset.\n", "    \"\"\"\n", "    file = os.popen(commands)\n", "    print (file.read().rstrip('\\n'))\n", "    exit_status = file.close()\n", "    if warn and exit_status != None:\n", "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n", "    return exit_status\n", "\n", "shell(\"\"\"\n", "ls requirements.txt >/dev/null 2>&1\n", "if [ ! $? = 0 ]; then\n", " rm -rf .tmp\n", " git clone https://github.com/cs236299-2020/lab4-2.git .tmp\n", " mv .tmp/tests ./\n", " mv .tmp/requirements.txt ./\n", " rm -rf .tmp\n", "fi\n", "pip install -q -r requirements.txt\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "id": "tribal-array", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook()"]}, {"cell_type": "markdown", "metadata": {"id": "IkDU500rvSzB"}, "source": ["# Lab 4-2 - Semantic interpretation with SQL"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Setup"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Download code for transforming grammars\n", "shell(\"\"\"\n", "  wget -nv -N -P scripts https://raw.githubusercontent.com/nlp-course/data/master/scripts/trees/transform.py\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["import sys\n", "\n", "import nltk\n", "import sqlite3\n", "\n", "# Import functions for transforming augmented grammars\n", "sys.path.insert(1, './scripts')\n", "import transform as xform"]}, {"cell_type": "markdown", "metadata": {"id": "t81A3m4Yvhwg"}, "source": ["## Introduction\n", "\n", "In the previous lab, you built a syntactic-semantic grammar for semantic interpretation of natural language into First-Order Logic (FOL) expressions.\n", "\n", "In this lab, you'll use a similar approach for a standard NLP task, natural language database query. You'll use the compositional semantics methods of the previous lab to convert natural language queries into SQL queries, which can be actually executed against a SQL database. Familiarity with this task will be useful in the fourth project segment, where you'll be building systems for natural language queries against the ATIS flight information database.\n", "\n", "For example, consider the phrase \"flights from Boston to New York\". The representation of the property denoted by this phrase might be (as per last lab), \n", "\n", "$$\\lambda x. Flight(x) \\land Origin(x, Boston) \\land Destination(x, New York)$$\n", "\n", "If instead we had a SQL database with a `flight` relation with fields `flight_id`, `origin`, and `destination`, we might translate this phrase into the following query\n", "```\n", "SELECT flight_id from flight WHERE origin == \"Boston\" and destination == \"NewYork\"\n", "```\n", "which returns the flight IDs of all the \"flights from Boston to New York\".\n", "\n", "Then, we will be able to run the generated SQL query on a database of flights, to actually answer the query.\n", "This process can be described as:\n", "\n", ">    NL query \u27f9 SQL query \u27f9 response\n", "\n", "We will focus on the first transformation (NL question \u27f9 SQL query) using a syntactic-semantic grammar. The second transformation (SQL query \u27f9 response) will be executed automatically by the database."]}, {"cell_type": "markdown", "metadata": {"id": "ThnQBDzmyAgI"}, "source": ["## Establishing the SQL database\n", "\n", "First, we will initialize our SQL dataset. We will populate our dataset similarly to the flight world of the previous lab. \n", "\n", "We initialize the same constants as in the previous lab:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "0Q8p_DMCyKAe"}, "outputs": [], "source": ["# Constants\n", "Boston = \"Boston\"\n", "NewYork = \"New York\"\n", "TelAviv = \"Tel Aviv\"\n", "DL10 = \"DL10\"\n", "DL11 = \"DL11\"\n", "DL13 = \"DL13\"\n", "LY01 = \"LY01\"\n", "LY12 = \"LY12\"\n", "Morning = \"Morning\"\n", "Evening = \"Evening\""]}, {"cell_type": "markdown", "metadata": {"id": "kLqeM0JjzWhu"}, "source": ["We populate our SQL dataset using a single table called **Flights**:\n", "\n", "**Flights**:\n", "\n", "| flightid \t| origin  \t| destination \t| departureTime \t| arrivalTime \t|\n", "|----------\t|---------\t|-------------\t|---------------\t|-------------\t|\n", "| DL10     \t| Boston  \t| NewYork     \t| Morning       \t| Evening     \t|\n", "| DL11     \t| Boston  \t| TelAviv     \t| Evening       \t| Morning     \t|\n", "| DL13     \t| NewYork \t| Boston      \t| Evening       \t| Evening     \t|\n", "| LY01     \t| TelAviv \t| NewYork     \t| Evening       \t| Morning     \t|\n", "| LY12     \t| NewYork \t| TelAviv     \t| Morning       \t| Evening     \t|\n", "\n", "> To reset the database when working on the lab, you can re-run the cell below.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 35}, "id": "iRzqMO5-TSB8", "outputId": "d86a86c3-f3ec-4a5f-eb09-6b29f8b0108f"}, "outputs": [], "source": ["def reset_database():\n", "    conn = sqlite3.connect(':memory:')\n", "    return conn.cursor()\n", "\n", "c = reset_database()\n", "\n", "c.execute('CREATE TABLE Flights (flightid TEXT, origin TEXT, destination TEXT, departureTime TEXT, arrivalTime TEXT)')\n", "c.executemany('INSERT INTO Flights VALUES (?, ?, ?, ?, ?)', \n", "              [(DL10, Boston, NewYork, Morning, Evening),\n", "               (DL11, Boston, TelAviv, Evening, Morning), \n", "               (DL13, NewYork, Boston, Evening, Evening), \n", "               (LY01, TelAviv, NewYork, Evening, Morning), \n", "               ])"]}, {"cell_type": "markdown", "metadata": {"id": "5d0tTpe5_sPM"}, "source": ["Let's query the table, to verify that it contains our inserted rows: "]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 101}, "id": "prCD6Q_R-DPX", "outputId": "020bb0e9-a194-4ae9-839a-43261e2e8c5b"}, "outputs": [], "source": ["print('Flights:')\n", "res = c.execute('SELECT * FROM Flights')\n", "for row in res:\n", "  print(row)"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "id": "wlZTsOwP_x3_"}, "source": ["Complete the initialization of the database (using `c.execute('INSERT INTO...')`), for the last flight:\n", "\n", "| flightid \t| origin  \t| destination \t| departureTime \t| arrivalTime \t|\n", "|----------\t|---------\t|-------------\t|---------------\t|-------------\t|\n", "| LY12     \t| NewYork \t| TelAviv     \t| Morning       \t| Evening     \t|\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: add_flight\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 101}, "id": "Tyg1ss1D04pw", "outputId": "7d34b508-66c1-437f-825d-33c5dde30e94"}, "outputs": [], "source": ["#TODO - Insert the final flight into the table\n", "..."]}, {"cell_type": "code", "execution_count": null, "id": "close-logan", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"add_flight\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can test that the row was properly added:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 101}, "id": "Tyg1ss1D04pw", "outputId": "7d34b508-66c1-437f-825d-33c5dde30e94"}, "outputs": [], "source": ["res = c.execute('SELECT * FROM Flights')\n", "for row in res:\n", "  print(row)"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "id": "1ERj-16PE74T"}, "source": ["In the previous lab, we created a syntactic-semantic grammar that used the lambda calculus to build FOL expressions to represent the meanings of queries. In this lab, w'll use the same syntactic productions, but instead, map constituents to functions that build SQL queries.\n", "\n", "Complete the following grammar. The first rule is provided as an example.\n", "\n", "**Hints**:\n", "\n", "1. Recall that the semantic composition functions are functions from right-hand side meanings (there might be zero or more) to the meaning of the whole.\n", "\n", "1. The general structure of SQL queries for this grammar will be: \n", "\n", "    `SELECT DISTINCT flightid from Flights WHERE ...`\n", "\n", "   For consistency, the queries will always have a `WHERE` clause. If there are no conditions required in the body of the `WHERE` clause, you can just use `1` as the `WHERE` body. (The `1` value is SQL's proxy for the Boolean value `TRUE`.) Again for consistency, you might want always to have a `1` as the final condition, e.g., \n", "   \n", "    `SELECT DISTINCT flightid from Flights WHERE origin == \"New York\" AND arrivalTime == \"Evening\" AND 1`\n", "    \n", "1. Use semantic types consistently. For instance, you might use the following typings for meanings:\n", "\n", "    Syntactic Type      | Semantic Type        | Example\n", "    --------------------|----------------------|------------------------\n", "    Q                   | str (a full query)   | 'SELECT DISTINCT flightid FROM Flights WHERE 1'\n", "    NP, PP, PP_PLACE, PP_TIME | str (a `WHERE` body) -> str (a `WHERE` body)  | lambda P: f'origin == NewYork AND {P}'\n", "    LOC                 | str (a place)        | 'NewYork'\n", "    TIME                | str (a time)         | 'Morning'\n", "\n", "    By making NP and PP meanings functions from `WHERE` bodies to `WHERE` bodies, it's easy to have compound conditions like \n", "    `origin == \"New York\" AND arrivalTime == \"Evening\"`.\n", "\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: grammar_spec\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "xncMiJlDE8xC"}, "outputs": [], "source": ["#TODO - Add augmentations to the grammar to generate SQL queries. \n", "# We gave you a few to get started.\n", "\n", "grammar_spec = \"\"\"\n", "    Q -> NP\n", "    NP -> 'flights'\n", "    NP -> NP PP\n", "    \n", "    PP -> PP_PLACE\n", "    PP -> PP_TIME\n", "    \n", "    PP_PLACE -> 'from' LOC\n", "              | 'leaving' LOC\n", "              | 'to' LOC\n", "              | 'arriving' 'at' LOC\n", "\n", "    PP_TIME -> 'arriving' TIME\n", "             | 'departing' TIME\n", "             | 'leaving' TIME\n", "\n", "    LOC -> 'Boston'                 : lambda: Boston\n", "    LOC -> 'New' 'York'             : lambda: NewYork\n", "    LOC -> 'Tel' 'Aviv'             : lambda: TelAviv\n", "\n", "    TIME -> 'in' 'the' 'morning'    : lambda: Morning\n", "    TIME -> 'in' 'the' 'evening'    : lambda: Evening\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["grammar, augmentations = xform.parse_augmented_grammar(grammar_spec, globals=globals())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To test the grammar, we can parse a sample query:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "xncMiJlDE8xC"}, "outputs": [], "source": ["parser = nltk.parse.BottomUpChartParser(grammar)\n", "\n", "for parse in parser.parse('flights from Boston leaving in the morning'.split()):\n", "  parse.pretty_print()"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "id": "R_vqeqG_6Mxe"}, "source": ["## Semantically interpreting syntactic trees\n", "\n", "With parse tree in hand, and the dictionary of semantic augmentations for each syntactic rule, we can recursively traverse the tree and compute its meaning.\n", "\n", "Write a function `interpret`, which takes a parse tree and a dictionary of semantic augmentations indexed by syntactic production (as returned by `xform.parse_augmented_grammar`) and returns the meaning for the tree.\n", "\n", "The function will be naturally recursive, since to compute the meaning of the tree you'll need to apply the appropriate semantic composition function to the meanings of the subtrees (that's the recursive bit).\n", "\n", "**Hints:**\n", "1. To iterate over a tree's child subtrees, you can simply iterate as if it were a list: `[child for child in tree]`. Note that `child` can be either a string (for terminals), or an `nltk.Tree` object storing the subtree (for nonterminals).\n", "\n", "2. To get the syntactic rule at the root of the tree, you can use `tree.productions()[0]`\n", "\n", "3. You'll want to know about Python's `*` operator. If you want to apply a function that takes multiple arguments and you have a list of its arguments, you can \"unpack\" the list using the `*` operator. For example, the following code is valid and works as expected:\n", "   ```\n", "   def f(a, b, c):\n", "     return a + b + c\n", "     \n", "   my_arguments = [1,2,3]\n", "   f(*my_arguments)\n", "   ```\n", "\n", "   This might be useful when calling a semantic composition function, since how many arguments it takes is only known at runtime.\n", "\n", "4. If you want to check whether the root of a subtree `t` is a nonterminal or a terminal, you can use `if isinstance(t, nltk.Tree)`. This will return `True` for nonterminals, and `False` for terminals (because terminals are just strings in NLTK).\n", "\n", "5. Use the `augmentations` dictionary to \"lookup\" the semantic rule for a given syntactic production rule.\n", "\n", "6. The solution is only a few lines of code. \n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: interpreting\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "LHp_ckbYOjDg"}, "outputs": [], "source": ["#TODO - write the `interpret` function\n", "#       that returns a string of SQL query\n", "def interpret(tree, augmentations):\n", "  result = ...\n", "  return result"]}, {"cell_type": "code", "execution_count": null, "id": "significant-photograph", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"interpreting\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Putting it all together\n", "\n", "Now we can put everything together to translate an NL query to SQL and execute the query against the database."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "LHp_ckbYOjDg"}, "outputs": [], "source": ["def query_to_sql(nl_query, parser, augmentations):\n", "  \"\"\"Parses a natural language query, interprets it as SQL, and \n", "     executes and returns the result of the query.\n", "  \"\"\"\n", "  sentence = nl_query.split()\n", "  parses = [p for p in parser.parse(sentence)]\n", "  for tree in parses:\n", "    tree.pretty_print()\n", "  return [interpret(parse, augmentations) for parse in parses]\n", "\n", "def query_to_answer(nl_query, parser, augmentations):\n", "  \"\"\"Parses and translates a natural language query `nl_query` to a SQL query \n", "     as per the provided `parser` and `augmentations` and executes the SQL\n", "     query on the database, printing the results.\"\"\"\n", "  sql_queries = query_to_sql(nl_query, parser, augmentations)\n", "  for query in sql_queries:\n", "    print(f'SQL query: {query}')\n", "    print('Result:')\n", "    res = list(c.execute(query))\n", "    for row in res:\n", "      print(row)\n", "  return res"]}, {"cell_type": "markdown", "metadata": {"id": "xTLreHacBCOz"}, "source": ["You can now test your parser by running some queries all of the way through.\n", "The expected SQL query for `flights from Boston` is: `SELECT DISTINCT flightid FROM Flights WHERE origin == \"Boston\" AND 1`.\n", "\n", "> Note the quotation marks around `Boston`. These are required so that SQL interprets it as a field value rather than a field name."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 286}, "id": "Kamd5D9HSA1G", "outputId": "e1edf0df-cb64-4266-d030-5c3345818b4a"}, "outputs": [], "source": ["res1 = query_to_answer('flights from Boston', parser, augmentations)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 302}, "id": "k3WxUXOYzJLs", "outputId": "6a17f355-6608-4d98-8216-a64df68312b8"}, "outputs": [], "source": ["res2 = query_to_answer('flights from Boston to New York', parser, augmentations)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 319}, "id": "rbRdMZRHzJOU", "outputId": "54b720ad-87b2-4056-ebb9-8ac41040355d"}, "outputs": [], "source": ["res3 = query_to_answer('flights from New York arriving in the evening', parser, augmentations)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 370}, "id": "RlsU4kNHzJRZ", "outputId": "d5dcb8b1-386e-4a2c-94f9-7d7197a07b62"}, "outputs": [], "source": ["res4 = query_to_answer('flights from New York to Tel Aviv departing in the morning arriving in the evening', parser, augmentations)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 370}, "id": "j54jMOvvzJai", "outputId": "e46411b7-b989-415b-942a-a3930d23ac71"}, "outputs": [], "source": ["res5 = query_to_answer('flights from Tel Aviv arriving at New York leaving in the evening arriving in the morning', parser, augmentations)"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "id": "C8-_4ha505lU"}, "source": ["Write your own sentence (that our syntactic grammar can parse) and check its results:\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: your_own\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 386}, "id": "_wCXAxh01EM8", "outputId": "ccbe3f7a-93d9-401c-cda4-1d40c439b152"}, "outputs": [], "source": ["#TODO - Write your own sentence that is parsable\n", "your_query = ...\n", "query_to_answer(your_query, parser, augmentations)"]}, {"cell_type": "code", "execution_count": null, "id": "improving-wound", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"your_own\")"]}, {"cell_type": "markdown", "metadata": {"id": "1BEyLDcRzjoX"}, "source": ["Our semantic parser requires that the natural language input sentence be syntactically well-formed according to the grammar. But what happens if the sentence fails to parse syntactically? \n", "\n", "For example, our parser can parse the sentence `flights from Boston`, but it cannot parse the sentence `show me flights from Boston`: "]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "id": "WDwbY-gp0zfB", "outputId": "ec03ca43-9dc6-4d08-acc8-75f1c6353fde"}, "outputs": [], "source": ["try:\n", "  query_to_answer('show me flights from Boston', parser, augmentations)\n", "except ValueError as e:\n", "  print(e)"]}, {"cell_type": "markdown", "metadata": {"id": "YwkINT8NzxQy"}, "source": ["If the sentence does not parse syntactically, our semantic parser won't be able to interpret it semantically.\n", "\n", "One possible way to address this problem is to use a *partial* syntactic parser -- a parser that provides a parse tree for the longest subsentence that it can parse. In the case of `show me flights from Boston`, such a partial parser will drop the words `show me` and provide the parse tree only for `flights from boston`. \n", "\n", "Another possible solution is to use neural sequence-to-sequence methods, that will naturally address this problem by having an embedding for \"unknown\" words (usually referred to as `<UNK>`). We'll turn to those in later labs."]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "---\n", "\n", "## Lab debrief \u2013 for consensus submission only\n", "\n", "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n", "\n", "* Was the lab too long or too short?\n", "* Were the readings appropriate for the lab? \n", "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n", "* Are there additions or changes you think would make the lab better?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_debrief\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "id": "previous-tenant", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "# End of Lab 4-2"]}, {"cell_type": "markdown", "id": "manual-harris", "metadata": {"deletable": false, "editable": false}, "source": ["---\n", "\n", "To double-check your work, the cell below will rerun all of the autograder tests."]}, {"cell_type": "code", "execution_count": null, "id": "alpha-functionality", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check_all()"]}], "metadata": {"celltoolbar": "Edit Metadata", "colab": {"collapsed_sections": [], "name": "lab4-2.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.3"}, "title": "CS187 Lab 4-2: Semantic interpretation with SQL"}, "nbformat": 4, "nbformat_minor": 4}