{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"title":"CS187 Lab 1-3: Naive Bayes classification","colab":{"name":"lab1-3.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"deletable":false,"editable":false,"jupyter":{"outputs_hidden":true,"source_hidden":true},"id":"BlhWBasdUNvX","executionInfo":{"status":"ok","timestamp":1604997693381,"user_tz":-120,"elapsed":8238,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"5f9f81c9-4ae4-437b-f74c-24f54b5db09d","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Please do not change this cell because some hidden tests might depend on it.\n","import os\n","\n","# Otter grader does not handle ! commands well, so we define and use our\n","# own function to execute shell commands.\n","def shell(commands, warn=True):\n","    \"\"\"Executes the string `commands` as a sequence of shell commands.\n","     \n","       Prints the result to stdout and returns the exit status. \n","       Provides a printed warning on non-zero exit status unless `warn` \n","       flag is unset.\n","    \"\"\"\n","    file = os.popen(commands)\n","    print (file.read().rstrip('\\n'))\n","    exit_status = file.close()\n","    if warn and exit_status != None:\n","        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n","    return exit_status\n","\n","shell(\"\"\"\n","ls requirements.txt >/dev/null 2>&1\n","if [ ! $? = 0 ]; then\n"," rm -rf .tmp\n"," git clone https://github.com/cs236299-2020/lab1-3.git .tmp\n"," mv .tmp/tests ./\n"," mv .tmp/requirements.txt ./\n"," rm -rf .tmp\n","fi\n","pip install -q -r requirements.txt\n","\"\"\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"XrE__grsUNvi","executionInfo":{"status":"ok","timestamp":1604997696897,"user_tz":-120,"elapsed":1922,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["# Initialize Otter\n","import otter\n","grader = otter.Notebook()"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"jupyter":{"source_hidden":true},"id":"MKqNcbfyUNvp"},"source":["%%latex\n","\\newcommand{\\vect}[1]{\\mathbf{#1}}\n","\\newcommand{\\cnt}[1]{\\sharp(#1)}\n","\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n","\\newcommand{\\softmax}{\\operatorname{softmax}}\n","\\newcommand{\\Prob}{\\Pr}\n","\\newcommand{\\given}{\\,|\\,}"]},{"cell_type":"markdown","metadata":{"id":"KxtIdrjlUNvq"},"source":["$$\n","\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n","\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n","\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n","\\renewcommand{\\softmax}{\\operatorname{softmax}}\n","\\renewcommand{\\Prob}{\\Pr}\n","\\renewcommand{\\given}{\\,|\\,}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"XTIuL9IxUNvr"},"source":["# Course 236299\n","## Lab 1-3 – Naive Bayes classification"]},{"cell_type":"markdown","metadata":{"id":"XvxSHLFkUNvs"},"source":["In this lab, you'll apply the naive Bayes method to the _Federalist_ papers' authorship attribution problem.\n","\n","After this lab, you should be able to\n","\n","* Derive the basic equations for the naive Bayes classification method;\n","* Estimate the parameters for the naive Bayes model;\n","* Determine where use of the \"log trick\" is indicated, and apply it."]},{"cell_type":"markdown","metadata":{"id":"qTyU-23QUNvt"},"source":["New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n","\n","* [`math.log2`](https://docs.python.org/3.8/library/math.html#math.log2)"]},{"cell_type":"markdown","metadata":{"id":"jhohf1lMUNvu"},"source":["## Preparation – Loading packages and data"]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":true},"id":"A6pZ_TZ3UNvv","executionInfo":{"status":"ok","timestamp":1604997700823,"user_tz":-120,"elapsed":1111,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["%matplotlib inline\n","import json\n","import math\n","import matplotlib\n","import matplotlib.pyplot as plt\n","matplotlib.style.use('tableau-colorblind10')\n","import numpy as np\n","\n","from collections import defaultdict"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"MWT8rsCoUNv1","executionInfo":{"status":"ok","timestamp":1604997705161,"user_tz":-120,"elapsed":951,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"8459d4a1-bb83-4126-ddd0-07451ae4ec01","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Download and read the Federalist data from the json file\n","shell('wget -nv -N -P data https://github.com/nlp-236299/data/raw/master/Federalist/federalist_data.json')\n","with open('data/federalist_data.json', 'r') as fin:\n","    dataset = json.load(fin)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lBzsWt0wUNv6","executionInfo":{"status":"ok","timestamp":1604997709472,"user_tz":-120,"elapsed":1151,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["# As before, we extract the papers by either of Madison and Hamilton\n","# to serve as training data.\n","training = list(filter(lambda ex: ex['authors'] in ['Madison', 'Hamilton'],\n","                       dataset))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pt_fp4kZUNv_"},"source":["## The Naive Bayes method reviewed\n","A quick review of the Naive Bayes (NB) method for text classification: In classification tasks, we're given a representation of some text as a vector $\\mathbf{x} = \\langle x_1, x_2, \\ldots, x_m \\rangle$ of feature values, and we'd like to determine which of a set of classes $\\{ c_1, c_2, \\ldots, c_k \\}$ the text should be classified as. \n","\n","> In the case at hand, the Federalist Papers, for a given document, we'll take $\\mathbf{x} = \\langle x_1, x_2, \\ldots, x_m \\rangle$ to be the sequence of words in the document, so each $x_i$ corresponds to a single word token.\n","\n","We might naturally think to choose that class that has the highest probability of being correct, that is, the class $c_i$ that maximizes $Pr(c_i \\mid \\mathbf{x})$.\n","\n","By Bayes rule (this is the \"Bayes\" part in the name \"Naive Bayes\"), \n","\n","\\begin{align*}\n","\\argmax{i} \\Prob(c_i \\given \\vect{x}) \n","&= \\argmax{i} \\frac{\\Prob(\\vect{x} \\given c_i) \\cdot \\Prob(c_i)}{\\Prob(\\vect{x})} \\\\\n","&= \\argmax{i} \\Prob(\\vect{x} \\given c_i) \\cdot \\Prob(c_i)\n","\\end{align*}"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"GV3O18hLUNwA"},"source":["<!-- BEGIN QUESTION -->\n","\n","**Question**: Why can we drop the denominator in the last step of this derivation?\n","<!--\n","BEGIN QUESTION\n","name: open_response_denominator\n","manual: true\n","-->"]},{"cell_type":"markdown","metadata":{"id":"I5614AOyUNwB"},"source":["Because it a constant, thus maximizing numerator will maximize the whole expression, therefore will maximize the postprior probability."]},{"cell_type":"markdown","metadata":{"id":"Mv_McrJQUNwC"},"source":["<!-- END QUESTION -->\n","\n","\n","\n","We use the following terminology: $\\Prob(c_i)$ is the _prior probability_. $\\Prob(\\vect{x} \\given c_i)$ is the _likelihood_. \n","$\\Prob(c_i \\given \\vect{x})$ is the _posterior probability_.\n","\n","By the chain rule, \n","\n","\\begin{align*}\n","\\Prob(\\vect{x} \\given c_i) &= \\Prob(x_1, \\ldots, x_m \\given c_i) \\\\\n","&= \\Prob(x_1 \\given c_i) \\cdot \\Prob(x_2, \\ldots, x_m \\given x_1, c_i) \\\\\n","&= \\Prob(x_1 \\given c_i) \\cdot \\Prob(x_2 \\given x_1, c_i) \\cdot \\Prob(x_3, \\ldots,\n","x_m \\given x_1, x_2, c_i) \\\\\n","\\cdots &= \\prod_{j=1}^m \\Prob(x_j \\given x_1, \\ldots, x_{j-1}, c_i)\n","\\end{align*}\n","\n","We further assume that each feature $x_i$ is independent of all the others given the class. (That's the \"naive\" part.) So \n","\n","$$\n","\\Prob(x_j \\given x_1, \\ldots, x_{j-1}, c_i) \\approx \\Prob(x_j \\given c_i)\n","$$\n","\n","Using this approximation, we'll calculate instead the class as per the following maximization:\n","\n","$$\n","\\argmax{i} \\Prob(c_i \\given \\vect{x}) \\approx \\argmax{i} \\Prob(c_i) \\cdot \\prod_{j=1}^m \\Prob(x_j \\given c_i)\n","$$\n","\n","> This independence assumption, in the text case, amounts to ignoring the order and even the cooccurence of words in a document, a quite aggressive and unrealistic independence assumption indeed.\n","\n","All we need, then, for the Naive Bayes classification method is values for $\\Prob(c_i)$ and $\\Prob(x_j \\given c_i)$ for each feature $x_j$ and each class $c_i$. These constitute the parameters of the model, which we will learn from a training dataset."]},{"cell_type":"markdown","metadata":{"id":"7DWfEqt9UNwD"},"source":["## Naive Bayes for the Federalist papers\n","\n","In applying Naive Bayes to an example in the Federalist dataset, we'll take the $x_i$ to be the _tokens in the example_. To make the calculations easier, in this lab, we won't use _all_ of the tokens, just the tokens of the four word types we've been attending to, but in an actual application of NB, we'd use all of the word types. As a reminder,"]},{"cell_type":"code","metadata":{"id":"oARpbdkqUNwF","executionInfo":{"status":"ok","timestamp":1604998653789,"user_tz":-120,"elapsed":895,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["keywords = ['on', 'upon', 'there', 'whilst']"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SH42yOpUUNwJ"},"source":["and the two class labels are"]},{"cell_type":"code","metadata":{"id":"5IIOYfTzUNwK","executionInfo":{"status":"ok","timestamp":1604998654911,"user_tz":-120,"elapsed":483,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["classes = ['Hamilton', 'Madison']"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ok2lY6NcUNwP"},"source":["### Estimating the prior probabilities"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"aS7oQT8YUNwQ"},"source":["Let's start with the prior probabilities $\\Prob(c_i)$. In our case, there are only two class labels, for Hamilton and Madison. We estimate the probability of a class $c_i$ by simply counting the proportion of examples that are labeled with that class. (This estimate is the _sample probability_, which is also referred to as the _maximum likelihood estimate_ for reasons we'll skip for the moment.) That is, we estimate \n","\n","$$ \\Prob(c_i) \\approx \\frac{\\cnt{c_i}}{N} $$\n","\n","where $N$ is the number of training examples, and $\\cnt{c_i}$ is the number of training examples of class $c_i$.\n","\n","In the cell below, write code to count how many of the training examples are labeled with Hamilton and how many are labeled with Madison. Use these to provide estimates of the Hamilton and Madison prior probabilities.\n","<!--\n","BEGIN QUESTION\n","name: priors\n","-->"]},{"cell_type":"code","metadata":{"id":"XLF6MU_fUNwR","executionInfo":{"status":"ok","timestamp":1604998121689,"user_tz":-120,"elapsed":649,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["#TODO - calculate the prior probability for Madison and Hamilton\n","prior_madison = len([ex for ex in training if ex['authors']=='Madison'])/len(training)\n","prior_hamilton = len([ex for ex in training if ex['authors']=='Hamilton'])/len(training)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"sXwGNXAZUNwf","executionInfo":{"status":"ok","timestamp":1604998125745,"user_tz":-120,"elapsed":960,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"cb571108-bb23-4836-e9a9-9f0b7224790d","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"priors\")"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"BrBWNqsjUNwk","executionInfo":{"status":"ok","timestamp":1604998127984,"user_tz":-120,"elapsed":956,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"a908b99d-32fc-4194-b4c0-af24d6bdea43","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(f\"Madison  prior: {prior_madison:4f}\\n\"\n","      f\"Hamilton prior: {prior_hamilton:4f}\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Madison  prior: 0.227273\n","Hamilton prior: 0.772727\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"aCKieKwHUNwo"},"source":["<!-- BEGIN QUESTION -->\n","\n","**Question:** How we might predict the class of a _Federalist_ document _prior_ to looking at the actual content of the document? (That's why these probabilities are called \"priors\".)\n","<!--\n","BEGIN QUESTION\n","name: open_response_priors\n","manual: true\n","-->"]},{"cell_type":"markdown","metadata":{"id":"xYHKcjo9UNwp"},"source":["We might predict it accroding to the prior probabilities, thus it will be labeled as 'Hamilton', as it's occurence probability through the documents labels is larger."]},{"cell_type":"markdown","metadata":{"id":"406_9fxlUNwr"},"source":["<!-- END QUESTION -->\n","\n","\n","\n","### Estimating the likelihood probabilities"]},{"cell_type":"markdown","metadata":{"id":"SeyEC1VSUNws"},"source":["Now for the likelihood probabilities, the conditional probability of a word given a class. For each likelihood $\\Prob(x_j \\given c_i)$, we need to estimate a value. We'll do so by simply counting the number of training examples with feature value $x_j$ that are labeled $c_i$ (notated as $\\cnt{x_j, c_i}$) as a proportion of the overall number of words labeled as $c_i$, that is,\n","\n","$$ \\Prob(x_j \\given c_i) \\approx \\frac{\\cnt{x_j, c_i}}{\\sum_k \\cnt{x_k, c_i}} $$\n","\n","Again, for the text case, each token counts as an instance of the corresponding word type in a training example. Note that $\\sum_k \\cnt{x_k, c_i}$ is not the same as $\\cnt{c_i}$.\n"," \n","We've provided a small table that shows, for each label (author) and each of the four word types of interest, how many tokens of the type occurred in training examples with that label."]},{"cell_type":"code","metadata":{"id":"h5v_oyTHUNwt","executionInfo":{"status":"ok","timestamp":1604998658205,"user_tz":-120,"elapsed":760,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"0e279703-7fa2-4837-fea8-4d0e5a9f0623","colab":{"base_uri":"https://localhost:8080/"}},"source":["def counts(dataset, label, index):\n","    \"\"\"Returns the total count for `index` for examples with the \n","       given `label`\"\"\"\n","    return sum([example['counts'][index] \n","                for example in dataset \n","                if example['authors'] == label])\n","\n","# print a table header\n","print(f\"{'':10}\", end=\"\")\n","for i in range(4):\n","    print(f\"{keywords[i]:>8}\", end=\"\")\n","print()\n","# print table entries for each label\n","for label in classes:\n","    print(f\"{label:10}\", end=\"\")\n","    for i in range(4):\n","        print(f\"{counts(training, label, i):8}\", end=\"\")\n","    print()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["                on    upon   there  whilst\n","Hamilton       390     377     369       1\n","Madison        308       7      32      12\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"LA8a_Jc9UNwz"},"source":["Given the counts in this table, what would an estimate be for the probability that a given word would be \"whilst\" given that the document was authored by Madison, that is, $\\Prob(\\mathrm{whilst} \\given \\mathrm{Madison})$?\n","<!--\n","BEGIN QUESTION\n","name: prob_whilst_madison\n","-->"]},{"cell_type":"code","metadata":{"id":"sbbfTHf1UNw0","executionInfo":{"status":"ok","timestamp":1604998791563,"user_tz":-120,"elapsed":672,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["#TODO\n","prob_whilst_madison = 12/(12+32+7+308)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"wAN5-ICYUNw5","executionInfo":{"status":"ok","timestamp":1604998786919,"user_tz":-120,"elapsed":648,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"97f4e9ca-a92b-4001-fc66-1fac8a233946","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"prob_whilst_madison\")"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"cugD8VDHUNw_"},"source":["What about the probability $\\Prob(on \\given Hamilton)$?\n","<!--\n","BEGIN QUESTION\n","name: prob_on_hamilton\n","-->"]},{"cell_type":"code","metadata":{"id":"yCKPuKUIUNxA","executionInfo":{"status":"ok","timestamp":1604998810119,"user_tz":-120,"elapsed":809,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["#TODO\n","prob_on_hamilton = 390/(390+377+369+1)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"6Q57GcSQUNxE","executionInfo":{"status":"ok","timestamp":1604998812135,"user_tz":-120,"elapsed":810,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"63d79086-a818-4f9a-ae06-b0cd7c14033d","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"prob_on_hamilton\")"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"seFy5cAtUNxI"},"source":["Consider a sample text \n","\n","> **whilst** depending neither **on** the American government nor **on** the British\n","\n","What would the Naive Bayes method estimate for the likelihood probability that this sentence was labeled as Hamilton? As Madison? (ignore all the words except for the four keywords in our little example. With a full-blown NB analysis, we'd be using *all* of the words in the text.)\n","<!--\n","BEGIN QUESTION\n","name: likelihoods\n","-->"]},{"cell_type":"code","metadata":{"id":"j3QRskWnUNxJ","executionInfo":{"status":"ok","timestamp":1605000147889,"user_tz":-120,"elapsed":684,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["#TODO\n","prob_whilst_hamilton = 1/(390+377+369+1)\n","prob_on_hamilton = 390/(390+377+369+1)\n","prob_on_hamilton = 390/(390+377+369+1)\n","likelihood_hamilton = prob_whilst_hamilton*prob_on_hamilton*prob_on_hamilton\n","prob_whilst_madison = 12/(12+32+7+308)\n","prob_on_madison = 308/(12+32+7+308)\n","prob_on_madison = 308/(12+32+7+308)\n","likelihood_madison = prob_whilst_madison*prob_on_madison*prob_on_madison"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"YstKfU57UNxO","executionInfo":{"status":"ok","timestamp":1604999722663,"user_tz":-120,"elapsed":1033,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"8fb76f33-6f66-43c1-9b3d-aaae3b6a3bef","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"likelihoods\")"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"C2G0zylTUNxS","executionInfo":{"status":"ok","timestamp":1604999722956,"user_tz":-120,"elapsed":447,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"80790ccf-9656-42c9-8e35-868799daed25","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(f\"Madison  likelihood: {likelihood_madison:4f}\\n\"\n","      f\"Hamilton likelihood: {likelihood_hamilton:4f}\")"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Madison  likelihood: 0.024604\n","Hamilton likelihood: 0.000103\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ya1_QY1CUNxY"},"source":["### Posterior probabilities"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"ZMGIQJdZUNxZ"},"source":["We're almost there. We simply need to combine the prior probabilities and the likelihood probabilities for each class to form the posterior, and select the largest one. (We don't actually calculate the posterior _probability_ because we aren't dividing through by $\\Prob(\\vect{x})$. Instead, we get something like a posterior _score_.)\n","\n","Calculate the posteriors for the two classes for the given text, and then specify which class – Hamilton or Madison – the NB method would predict for the sample text.\n","<!--\n","BEGIN QUESTION\n","name: posteriors\n","-->"]},{"cell_type":"code","metadata":{"id":"X2kLqLzPUNxa","executionInfo":{"status":"ok","timestamp":1605000438943,"user_tz":-120,"elapsed":651,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["#TODO\n","posterior_madison = likelihood_madison * prior_madison\n","posterior_hamilton = likelihood_hamilton * prior_hamilton\n","sample_classification = 'Madison' if posterior_madison > posterior_hamilton else 'Hamilton' "],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"41R37C8vUNxe","executionInfo":{"status":"ok","timestamp":1605000440637,"user_tz":-120,"elapsed":757,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"ef480e47-9599-4072-b920-b0a9cdcaa26c","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"posteriors\")"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"0i7ENsgzUNxh","executionInfo":{"status":"ok","timestamp":1605000442449,"user_tz":-120,"elapsed":658,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"8d708545-8f2f-43f4-ddc5-3d4dcdf86e36","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(f\"Madison  posterior: {posterior_madison:4f}\\n\"\n","      f\"Hamilton posterior: {posterior_hamilton:4f}\\n\"\n","      f\"Sample classification: {sample_classification}\")"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Madison  posterior: 0.005592\n","Hamilton posterior: 0.000080\n","Sample classification: Madison\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"3Hjd-ftZUNxo"},"source":["<!-- BEGIN QUESTION -->\n","\n","**Question:** Is the NB-predicted classification the same as or different from the classification based on the **priors only**? Why?\n","<!--\n","BEGIN QUESTION\n","name: open_response_nb_v_priors\n","manual: true\n","-->"]},{"cell_type":"markdown","metadata":{"id":"oLPLyEHhUNxp"},"source":["It is different, because despite the fact that the prior probability of 'Hamilton' is larger, looking at the relevant words which appear in the document closely and calculating their likelihood and then multiplying it by the prior, according to the postprior formula, gives us a very poor postprior probability for Hamilton. In contrast those words (features) are very likely to appear in Madison's texts (as we see according to their likelihood), thus the prediction of 'Madison' much more accurate, as expected that the likelihood to infer much more accurate predictions with the prior then just the prior."]},{"cell_type":"markdown","metadata":{"id":"jTP0VbOgUNxq"},"source":["<!-- END QUESTION -->\n","\n","\n","\n","### A practical issue\n","\n","The computations of what we've been calling the posterior scores\n","$$\\Prob(c_i \\given \\vect{x}) \\approx \\Prob(c_i) \\cdot \\prod_{j=1}^m \\Prob(x_j \\given c_i)$$\n","involve the multiplication of many extremely small numbers. This is a recipe for [_arithmetic underflow_](https://en.wikipedia.org/wiki/Arithmetic_underflow), leading to a numerical instability.\n","\n","Instead, rather than maximizing the posterior, we maximize its logarithm. Since the logarithm function is monotonic (see the next cell for a figure), whichever $i$ maximizes the posterior maximizes its log as well."]},{"cell_type":"code","metadata":{"id":"Wuq6FthBUNxs","executionInfo":{"status":"ok","timestamp":1605000504056,"user_tz":-120,"elapsed":919,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"d57a0c18-8804-4026-f73b-1157b443b031","colab":{"base_uri":"https://localhost:8080/","height":281}},"source":["def log_plot():\n","    x = np.linspace(1e-10, 1, 100)\n","    fig, ax = plt.subplots()\n","    ax.plot(x, np.log2(x), label = \"log_2\")\n","    plt.title(\"Monotonicity of logarithm\")\n","    plt.legend()\n","    \n","log_plot()"],"execution_count":32,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"w11yrOPKUNxw"},"source":["The log of the posterior is\n","$$\\log \\left(\\Prob(c_i) \\cdot \\prod_{j=1}^m \\Prob(x_j \\given c_i)\\right)\n","      = \\log\\Prob(c_i) + \\sum_{j=1}^m \\log\\Prob(x_j \\given c_i)$$\n","so that the calculation now involves the **sum** of a bunch of numbers rather than the **product**. In practice, this computation is much more numerically stable.\n","\n","> A log-of-probability value is referred to, colloquially if not quite accurately, as a _logit_, because of a resemblance to the values of [the logit function](https://en.wikipedia.org/wiki/Logit).\n","\n","Calculate the log of the posterior for Madison for the sample text by summing up all of the pertinent parts, and similarly for Hamilton. Use the base 2 logarithm.\n","<!--\n","BEGIN QUESTION\n","name: log_posteriors\n","-->"]},{"cell_type":"code","metadata":{"id":"17ez2z9rUNxw","executionInfo":{"status":"ok","timestamp":1605000716950,"user_tz":-120,"elapsed":650,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["#TODO - calculate the log of the posterior for Madison by summing up all of the pertinent parts\n","log_posterior_madison = math.log2(prior_madison)+math.log2(prob_whilst_madison)+math.log2(prob_on_madison)+math.log2(prob_on_madison)\n","log_posterior_hamilton = math.log2(prior_hamilton)+math.log2(prob_whilst_hamilton)+math.log2(prob_on_hamilton)+math.log2(prob_on_hamilton)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"kYFTwjTIUNx0","executionInfo":{"status":"ok","timestamp":1605000718808,"user_tz":-120,"elapsed":671,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"065a4189-777d-43a1-f19f-57d1c479a139","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"log_posteriors\")"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"Y8kMIn4wUNx4","executionInfo":{"status":"ok","timestamp":1605000721798,"user_tz":-120,"elapsed":640,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"e7686eb2-9dd7-4b38-b3b1-30c2ba883323","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(f\"Madison  log posterior: {log_posterior_madison:8.3f}\\n\"\n","      f\"Hamilton log posterior: {log_posterior_hamilton:8.3f}\")"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Madison  log posterior:   -7.482\n","Hamilton log posterior:  -13.610\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"EKGqSSg5UNx7"},"source":["<!-- BEGIN QUESTION -->\n","\n","**Question:** Which one of the two is larger? Does this accord with your expectation?\n","<!--\n","BEGIN QUESTION\n","name: open_response_posterior\n","manual: true\n","-->"]},{"cell_type":"markdown","metadata":{"id":"WM3yB6kAUNx8"},"source":["The log postprior probability of Madison is larger as expected accordingly to the probabilities multiplication we saw before (in which Madison's prob was larger either), according to the fact that a log is a monotonic function, which is used replacing the multiplication term by summarization, profitly to avoid underflow and increase speed."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"HJmNK0o6UNx9"},"source":["<!-- END QUESTION -->\n","\n","<!-- BEGIN QUESTION -->\n","\n","## Lab debrief – for consensus submission only\n","\n","**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n","\n","* Was the lab too long or too short?\n","* Were the readings appropriate for the lab? \n","* Was it clear (at least after you completed the lab) what the points of the exercises were? \n","* Are there additions or changes you think would make the lab better?\n","\n","<!--\n","BEGIN QUESTION\n","name: open_response_debrief\n","manual: true\n","-->"]},{"cell_type":"markdown","metadata":{"id":"TXh3RY-JUNx-"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","metadata":{"id":"csFo_wLzUNx_"},"source":["<!-- END QUESTION -->\n","\n","\n","\n","# End of lab 3"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"M65nth7nUNyA"},"source":["---\n","\n","To double-check your work, the cell below will rerun all of the autograder tests."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"E_NACVDmUNyA"},"source":["grader.check_all()"],"execution_count":null,"outputs":[]}]}