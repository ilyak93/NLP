{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "jupyter": {"outputs_hidden": true, "source_hidden": true}}, "outputs": [], "source": ["# Please do not change this cell because some hidden tests might depend on it.\n", "import os\n", "\n", "# Otter grader does not handle ! commands well, so we define and use our\n", "# own function to execute shell commands.\n", "def shell(commands, warn=True):\n", "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n", "     \n", "       Prints the result to stdout and returns the exit status. \n", "       Provides a printed warning on non-zero exit status unless `warn` \n", "       flag is unset.\n", "    \"\"\"\n", "    file = os.popen(commands)\n", "    print (file.read().rstrip('\\n'))\n", "    exit_status = file.close()\n", "    if warn and exit_status != None:\n", "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n", "    return exit_status\n", "\n", "shell(\"\"\"\n", "ls requirements.txt >/dev/null 2>&1\n", "if [ ! $? = 0 ]; then\n", " rm -rf .tmp\n", " git clone https://github.com/cs236299-2020/lab2-1.git .tmp\n", " mv .tmp/tests ./\n", " mv .tmp/requirements.txt ./\n", " rm -rf .tmp\n", "fi\n", "pip install -q -r requirements.txt\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook()"]}, {"cell_type": "raw", "metadata": {}, "source": ["%%latex\n", "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\newcommand{\\softmax}{\\operatorname{softmax}}\n", "\\newcommand{\\Prob}{\\Pr}\n", "\\newcommand{\\given}{\\,|\\,}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["$$\n", "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n", "\\renewcommand{\\Prob}{\\Pr}\n", "\\renewcommand{\\given}{\\,|\\,}\n", "$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Course 236299\n", "## Lab 2-1 \u2013 Language modeling with n-grams\n", "We turn from tasks that _classify_ texts \u2013 mapping texts into a finite set of classes \u2013 to tasks that _model_ texts by providing a full probability distribution over texts (or providing a similar scoring metric). Such _language models_ attempt to answer the question \"How likely is a token sequence to be generated as an instance of the language?\".\n", "\n", "We'll start with n-gram language models. Given a token sequence $x_1, x_2, \\ldots, x_N$, its probability $\\Prob(x_1, x_2, \\ldots, x_N)$ can be calculated using the chain rule of probability:\n", "\n", "$$\\Prob(A, B \\given \\theta)= \\Prob(A \\given \\theta) \\cdot \\Prob(B \\given A, \\theta) $$\n", "\n", "Thus, \n", "\n", "\\begin{align}\n", "\\Prob(x_1, x_2, \\ldots, x_N) & = \\Prob(x_1) \\cdot \\Prob(x_2, \\ldots, x_N \\given x_1) \\\\\n", "& = \\Prob(x_1) \\cdot \\Prob(x_2 \\given x_1) \\cdot \\Prob(x_3, \\ldots, x_N \\given x_1, x_2) \\\\\n", "& \\cdots \\\\\n", "& = \\prod_{i=1}^N \\Prob (x_i \\given x_1, \\cdots, x_{i-1}) \\\\\n", "& \\approx \\prod_{i=1}^N \\Prob (x_i \\given x_{i-n+1}, \\cdots, x_{i-1})\\tag{1}\n", "\\end{align}\n", "\n", "In the last step, we replace the probability $\\Prob (x_i \\given x_1, \\cdots, x_{i-1})$, which conditions $x_i$ on _all_ of the preceding tokens, with $\\Prob (x_i \\given x_{i-n+1}, \\cdots, x_{i-1})$, which conditions $x_i$ only on the $n-1$ preceding tokens. We call the $n-1$ preceding tokens ($x_{i-n+1}, \\cdots, x_{i-1}$) the _context_ and $x_i$ the _target_. Taken together, these $n$ tokens form an $n$-gram, hence the term _$n$-gram model_.\n", "\n", "In this lab you'll work with $n$-gram models: generating them, sampling from them, and scoring held-out texts according to them. We'll find some problems with $n$-gram models as language models:\n", "\n", "1. They are profligate with memory.\n", "2. They are sensitive to very limited context.\n", "3. They don't generalize well across similar words.\n", "\n", "In the next lab, we'll explore neural models to address these failings."]}, {"cell_type": "markdown", "metadata": {}, "source": ["New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n", "\n", "* [`itertools.product`](https://docs.python.org/3.8/library/itertools.html#itertools.product)\n", "* [`random.random`](https://docs.python.org/3.8/library/random.html#random.random)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preparation \u2013 Loading packages and data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["import itertools\n", "import math\n", "import random\n", "import re\n", "import torchtext as tt\n", "\n", "from collections import defaultdict, Counter\n", "from sys import getsizeof\n", "\n", "# Set random seeds\n", "SEED = 1234\n", "random.seed(SEED)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Some utilities to manipulate the corpus\n", "\n", "def preprocess(text):\n", "    \"\"\"Strips #comments and empty lines from a string\n", "    \"\"\"\n", "    result = []\n", "    for line in text.split(\"\\n\"):\n", "        line = line.strip()              # trim whitespace\n", "        line = re.sub('#.*$', '', line)  # trim comments\n", "        if line != '':                   # drop blank lines\n", "            result.append(line)\n", "    return result\n", "\n", "def geah_tokenize(lines):\n", "    \"\"\"Specialized tokenizer for GEaH corpus handling speaker IDs\"\"\"\n", "    result = []\n", "    for line in lines:\n", "        # tokenize\n", "        tokens = tt.data.get_tokenizer(\"basic_english\")(line)\n", "        # revert the speaker ID token\n", "        if tokens[0] == \"sam\":\n", "            tokens[0] = \"SAM:\"\n", "        elif tokens[0] == \"guy\":\n", "            tokens[0] = \"GUY:\"\n", "        else:\n", "            raise ValueError(\"format problem - bad speaker ID\")\n", "        # add a start of sentence token\n", "        result += [\"<s>\"] + tokens\n", "    return result\n", "                    \n", "def postprocess(tokens):\n", "    \"\"\"Converts `tokens` to a string with one sentence per line\"\"\"\n", "    return ' '.join(tokens)\\\n", "              .replace(\"<s> \", \"\\n\")\n", "\n", "# Read the GEaH data and preprocess into training and test streams of tokens\n", "geah_filename = (\"https://github.com/nlp-236299/data/raw/master/Seuss/\"\n", "                 \"seuss - 1960 - green eggs and ham.txt\")\n", "shell(f'wget -nv -N -P data \"{geah_filename}\"')\n", "\n", "def split(list, portions, offset):\n", "    \"\"\"Splits `list` into a \"large\" and a \"small\" part, returning them as a pair.\n", "    \n", "    The two parts are formed by partitioning `list` into `portions` disjoint pieces.\n", "    The small part is the piece at index `offset`; the large part is the remainder.\n", "    \"\"\"\n", "    return ([list[i] for i in range(0, len(list)) if i % portions != offset],\n", "            [list[i] for i in range(0, len(list)) if i % portions == offset])\n", "\n", "with open(\"data/seuss - 1960 - green eggs and ham.txt\", 'r') as fin:\n", "    lines = preprocess(fin.read())\n", "    train_lines, test_lines = split(lines, 12, 0)\n", "    train_tokens = geah_tokenize(train_lines)\n", "    test_tokens = geah_tokenize(test_lines)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We've already loaded in the text of _Green Eggs and Ham_ for you and split it (about 90%/10%) into two token sequences, `train_tokens` and `test_tokens`. Here's a preview:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(train_tokens[:50])\n", "print(postprocess(train_tokens[:50]))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(test_tokens[:50])\n", "print(postprocess(test_tokens[:50]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We extract the vocabulary from the training text."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Extract vocabulary from dataset\n", "vocabulary = set(train_tokens)\n", "print(vocabulary)"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["## Generating $n$-grams\n", "\n", "The _$n$-grams_ in a text are the contiguous subsequences of $n$ tokens. We will implement them as Python tuples. In theory, any sequence of $n$ tokens is a potential $n$-gram type. Let's generate a list of all the possible $n$-gram types over a vocabulary. Notice how the type/token distinction is useful for talking about $n$-grams, just as it is for words.\n", "<!--\n", "BEGIN QUESTION\n", "name: all_ngrams\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "def all_ngrams(vocabulary, n):\n", "    \"\"\"Returns a list of all the possible `n`-long *tuples* of elements of the `vocabulary`,\n", "    with repetitions, and order matters.\n", "    \n", "    For instance,  \n", "        >>> all_ngrams([\"one\", \"two\"], 3)\n", "        [('one', 'one', 'one'),\n", "         ('one', 'one', 'two'),\n", "         ('one', 'two', 'one'),\n", "         ('one', 'two', 'two'),\n", "         ('two', 'one', 'one'),\n", "         ('two', 'one', 'two'),\n", "         ('two', 'two', 'one'),\n", "         ('two', 'two', 'two')]\n", "         \n", "    Order of returned list is not specified or guaranteed.\n", "    When `n`=0, return [()].\n", "    \"\"\"\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"all_ngrams\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can generate a list of all of the $n$-grams (tokens, not types) in a text."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def ngrams(tokens, n):\n", "    \"\"\"Returns a list of all `n`-grams in a list of `tokens`.\"\"\"\n", "    return [tuple(tokens[i : i + n])\n", "            for i in range(0, len(tokens) - n + 1)]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print (train_tokens[:6])\n", "print (ngrams(train_tokens[:6], 3))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Counting $n$-grams\n", "\n", "We conceptualize an $n$-gram as having two parts:\n", "\n", "* The _context_ is the first $n-1$ tokens in the $n$-gram.\n", "* The _target_ is the final token in the $n$-gram.\n", "\n", "An $n$-gram language model specifies a probability for each $n$-gram type. We'll implement a model as a 2-D dictionary, indexed first by context and then by target, providing the probability for the $n$-gram.\n", "\n", "We start by generating a similar data structure for counting up the $n$-grams in a token sequence."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def ngram_counts(vocabulary, tokens, n):\n", "    \"\"\"Returns a dictionary of counts of the `n`-grams in `tokens`.\n", "    \n", "    The dictionary is structured with first index by (n-1)-gram context\n", "    and second index by the final target token.\n", "    \"\"\"\n", "    context_dict = defaultdict(lambda: defaultdict(int))\n", "    # zero all ngrams\n", "    for context in all_ngrams(vocabulary, n - 1):\n", "        for target in vocabulary:\n", "            context_dict[context][target] = 0\n", "    # add counts for attested ngrams\n", "    for ngram, count in Counter(ngrams(tokens, n)).items():\n", "        context_dict[ngram[:-1]][ngram[-1]] = count\n", "    return context_dict"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Use the `ngram_counts` function to generate count data structures for unigrams, bigrams, and trigrams for the _Green Eggs and Ham_ training text.\n", "<!--\n", "BEGIN QUESTION\n", "name: ngram_counts\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "unigram_counts = ...\n", "bigram_counts = ...\n", "trigram_counts = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"ngram_counts\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check your work by examining the total count of unigrams, bigrams, and trigrams. Do the numbers make sense?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Calculate total counts of tokens, unigrams, bigrams, and trigrams\n", "token_count = len(train_tokens)\n", "unigram_count = sum(len(unigram_counts[cntxt]) for cntxt in unigram_counts)\n", "bigram_count = sum(len(bigram_counts[cntxt]) for cntxt in bigram_counts)\n", "trigram_count = sum(len(trigram_counts[cntxt]) for cntxt in trigram_counts)               \n", "\n", "# Report on the totals\n", "print(f\"Tokens:   {token_count:6}\\n\"\n", "      f\"Unigrams: {unigram_count:6}\\n\"\n", "      f\"Bigrams:  {bigram_count:6}\\n\"\n", "      f\"Trigrams: {trigram_count:6}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["## Calculating $n$-gram probabilities\n", "\n", "We can convert the counts into a probability model by _normalizing_ the counts. Given an $n$-gram type $x_1, x_2, \\ldots, x_n$, instead of storing the count $\\cnt{x_1, x_2, \\ldots, x_n}$, we store an estimate of the probability \n", "\n", "\\begin{align*}\n", "  \\Pr(x_n \\given x_1, x_2, \\ldots, x_{n-1})\n", "  & \\approx \\frac{\\cnt{x_1, x_2, \\ldots, x_n}}{\\cnt{x_1, x_2, \\ldots, x_{n-1}}} \\\\\n", "  & = \\frac{\\cnt{x_1, x_2, \\ldots, x_n}}{\\sum_{x'} \\cnt{x_1, x_2, \\ldots, x_{n-1}, x'}}\n", "\\end{align*}\n", "\n", "that is, the ratio of the count of the $n$-gram and the sum of the counts of all $n$-grams with the same context. Fortunately, all of those counts are already stored in the count data structures we've already built. \n", "\n", "Write a function that takes an $n$-gram count data structure and returns an $n$-gram probability data structure. As with the counts, the probabilities should be stored indexed first by context and then by target.\n", "<!--\n", "BEGIN QUESTION\n", "name: ngram_model\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "def ngram_model(ngram_counts):\n", "    \"\"\"Returns an n-gram probability model calculated by normalizing the \n", "       provided `ngram-counts` dictionary\n", "    \"\"\"\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"ngram_model\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can now build some $n$-gram models \u2013 unigram, bigram, and trigram \u2013 based on the counts."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["unigram_model = ngram_model(unigram_counts)\n", "bigram_model = ngram_model(bigram_counts)\n", "trigram_model = ngram_model(trigram_counts)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Space considerations"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For the most part, we aren't too concerned in this course about matters of time or space efficiency, though these are crucial issues in the engineering of NLP systems. But the size of $n$-gram models merits consideration, looking especially at their size as $n$ grows. We can use Python's `sys.getsizeof` function to get a rough sense of the size of the models we've been working with."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Tokens:   {getsizeof(train_tokens):6}\\n\"\n", "      f\"Unigrams: {getsizeof(unigram_model):6}\\n\"\n", "      f\"Bigrams:  {getsizeof(bigram_model):6}\\n\"\n", "      f\"Trigrams: {getsizeof(trigram_model):6}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** What do these sizes tell you about the memory usage of $n$-gram models? With a larger vocabulary of, say, 10,000 words, would it be practical to run, say, 5-gram models on your laptop?\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_sizes\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "## Sampling from an $n$-gram model\n", "\n", "We have cleverly constructed the models to index by context. This allows us to sample a word given its context. For instance, in the trigram context `(\"<s>\", \"SAM:\")`, the following probability distribution captures which words can come next and with what probability:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["trigram_model[(\"<s>\", \"SAM:\")]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can sample a single token according to this probability distribution. Here's one way to do so."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sample(model, context):\n", "    \"\"\"Returns a token sampled from the `model` assuming the `context`\"\"\"\n", "    distribution = model[context]\n", "    prob_remaining = random.random()\n", "    for token, prob in sorted(distribution.items()):\n", "        if prob_remaining < prob:\n", "            return token\n", "        else:\n", "            prob_remaining -= prob\n", "    raise ValueError"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["We can extend the sampling to a sequence of words by updating the context as we sample each word.\n", "\n", "Define a function `sample_sequence` that performs this sampling of a sequence. It's given a model and a starting context and begins by sampling the first token based on the starting context, then updates the starting context to reflect the word just sampled, repeating the process until a specified number of tokens have been sampled.\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: sample_sequence\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "def sample_sequence(model, start_context, count=100):\n", "    \"\"\"Returns a sequence of `count` tokens sampled successively\n", "       from the `model` *following the `start_context`*.\n", "       The length of the returned list should be `count+len(start_context)`.\n", "    \"\"\"\n", "    random.seed(SEED) # for reproducibility, do not change\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"sample_sequence\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's try it."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(postprocess(sample_sequence(unigram_model, ())))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(postprocess(sample_sequence(bigram_model, (\"<s>\",))))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(postprocess(sample_sequence(trigram_model, (\"<s>\", \"SAM:\"))))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Evaluating text according to an $n$-gram model\n", "\n", "### The probability metric\n", "\n", "The main point of a language model is to assign probabilities (or similar scores) to texts. For $n$-gram models, that's done according to Equation (1) at the start of the lab. Let's implement that. We define a function `probability` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the probability of the token sequence  according to the model. It merely multiplies all of the $n$-gram probabilities for all of the $n$-grams in the token sequence.\n", "\n", "> Throughout this lab, we ignore the scores of the first n-1 tokens as our n-gram model cannot score them due to the lack of context. In the next lab you will see how to solve this issue in practice."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def probability(tokens, model, n):\n", "    \"\"\"Returns the probability of a sequence of `tokens` according to an\n", "       `n`-gram `model`\n", "    \"\"\"\n", "    score = 1.0\n", "    context = tokens[0:n-1]\n", "    # Ignores the scores of the first n-1 tokens\n", "    for token in tokens[n-1:]:\n", "        prob = model[tuple(context)][token]\n", "        score *= prob\n", "        context = (context + [token])[1:]\n", "    return score"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We test it on the test text that we held out from the training text."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Test probability - unigram: {probability(test_tokens, unigram_model, 1):6e}\\n\"\n", "      f\"Test probability -  bigram: {probability(test_tokens, bigram_model, 2):6e}\\n\"\n", "      f\"Test probability - trigram: {probability(test_tokens, trigram_model, 3):6e}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["### The negative log probability metric\n", "\n", "Yikes, those probabilities are _really small_. Multiplying all those small numbers is likely to lead to underflow. \n", "\n", "To solve the underflow problem, we'll do our usual trick of using negative log probabilities \n", "\n", "$$ - \\log_2 \\left(\\prod_{i=1}^N \\Prob (x_i \\given x_{i-n+1}, \\cdots, x_{i-1})\\right)$$\n", "\n", "instead of probabilities.\n", "\n", "Define a function `neglogprob` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the negative log probability of the token sequence according to the model, calculating it in such a way as to avoid underflow. (You'll want to simplify the formula above before implementing it.)\n", "\n", "> Be careful when confronting zero probabilities. Taking `-math.log2(0)` raises a \"Math domain error\". Instead, you should use `math.inf` (Python's representation of infinity) as the value for the negative log of zero. This accords with our understanding that an impossible event would require infinite bits to specify.\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: neglogprob\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "def neglogprob(tokens, model, n):\n", "    \"\"\"Returns the negative log probability of a sequence of `tokens`\n", "       according to an `n`-gram `model`\n", "    \"\"\"\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"neglogprob\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We compute the negative log probabilities of the test text using the different models and report on them."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["unigram_test_nlp = neglogprob(test_tokens, unigram_model, 1)\n", "bigram_test_nlp = neglogprob(test_tokens, bigram_model, 2)\n", "trigram_test_nlp = neglogprob(test_tokens, trigram_model, 3)\n", "\n", "print(f\"Test neglogprob - unigram: {unigram_test_nlp:6f}\\n\"\n", "      f\"Test neglogprob -  bigram: {bigram_test_nlp:6f}\\n\"\n", "      f\"Test neglogprob - trigram: {trigram_test_nlp:6f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There, those numbers seem more manageable. We can even convert the neglogprobs back into probabilities as a sanity check."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Test probability - unigram: {2 ** (-unigram_test_nlp):6e}\\n\"\n", "      f\"Test probability -  bigram: {2 ** (-bigram_test_nlp):6e}\\n\"\n", "      f\"Test probability - trigram: {2 ** (-trigram_test_nlp):6e}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** Why does the bigram model assign a lower neglogprob (that is, a higher probability) to the test text than the unigram model? Why does the trigram model assign a higher neglogprob (lower probability) to the test text than the other models?\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_ordering\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "### The perplexity metric\n", "\n", "Another metric that is commonly used is _perplexity_. Jurafsky and Martin give a definition for perplexity as the \"inverse probability of the test set normalized by the number of words\":\n", "\n", "$$ PP(x_1, x_2, \\ldots, x_N) = \n", "     \\sqrt[N]{\\frac{1}{\\prod_{i=1}^N \\Prob (x_i \\given x_{i-n+1}, \\cdots, x_{i-1})}}\n", "$$\n", "\n", "Define a function `perplexity` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the perplexity of the token sequence according to the model, calculating it in such a way as to avoid underflow. (By now you're smart enough to realize that you'll want to carry out most of that calculation inside a $\\log$.)\n", "\n", "> Remember that we ignored the scores of the first n-1 tokens, what should the number of words `N` be?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: perplexity\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "def perplexity(tokens, model, n):\n", "    \"\"\"Returns the perplexity of a sequence of `tokens` according to an\n", "       `n`-gram `model`\n", "    \"\"\"\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"perplexity\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can look at the perplexity of the test sample according to each of the models."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Test perplexity - unigram: {perplexity(test_tokens, unigram_model, 1):.3e}\\n\"\n", "      f\"Test perplexity -  bigram: {perplexity(test_tokens, bigram_model, 2):.3e}\\n\"\n", "      f\"Test perplexity - trigram: {perplexity(test_tokens, trigram_model, 3):.3e}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A perplexity value of $P$ can be interpreted as a measure of a model's average uncertainty in selecting each word equivalent to selecting among $P$ equiprobable words on average. The bigram model gives a perplexity of less than 3, indicating that at each word in the sentence, the model is acting as if selecting among (slightly less than) three equiprobable words.\n", "\n", "For comparison, state of the art $n$-gram language models for more representative English text achieve perplexities of about 250."]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "## Consensus section: Smoothing $n$-gram language models\n", "\n", "> **This section is more open-ended in nature and need only be turned in for the consensus submission of the lab.**\n", "\n", "The models we've been using have lots of zero-probability $n$-grams. Essentially any $n$-gram that doesn't appear on the training text is imputed a probability of zero, which means that any sentence that contains that $n$-gram will also be given a zero probability. Clearly this is not an accurate estimate.\n", "\n", "There are many ways to _smooth_ $n$-gram models, just as you smoothed classification models in earlier labs. The simplest is probably add-$\\delta$ smoothing. \n", "\n", "$$ \\Prob(x_i \\given x_1 \\ldots x_{i-1})\n", "  \\approx \\frac{\\cnt{x_1, x_2, \\ldots, x_n} + \\delta}{\\cnt{x_1, x_2, \\ldots, x_{n-1}} + \\delta \\cdot |V|}\n", "$$\n", "\n", "Another useful method is to interpolate multiple $n$-gram models, for instance, estimating probabilities as an interpolation of trigram, bigram, and unigram models.\n", "\n", "$$ \\Prob(x_i \\given x_1 \\ldots x_{i-1}) \\approx\n", "     \\lambda_2 \\Prob(x_i \\given x_{i-2}, x_{i-1}) \n", "     + \\lambda_1 \\Prob(x_i \\given x_{i-1}) \n", "     + (1 - \\lambda_1 - \\lambda_2) \\Prob(x_i)\n", "$$\n", "\n", "Finally, a method called _backoff_ uses higher-order $n$-gram probabilities where available, \"backing off\" to lower order where necessary.\n", "\n", "$$\n", "\\Prob(x_i \\given x_1 \\ldots x_{i-1}) \\approx \\begin{cases}\n", "    \\Prob(x_i \\given x_{i-2}, x_{i-1}) & \\mbox{if $\\Prob(x_i \\given x_{i-2}, x_{i-1}) > 0$}\\\\\n", "    \\Prob(x_i \\given x_{i-1})          & \\mbox{if $\\Prob(x_i \\given x_{i-2}, x_{i-1}) = 0$ and $\\Prob(x_i \\given x_{i-1}) > 0$}\\\\\n", "    \\Prob(x_i)                         & \\mbox{otherwise}\n", "  \\end{cases}\n", "$$\n", "\n", "Define a function `ngram_model_smoothed`, like the `ngram_model` function from above, but implementing one of these smoothing methods. Compare its perplexity on some sample text to the unsmoothed model. \n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_smoothed_model\n", "manual: true\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "#TODO\n", "Place your definition of `ngram_model_smoothed` and whatever other testing \n", "of it you'd like to do in this and subsequent cells.\n", "\"\"\"\n", "..."]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "<!-- BEGIN QUESTION -->\n", "\n", "## Lab debrief \u2013 for consensus submission only\n", "\n", "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n", "\n", "* Was the lab too long or too short?\n", "* Were the readings appropriate for the lab? \n", "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n", "* Are there additions or changes you think would make the lab better?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_debrief\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "# End of Lab 2-1"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["---\n", "\n", "To double-check your work, the cell below will rerun all of the autograder tests."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check_all()"]}], "metadata": {"celltoolbar": "Edit Metadata", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.3"}, "title": "CS187 Lab 2-1: Language modeling with n-grams"}, "nbformat": 4, "nbformat_minor": 4}