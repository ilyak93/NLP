{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "jupyter": {"outputs_hidden": true, "source_hidden": true}}, "outputs": [], "source": ["# Please do not change this cell because some hidden tests might depend on it.\n", "import os\n", "\n", "# Otter grader does not handle ! commands well, so we define and use our\n", "# own function to execute shell commands.\n", "def shell(commands, warn=True):\n", "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n", "     \n", "       Prints the result to stdout and returns the exit status. \n", "       Provides a printed warning on non-zero exit status unless `warn` \n", "       flag is unset.\n", "    \"\"\"\n", "    file = os.popen(commands)\n", "    print (file.read().rstrip('\\n'))\n", "    exit_status = file.close()\n", "    if warn and exit_status != None:\n", "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n", "    return exit_status\n", "\n", "shell(\"\"\"\n", "ls requirements.txt >/dev/null 2>&1\n", "if [ ! $? = 0 ]; then\n", " rm -rf .tmp\n", " git clone https://github.com/cs236299-2020/lab1-2.git .tmp\n", " mv .tmp/tests ./\n", " mv .tmp/requirements.txt ./\n", " rm -rf .tmp\n", "fi\n", "pip install -q -r requirements.txt\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Course 236299\n", "## Lab 1-2 \u2014 Text classification and evaluation methodology"]}, {"cell_type": "markdown", "metadata": {}, "source": ["After this lab, you should be able to\n", "\n", "* Understand the distinction between training and test corpora, and why both are needed;\n", "* Understand the role of gold labels;\n", "* Implement a majority class baseline as a benchmark to compare other methods;\n", "* Implement nearest neighbor classification, and understand the role of distance metrics in its operation;\n", "* Compare multiple methods for acccuracy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful, include\n", "\n", "* [`collections.Counter`](https://docs.python.org/3/library/collections.html#collections.Counter)\n", "* [`collections.Counter.most_common`](https://docs.python.org/3/library/collections.html#collections.Counter.most_common)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preparation \u2013 Loading packages and data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Please do not change these imports because some hidden tests might depend on them.\n", "# You can add a cell below if you need to import anything else.\n", "import collections\n", "import json\n", "import math\n", "import numpy as np\n", "from pprint import pprint\n", "import re"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## The Federalist Papers\n", "\n", "<img src=\"https://github.com/nlp-236299/data/raw/master/Federalist/federalist.jpg\" width=150 align=right />\n", "\n", "The _Federalist_ papers is a collection of 85 essays written pseudonymously by Alexander Hamilton, John Jay, and James Madison following the Constitutional Convention of 1787, promoting the ratification of the nascent United States Constitution.\n", "\n", "The authorship of many of the individual papers has been well established and acknowledged by the various authors, but a number of the papers have been contentious, with both Madison and Hamilton as possible authors. Determining the authorship of these disputed papers is a classic text classification problem, and one that has received great attention. The seminal work on the problem is that of [Mosteller and Wallace](http://www.historyofinformation.com/detail.php?entryid=4799), who applied then-novel statistical methods to the problem. In this lab, we'll use the _Federalist_ data to experiment with some of the ideas about distance metrics and classification methods that you've read about.\n", "\n", "Mosteller and Wallace used the frequencies of various words in the papers as the raw data for determining authorship. We've provided access to a heavily pre-digested version of this data. (If you're interested, you can find the raw data \u2013 all 85 papers \u2013 and the notebook used to generate the pre-digested data in the [course `data` github repository](https://github.com/nlp-236299/data).)\n", "\n", "Start by evaluating the cells below to load the data and view a sample."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Read the Federalist data from the json file\n", "shell('wget -nv -N -P data https://github.com/nlp-236299/data/raw/master/Federalist/federalist_data.json')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with open('data/federalist_data.json', 'r') as fin:\n", "    dataset = json.load(fin)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# View a sample of the data\n", "print(f\"Number of papers in the dataset: {len(dataset)}\")\n", "print(\"Some examples:\")\n", "pprint(dataset[:3])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You'll see above that the dataset is a list of _examples_, one for each paper, each a dictionary providing the paper number, its title and author(s), and the raw counts for a few important words in the papers. (From the last lab, you'll recognize the `counts` field as a bag-of-words representation of the document.) The `counts` field is the document representation that we will be wanting to classify, and the `authors` field contains the pertinent class label for each example. \n", "\n", "For your reference, here are the words that were used to derive the counts:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["keywords = ['on', 'upon', 'there', 'whilst']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Thus in the first example paper, _Federalist 1_, there were 9 tokens of \"on\", 6 of \"upon\", 2 of \"there\", and none of \"whilst\". \n", "\n", "The `authors` field takes on various values. Here's a table of the frequency of each of the values. (This will come in handy later.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Generate a table of the number of papers by each author label\n", "cnt = collections.Counter(map(lambda ex: ex['authors'],\n", "                              dataset))\n", "for author, count in cnt.items():\n", "    print(f\"{count:3d} ({count/len(dataset):.3f}%) {author}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you can see, some of the papers are of known authorship by Madison or Hamilton. We can use these as training data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Extract the papers by either of Madison and Hamilton\n", "training = list(filter(lambda ex: ex['authors'] in ['Madison', 'Hamilton'],\n", "                       dataset))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# View a sample of the training data\n", "print(f\"Number of papers in the dataset: {len(training)}\")\n", "print(\"Some examples:\")\n", "pprint(training[:3])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Others of the papers are of ambiguous authorship. They are shown as having `'Hamilton or Madison'` as author. These will be the elements that we want to test our models on."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Extract the papers of unknown authorship\n", "testing = list(filter(lambda ex: ex['authors'] == 'Hamilton or Madison',\n", "                      dataset))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# View a sample of the data\n", "print(f\"Number of papers in the dataset: {len(testing)}\")\n", "print(\"Some sample elements:\")\n", "pprint(testing[:3])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Models for text classification"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["We can think of a _model_ for a text classification problem as a function taking a test example and returning a class label for the test example. Generating the model will rely on a corpus of training data.\n", "\n", "With a model in hand, we can evaluate its _accuracy_ on a test corpus by computing the proportion of test examples that the model correctly classifies. Define a higher-order function `accuracy` that takes a test corpus (like `testing`) and a model (which is a function, remember), and returns the accuracy of the model on that corpus. \n", "\n", "> `accuracy` is a _higher-order function_ since it _takes a function as an argument_. Yes, [higher-order functions are possible in Python](https://en.wikipedia.org/wiki/Higher-order_function#Python).\n", "<!--\n", "BEGIN QUESTION\n", "name: accuracy\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "def accuracy(test_corpus, model):\n", "    ..."]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["### Majority class classification\n", "\n", "An especially simple classification model labels each test example with whichever label happens to occur most frequently in the training data. It completely ignores the test example that it classifies!\n", "\n", "By examination of the table provided above, what is the majority class label for this dataset?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: maj_class_label\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "maj_class_label = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"maj_class_label\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Define a function `majority_class_label` that returns the majority class label for a training set.\n", "<!--\n", "BEGIN QUESTION\n", "name: majority_class_label\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "def majority_class_label(training):\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"majority_class_label\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["What proportions of the **training** examples would be classified correctly by the majority class model?\n", "<!--\n", "BEGIN QUESTION\n", "name: maj_class_accuracy_guess\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "maj_class_accuracy_guess = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"maj_class_accuracy_guess\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Now define a function `majority_class` that takes a single argument, a test example, and returns the class label that is most frequent in the training data `training` (regardless of what the test example is).\n", "<!--\n", "BEGIN QUESTION\n", "name: majority_class\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO - define the `majority_class` model\n", "def majority_class(example):\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"majority_class\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Now we can see how well this majority class model works by trying it out on some examples. Use the `accuracy` function to determine the model's accuracy when applied to the task of labeling the _training_ data.\n", "<!--\n", "BEGIN QUESTION\n", "name: accuracy_maj_class_train\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO - define `maj_class_on_train` to be the accuracy of the majority class model on the training data\n", "accuracy_maj_class_train = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"accuracy_maj_class_train\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Accuracy of the majority class model on training data: {accuracy_maj_class_train:.3f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Was your estimation from above right?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Nearest neighbor classification\n", "\n", "Recall that nearest neighbor classification classifies a test example with the label of the nearest training example. To calculate nearest neighbors, we need a distance metric between the representations of the documents. Below we've provided two such metrics, familiar from the previous lab, for Euclidean distance and cosine distance."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def euclidean_distance(v1, v2):\n", "    '''Returns the Euclidean distance between two vectors'''\n", "    return np.linalg.norm(np.array(v1) - np.array(v2))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def safe_acos(x):\n", "    '''Returns the arc cosine of `x`. Unlike `math.acos`, it \n", "       does not raise an exception for values of `x` out of range, \n", "       but rather clips `x` at -1..1, thereby avoiding math domain\n", "       errors in the case of numerical errors.'''\n", "    return math.acos(math.copysign(min(1.0, abs(x)), x))\n", "        \n", "def cosine_distance(v1, v2):\n", "    '''Returns the cosine distance between two vectors'''\n", "    return (safe_acos(np.dot(v1, v2) \n", "                      / (np.linalg.norm(v1, 2) * np.linalg.norm(v2, 2)))\n", "            / math.pi)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Generating nearest neighbor models"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["To specify a nearest neighbor model, we need both a training corpus (like `training`) and a distance metric (like `euclidean_distance` or `cosine_distance` defined just above). \n", "\n", "Define a function called `define_nearest_neighbor` that takes a training corpus and a metric and returns a model - that is, a **function** that classifies a single test example. The model should return the class label of that training example whose counts vector is closest to that of the test example according to the metric.\n", "\n", "> Again, `define_nearest_neighbor` is a higher-order function since it _returns a function_. \n", "<!--\n", "BEGIN QUESTION\n", "name: define_nearest_neighbor\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "def define_nearest_neighbor(corpus, metric):\n", "    ..."]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can use the `define_nearest_neighbor` function to define two new models for nearest neighbor classification, one using Euclidean distance and one using cosine distance."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nearest_neighbor_euclidean_model = \\\n", "    define_nearest_neighbor(training, euclidean_distance)\n", "\n", "nearest_neighbor_cosine_model = \\\n", "    define_nearest_neighbor(training, cosine_distance)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Testing the nearest neighbor models on the training data"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["How accurate are these models when used to label the training data (as we did for the majority class model above)? Use the `accuracy` function above to calculate the accuracy of `nearest_neighbor_euclidean_model` in labeling the _training_ data (not the test data), and similarly for `nearest_neighbor_cosine_model`.\n", "<!--\n", "BEGIN QUESTION\n", "name: accuracy_train\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO - define the variable to be the calculated accuracy \n", "accuracy_nn_euclidean_train = ...\n", "accuracy_nn_cosine_train = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"accuracy_train\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Accuracy of the nearest neighbor euclidean model tested on training data: \"\n", "      f\"{accuracy_nn_euclidean_train:.3f}\")\n", "print(f\"Accuracy of the nearest neighbor cosine model tested on training data: \"\n", "      f\"{accuracy_nn_cosine_train:.3f}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** Does the performance of these classifiers on the training data seem to you to be representative of how good a classifier each is? Why or why not?\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_1\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "#### Testing the nearest neighbor models on the testing data"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["To get a better sense of how the nearest neighbor models perform, let's try them out on the testing data that we have. (Recall that the testing data in `testing` were the ambiguously-authored Federalist papers, where the `authors` field was `'Hamilton or Madison'`.)\n", "\n", "We start by looking in detail at the predictions generated by the two nearest neighbor models. Print out a table that lists, for each `testing` example, the paper number and the authors predicted under the nearest neighbor Euclidean model and the nearest neighbor cosine model.\n", "<!--\n", "BEGIN QUESTION\n", "name: print_table\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO - print out the requested table\n", "..."]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "What do you notice about the two models?\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_2\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "#### Accuracy on the testing corpus\n", "\n", "Now use the `accuracy` function to calculate the accuracy of the two nearest neighbor models as you did above, but this time calculating accuracy on the testing corpus rather than the training corpus. (Expect to find a surprising result. Read ahead for an explanation if you're confused.)\n", "<!--\n", "BEGIN QUESTION\n", "name: accuracy_test\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO - define the variables to be the calculated accuracy of the nearest \n", "# neighbor Euclidean model and the cosine model on the testing data\n", "accuracy_nn_euclidean_test = ...\n", "accuracy_nn_cosine_test = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"accuracy_test\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Accuracy of the nearest neighbor euclidean model tested on testing data: \"\n", "      f\"{accuracy_nn_euclidean_test:.3f}\")\n", "print(f\"Accuracy of the nearest neighbor cosine model tested on testing data: \"\n", "      f\"{accuracy_nn_cosine_test:.3f}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** Does the performance of these classifiers on the testing data seem to you to be representative of how good a classifier each is? Why or why not?\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_3\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "#### The importance of gold labels\n", "\n", "In order to evaluate the accuracy of the nearest neighbor model \u2013 and any model \u2013 we need to have the true labels for the testing corpus, the so-called _gold_ labels. What shall we use for gold labels? Mosteller and Wallace's much more extensive analysis concluded that all of the papers of ambiguous origin were penned by Madison, so we'll use that. We should modify the `testing` corpus to inject the gold labels. \n", "\n", "Write some code to update the testing corpus with the gold labels.\n", "<!--\n", "BEGIN QUESTION\n", "name: get_gold\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO - write code to update the testing corpus with the gold labels\n", "..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"get_gold\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can rerun the accuracy calculations."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accuracy_nn_euclidean_test_with_gold = accuracy(testing, nearest_neighbor_euclidean_model)\n", "accuracy_nn_cosine_test_with_gold = accuracy(testing, nearest_neighbor_cosine_model)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Accuracy of the nearest neighbor euclidean model tested on testing data: \"\n", "      f\"{accuracy_nn_euclidean_test_with_gold:.3f}\")\n", "print(f\"Accuracy of the nearest neighbor cosine model tested on testing data: \"\n", "      f\"{accuracy_nn_cosine_test_with_gold:.3f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Do these results make more sense?"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "## Lab debrief \u2013 for consensus submission only\n", "\n", "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n", "\n", "* Was the lab too long or too short?\n", "* Were the readings appropriate for the lab? \n", "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n", "* Are there additions or changes you think would make the lab better?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_debrief\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "# End of lab 1-2"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["---\n", "\n", "To double-check your work, the cell below will rerun all of the autograder tests."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check_all()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.3"}, "title": "CS187 Lab 1-2: Text classification and evaluation methodology"}, "nbformat": 4, "nbformat_minor": 4}