{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "jupyter": {"outputs_hidden": true, "source_hidden": true}}, "outputs": [], "source": ["# Please do not change this cell because some hidden tests might depend on it.\n", "import os\n", "\n", "# Otter grader does not handle ! commands well, so we define and use our\n", "# own function to execute shell commands.\n", "def shell(commands, warn=True):\n", "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n", "     \n", "       Prints the result to stdout and returns the exit status. \n", "       Provides a printed warning on non-zero exit status unless `warn` \n", "       flag is unset.\n", "    \"\"\"\n", "    file = os.popen(commands)\n", "    print (file.read().rstrip('\\n'))\n", "    exit_status = file.close()\n", "    if warn and exit_status != None:\n", "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n", "    return exit_status\n", "\n", "shell(\"\"\"\n", "ls requirements.txt >/dev/null 2>&1\n", "if [ ! $? = 0 ]; then\n", " rm -rf .tmp\n", " git clone https://github.com/cs236299-2020/lab2-4.git .tmp\n", " mv .tmp/tests ./\n", " mv .tmp/requirements.txt ./\n", " rm -rf .tmp\n", "fi\n", "pip install -q -r requirements.txt\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook()"]}, {"cell_type": "raw", "metadata": {"jupyter": {"source_hidden": true}}, "source": ["%%latex\n", "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\newcommand{\\softmax}{\\operatorname{softmax}}\n", "\\newcommand{\\Prob}{\\Pr}\n", "\\newcommand{\\given}{\\,|\\,}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["$$\n", "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n", "\\renewcommand{\\Prob}{\\Pr}\n", "\\renewcommand{\\given}{\\,|\\,}\n", "$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Course 236299\n", "## Lab 2-4 \u2013 Sequence labeling with hidden Markov models"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["import math\n", "from collections import defaultdict"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Hidden Markov models (HMM) are a fundamental generative method for sequence labeling NLP tasks such as part-of-speech tagging (as in the present lab) and information extraction (as in the second project segment). In this lab, you'll train, apply, and evaluate some simple sequence labeling algorithms culminating in HMM.\n", "\n", "To keep things manageable, the dataset you'll use will involve very few word types, only six (plus a special beginning of sentence token), but these are quite ambiguous with regard to part of speech. We'll use the following codes for parts of speech:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parts_of_speech = [\n", "    \"<bos>\", # beginning of sentence marker\n", "    \"N\",     # noun\n", "    \"V\",     # main verb\n", "    \"M\",     # modal verb\n", "    \"P\",     # preposition\n", "    \"A\",     # adjective\n", "    \"R\"      # adverb\n", "]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The vocabulary of word types, along with their parts of speech, is given by the following dictionary:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vocabulary = {\n", "    \"<bos>\":   [\"<bos>\"],\n", "    \"can\":     [\"N\", \"V\", \"M\"],\n", "    \"canned\":  [\"A\", \"V\"],\n", "    \"canners\": [\"N\"],\n", "    \"fish\":    [\"N\", \"V\"],\n", "    \"for\":     [\"P\"],\n", "    \"not\":     [\"R\"]\n", "}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here are a few sentences constructed with these words:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["text = \"\"\"\n", "    <bos> canners canned fish\n", "    <bos> can canners can fish\n", "    <bos> fish can not fish\n", "    <bos> can fish can fish can\n", "    <bos> canners fish fish for can\n", "    <bos> canners can fish for fish\n", "    <bos> canners fish for fish\n", "    <bos> fish can canned fish\n", "    <bos> canners can not can canned fish\n", "    <bos> fish can can fish for canners\n", "\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["and the corresponding POS sequences:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["text_pos = \"\"\"\n", "    <bos> N V N\n", "    <bos> M N V N\n", "    <bos> N M R V\n", "    <bos> M N V N N\n", "    <bos> N V N P N\n", "    <bos> N M V P N\n", "    <bos> N V P N\n", "    <bos> N V A N\n", "    <bos> N M R V A N\n", "    <bos> N M V N P N\n", "\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["We tokenize the sentences and label the tokens with their POS."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tokenize(text):\n", "  result = []\n", "  for line in text.strip().split(\"\\n\"):\n", "    result.append([item for item in line.strip().split()])\n", "  return result\n", "\n", "tagged_text = [list(zip(sentence, poses))\n", "                 for sentence, poses \n", "                   in zip(tokenize(text), tokenize(text_pos))]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here are a couple of examples to indicate what the tagged sentences look like."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(tagged_text[0])\n", "print(tagged_text[1])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For reference, here is a table showing the frequency distribution for each word type and each part of speech it can be used as."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["counts = defaultdict(lambda: defaultdict(int))\n", "for sentence in tagged_text:\n", "  for type, pos in sentence:\n", "    counts[type][pos] += 1\n", "\n", "print(f'{\"TYPE\":8} {\"POS\":6} {\"COUNT\"}')\n", "for type, type_counts in counts.items():\n", "  for pos, count in type_counts.items():\n", "    print(f'{type:8} {pos:6} {count:2}')"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["## Majority label\n", "The first sequence labeling method we'll use is simply to choose for each word the POS label it most frequently occurs as in the training data. The table above provides the required information directly.\n", "\n", "Choosing the majority label for a word sequence $\\vect{x} = \\langle{x_1, x_2, \\ldots, x_m}\\rangle$ is tantamount to maximizing the probability of the label sequence assuming independence of the label conditioned on the word, that is, selecting the tag sequence $\\vect{t} = \\langle{t_1, t_2, \\ldots, t_m}\\rangle$ given by\n", "$$ \\argmax{\\vect{t}} \\prod_{i=1}^m \\Prob(t_i \\given x_i) $$\n", "\n", "How would the majority label method label the following test sentence (which we've marked with the words' correct (\"gold\") parts of speech)?\n", "\n", "> \\<bos\\>[\\<bos\\>] canners[N] can[V] canned[A] fish[N]\n", "\n", "Write a function that receives a list of words, and returns a list of the corresponding POS tags.\n", "<!--\n", "BEGIN QUESTION\n", "name: example_majority_labeling\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "text = '<bos> canners can canned fish'\n", "\n", "def pos_label_majority(input_seq):\n", "    pos_tags = []\n", "    \n", "    return pos_tags\n", "\n", "example_majority_labeling = pos_label_majority(text.split())"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"example_majority_labeling\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["What is the accuracy of the majority labeling, given as a proportion of the words?\n", "<!--\n", "BEGIN QUESTION\n", "name: example_maj_label_accuracy\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "gold_example_pos_labels = [\"<bos>\", \"N\", \"V\", \"A\", \"N\"]\n", "example_maj_label_accuracy = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"example_maj_label_accuracy\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["## Majority bigram labeling\n", "\n", "It may occur to you that what part of speech a word has _depends on its context_. Suppose we relax the assumption that tag probabilities depend only on the word being tagged, and condition them on the previous word as well. (For the first word in the sentence, we'll condition on that fact, by conditioning it on the special start token.) In summary, we'll condition on the bigram that ends at the word being tagged:\n", "$$ \\argmax{\\vect{t}} \\prod_{i=1}^m \\Prob(t_i \\given x_{i-1} x_i) $$\n", "\n", "What is the majority bigram labeling of the test sentence? Again, give your answer in the form of a list of strings for the POS labels.\n", "You can either write a function that computes the answer or find the answer manually.\n", "<!--\n", "BEGIN QUESTION\n", "name: example_majority_bigram_labeling\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "example_majority_bigram_labeling = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"example_majority_bigram_labeling\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["What is the accuracy of the majority bigram labeling, given as a proportion of the words?\n", "<!--\n", "BEGIN QUESTION\n", "name: example_maj_bigram_label_accuracy\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "gold_example_pos_labels = [\"<bos>\", \"N\", \"V\", \"A\", \"N\"]\n", "example_maj_bigram_label_accuracy = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"example_maj_bigram_label_accuracy\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Hidden Markov models\n", "\n", "Now we get to the real point, using an HMM model. Recall that in an HMM model, we assume that the joint tag/word sequence is generated by \n", "\n", "1. Selecting a tag sequence according to a Markov model whose states correspond to tags and whose transitions from state $t_i$ to $t_j$ are governed by a _transition probability_ $a_{ij} = \\Prob(t_i \\rightarrow t_j)$, and then\n", "2. Selecting a word sequence from the tag sequence where for tag $t_i$ we observe word $x_i$ of type $w_j$ governed by an _emission probability_ $b_{i}(w_j) = \\Prob(t_i \\rightarrow w_j)$.\n", "\n", "### Estimating the transition and emission probabilities\n", "\n", "We estimate these transition and emission probabilities by looking at the empirical probabilities in the training data, counting and perhaps smoothing as usual. That is, for the (unsmoothed) transition probabilities, we estimate\n", "$$ a_{ij} \\approx \\frac{\\cnt{t_i \\rightarrow t_j}}{\\cnt{t_i}} $$\n", "and for the emission probabilities\n", "$$ b_i(w_j) \\approx \\frac{\\cnt{t_i \\rightarrow w_j}}{\\cnt{t_i}} $$\n", "\n", "For instance, we note that there are 4 times in the training data where the tag $N$ is followed by the tag $M$, out of the 21 occurrences of the tag $N$. Thus, we estimate the corresponding transition probability $a_{NM} \\approx 4/21$.\n", "\n", "\n", "Similarly, the emission probability $b_M(can)$ for tag $M$ generating the word $can$ is $6/6 = 1$, since every occurrence of the tag $M$ corresponds to the word $can$ in the training data.\n", "\n", "For your convenience, we've computed and provided full tables for the transition and emission probabilities below."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Generate counts\n", "bigram_tag_counts = defaultdict(lambda: defaultdict(int))\n", "unigram_tag_counts = defaultdict(int)\n", "tag_word_counts = defaultdict(lambda: defaultdict(int))\n", "tag_counts = defaultdict(int)\n", "\n", "for sentence in tagged_text:\n", "    for (w1, t1), (w2, t2) in list(zip(sentence, sentence[1:])):\n", "        bigram_tag_counts[t1][t2] += 1\n", "        unigram_tag_counts[t1] += 1\n", "    for w, t in sentence:\n", "        tag_word_counts[t][w] += 1\n", "        tag_counts[t] += 1\n", "\n", "# Generate transition and emission probabilities\n", "a = defaultdict(lambda: defaultdict(int))\n", "b = defaultdict(lambda: defaultdict(int))\n", "\n", "for t1 in parts_of_speech:\n", "    for t2 in parts_of_speech:\n", "        a[t1][t2] = bigram_tag_counts[t1][t2] / unigram_tag_counts[t1]\n", "    for w1 in vocabulary.keys():\n", "        b[t1][w1] = tag_word_counts[t1][w1] / tag_counts[t1]\n", "\n", "# Print tables of probabilities\n", "\n", "print(\"Transition probabilities: a_ij\")\n", "print(f\"{' ':6}\", end=\"\")\n", "for t in parts_of_speech:\n", "    print(f\"{t:>6}\", end=\"\")\n", "print()\n", "for t1 in parts_of_speech:\n", "    print(f\"{t1:<6}\", end=\"\")\n", "    for t2 in parts_of_speech:\n", "        print(f\"{a[t1][t2]:>6.2f}\", end=\"\")\n", "    print(\"\")\n", " \n", "print(\"\\nEmission probabilities: b_i(w_j)\")\n", "print(f\"{' ':6}\", end=\"\")\n", "for w in vocabulary.keys():\n", "    print(f\"{w:>8}\", end=\"\")\n", "print()\n", "for t in parts_of_speech:\n", "    print(f\"{t:<6}\", end=\"\")\n", "    for w in vocabulary.keys():\n", "        print(f\"{b[t][w]:>8.2f}\", end=\"\")\n", "    print()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### An example HMM trellis\n", "\n", "<img src=\"https://github.com/nlp-236299/data/raw/master/Resources/hmm-figure.png\" width=\"75%\" align=right />\n", "\n", "Now consider the HMM generating the example sentence \"canners can canned fish\". The figure at right contains the _trellis_ for the sentence. The horizontal axis corresponds to the words in the sentence, one at a time. The vertical axis corresponds to the states of the HMM (that is, the parts of speech). The gray arrows that connect a tag on the left to a tag on the right correspond to the transition probabilities. The red arrows that connect a tag to a word directly below correspond to the emission probabilities. \n", "\n", "For convenient reference, we've labeled the rows with letters and the columns with numbers so that particular nodes can be referred to as, for example, B1.\n", "\n", "(The red lines are intended to go from a state node to the word node directly below it; for instance, the red line immediately below B1 goes to the word node F1 at the bottom of the figure. Some of those lines are depicted with a crossbar to indicate that they are running \"underneath\" other graphic objects in the figure.)\n", "\n", "We've highlighted two paths through the trellis from the beginning to the end of the sentence, corresponding to different taggings of the sentence:\n", "\n", "1. A0-B1-C2-E3-B4\n", "2. A0-B1-D2-C3-B4\n", "\n", "Answer the following questions about this trellis."]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["What is the path through the trellis corresponding to the _majority bigram labeling_ that you determined above? Give your answer as a string, in the same \"format\" as the path examples in the previous cell.\n", "<!--\n", "BEGIN QUESTION\n", "name: majority_path\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["majority_path = ..."]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["The probability of a path is just the product of the probabilities of all the transitions along the path and all the emissions from nodes in the path to observed words. Use the tables above to calculate the probability of the first highlighted path (A0-B1-C2-E3-B4) by multiplying together the appropriate probabilities. Don't forget the emission probabilities, corresponding to the edges A0-F0, B1-F1, C2-F2, E3-F3, and B4-F4.\n", "<!--\n", "BEGIN QUESTION\n", "name: highlight1_path_probability\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "highlight1_path_probability = ...\n", "highlight1_path_probability"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"highlight1_path_probability\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Do the same for the second highlighted path (A0-B1-D2-C3-B4).\n", "<!--\n", "BEGIN QUESTION\n", "name: highlight2_path_probability\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "highlight2_path_probability = ...\n", "highlight2_path_probability"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"highlight2_path_probability\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["These two paths turn out to be the two paths through the trellis with the highest probabilities. (You'll have to trust us.) \n", "Based on that fact, which tagging has the highest probability according to this HMM? \n", "Give your solution as a list of POS tags like `[\"<bos>\", \"A\", \"B\", ...]` (in the same format as you did for `example_majority_labeling` above).\n", "<!--\n", "BEGIN QUESTION\n", "name: example_highest_labeling\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "example_highest_labeling = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"example_highest_labeling\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["By inspection, what is the accuracy of the highest probability HMM labeling, given as a proportion of the words?\n", "<!--\n", "BEGIN QUESTION\n", "name: example_highest_label_accuracy\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "example_highest_label_accuracy = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"example_highest_label_accuracy\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Now, recall the majority bigram labeling, and the path for it that you developed above. What is the probability of that path according to the HMM?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: majority_path_probability\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["majority_path_probability = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"majority_path_probability\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Calculating the highest probability tagging - The Viterbi algorithm\n", "\n", "Above, we merely asserted that the two highlighted paths are the two most probable, so that it was a simple matter to find the highest probability tagging by just comparing the probabilities of those two. But in general there can be a huge number of paths through a trellis such as this."]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** If there are $N$ tags and a sentence of length $M$, how many paths through the HMM trellis will there be (using big-$O$ notation)?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_how_many_paths\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "The Viterbi algorithm, named after famed electrical engineer [Andrew Viterbi](https://en.wikipedia.org/wiki/Andrew_Viterbi), is an efficient dynamic programming algorithm for performing this (otherwise impractical) computation. We'll do the first few steps of the Viterbi algorithm for the example here.\n", "\n", "Given a string of words $\\vect{x} = \\langle x_0, x_1, \\ldots, x_M \\rangle$ and a set of states (tags) $\\vect{q} = \\{q_0, q_1, \\ldots, q_N\\}$, the algorithm works by calculating a series of values $v_i(j)$ where $i$ ranges over the words in the sentence from $1$ to $M$ and $j$ ranges over the tags from $1$ to $N$. For simplicity, we'll assume an extra word and tag at the beginning of the sentence, as above, so $x_0 = \\texttt{<bos>}$ and $q_0 = \\texttt{<bos>}$.\n", "The definition for $v$ then is:\n", "\n", "\\begin{align*}\n", "  v_0(0) &= 1 \\\\\n", "  v_0(j) &= 0  &\\mbox{for $j > 0$} \\\\\n", "  v_i(j) &= \\max_{j'=1}^N v_{i-1}(j') \\cdot a_{j' j} \\cdot b_{j}(x_i)\n", "         & \\mbox{for $i > 0$}\n", "\\end{align*}"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** For the sample sentence above (\"canners can canned fish\"), calculate (at least) the first three \"layers\" of the Viterbi algorithm, that is, $v_0$, $v_1$, and $v_2$, filling in the table below. (We've filled in the zero-th layer for you already, as per the first two lines in the definition of the Viterbi calculation.)\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_viterbi_table\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!--TODO-->\n", "|   | tag     | v_0  \\<bos\\> | v_1 canners | v_2 can | v_3 canned | v_4 fish |\n", "|---|---------|--------------|-------------|---------|------------|----------|\n", "| 0 | \\<bos\\> |    1         |             |         |            |          |\n", "| 1 | N       |    0         |             |         |            |          |\n", "| 2 | V       |    0         |             |         |            |          |\n", "| 3 | M       |    0         |             |         |            |          |\n", "| 4 | P       |    0         |             |         |            |          |\n", "| 5 | A       |    0         |             |         |            |          |\n", "| 6 | R       |    0         |             |         |            |          |"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "<!-- BEGIN QUESTION -->\n", "\n", "Doing this calculation by hand is painful, but it should make clear what's going on. At each point $v_i(j)$ in the table, we've calculated the probability of the best path through the trellis from the beginning of the sentence to the current word  $x_i$, starting in the start state and ending in the current state $q_j$. To get the maximum probability of all paths in the trellis for the full sentence ending in any state, we merely look up the maximum value of $v_M(j)$.\n", "\n", "**Question:** What is the complexity of filling in all of the entries in the Viterbi table? How does that compare with the complexity of the total number of paths through the trellis that you calculated above?\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_how_many_paths_viterbi\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "<!-- BEGIN QUESTION -->\n", "\n", "## Lab debrief \u2013 for consensus submission only\n", "\n", "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n", "\n", "* Was the lab too long or too short?\n", "* Were the readings appropriate for the lab? \n", "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n", "* Are there additions or changes you think would make the lab better?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_debrief\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "# End of lab 2-4"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["---\n", "\n", "To double-check your work, the cell below will rerun all of the autograder tests."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check_all()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.3"}, "title": "CS187 Lab 2-4: Sequence labeling with hidden Markov models"}, "nbformat": 4, "nbformat_minor": 4}