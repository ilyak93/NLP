{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of lab4-3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "title": "CS187 Lab 4-4: Sequence-to-sequence models with attention",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "id": "5VO3GmvPMDaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32509dbd-fbef-450a-acf6-17ed6c1a96a1"
      },
      "source": [
        "# Please do not change this cell because some hidden tests might depend on it.\n",
        "import os\n",
        "\n",
        "# Otter grader does not handle ! commands well, so we define and use our\n",
        "# own function to execute shell commands.\n",
        "def shell(commands, warn=True):\n",
        "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
        "     \n",
        "       Prints the result to stdout and returns the exit status. \n",
        "       Provides a printed warning on non-zero exit status unless `warn` \n",
        "       flag is unset.\n",
        "    \"\"\"\n",
        "    file = os.popen(commands)\n",
        "    print (file.read().rstrip('\\n'))\n",
        "    exit_status = file.close()\n",
        "    if warn and exit_status != None:\n",
        "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
        "    return exit_status\n",
        "\n",
        "shell(\"\"\"\n",
        "ls requirements.txt >/dev/null 2>&1\n",
        "if [ ! $? = 0 ]; then\n",
        " rm -rf .tmp\n",
        " git clone https://github.com/cs236299-2020/lab4-4.git .tmp\n",
        " mv .tmp/tests ./\n",
        " mv .tmp/requirements.txt ./\n",
        " rm -rf .tmp\n",
        "fi\n",
        "pip install -q -r requirements.txt\n",
        "\"\"\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "26dBOK_PMDaY"
      },
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "jiQDVlPVMDaZ"
      },
      "source": [
        "%%latex\n",
        "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\newcommand{\\Prob}{\\Pr}\n",
        "\\newcommand{\\given}{\\,|\\,}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZfdzM8qMDaa"
      },
      "source": [
        "$$\n",
        "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\renewcommand{\\Prob}{\\Pr}\n",
        "\\renewcommand{\\given}{\\,|\\,}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHfkzFzKMDac"
      },
      "source": [
        "# Lab 4-4 - Sequence-to-sequence models with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybWloTSJMDad"
      },
      "source": [
        "In lab 4-3, we built a sequence-to-sequence model in its most basic form and applied it to the task of words-to-numbers conversion. In that model, we first used an encoder to encode the source sequence into a fixed-size vector (encoder final states), and then decoded based on that vector. Since the only way information from the source side can flow to the target side is through this fixed-size vector, it presents a bottleneck in the encoder-decoder model: no matter how long the source sentence is, we always need to compress it into this fixed-size vector.\n",
        "\n",
        "An _attention mechanism_ (proposed in [this seminal paper by Bahdanau et al., 2014](https://arxiv.org/abs/1409.0473)) offers a workaround by providing the decoder a dynamic view of the source-side as the decoding proceeds. Instead of compressing the source sequence into a *fixed-size* vector, we preserve the \"resolution\" and encode the source sequence into a *sequence of vectors* (usually with the same length as the source sequence) which is sometimes called a *memory bank*. When predicting each word, the decoder \"attends to\" this memory bank and assigns a weight to each vector in the sequence, and the weighted sum of those vectors will be used to make a prediction. Hopefully, the decoder will assign higher weights to more relevant source words when predicting a target word, which we'll test in this lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdEfyWLAMDaf"
      },
      "source": [
        "## Pytorch functions used in solution code\n",
        "\n",
        "* [torch.transpose](https://pytorch.org/docs/stable/generated/torch.transpose.html): swaps two dimensions of a tensor.\n",
        "* [torch.reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html): reshapes a tensor.\n",
        "* [torch.bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html) performs batched matrix multiplication.\n",
        "* [torch.nn.utils.rnn.pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html): (imported as `pack`) handles paddings. A more detailed explanation can be found [here](https://stackoverflow.com/a/56211056).\n",
        "* [torch.nn.utils.rnn.pad_packed_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html): (imported as `unpack`) handles paddings.\n",
        "* [torch.masked_fill](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.masked_fill): fills tensor elements with a value where mask is True.\n",
        "* [torch.softmax](https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#softmax): computes softmax.\n",
        "* [torch.repeat](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.repeat): repeats a tensor along the specified dimensions.\n",
        "* [torch.triu](https://pytorch.org/docs/stable/generated/torch.triu.html): returns the upper triangular part of a matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA4cuiItMDag"
      },
      "source": [
        "## Preparation - Loading data\n",
        "\n",
        "We use the same data as we used in lab 4-3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "7sKtR56-HajG"
      },
      "source": [
        "import math\n",
        "import warnings\n",
        "import copy\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext as tt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "TVjMyQTrMDah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "843d0892-339d-4685-da37-a4b0802adee3"
      },
      "source": [
        "%matplotlib inline\n",
        "plt.style.use('tableau-colorblind10')\n",
        "\n",
        "# GPU check, make sure to use GPU where available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (device)\n",
        "\n",
        "## Turn off annoying torchtext warnings about pending deprecations\n",
        "warnings.filterwarnings(\"ignore\", module=\"torchtext\", category=UserWarning)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcyDtQo8MDah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e361d705-a1bb-48a5-fb52-f71e894e3990"
      },
      "source": [
        "# Download data\n",
        "shell(\"\"\"\n",
        "  wget -nv -N -P data https://raw.githubusercontent.com/nlp-236299/data/master/Words2Num/train.src\n",
        "  wget -nv -N -P data https://raw.githubusercontent.com/nlp-236299/data/master/Words2Num/train.tgt\n",
        "  wget -nv -N -P data https://raw.githubusercontent.com/nlp-236299/data/master/Words2Num/dev.src\n",
        "  wget -nv -N -P data https://raw.githubusercontent.com/nlp-236299/data/master/Words2Num/dev.tgt\n",
        "  wget -nv -N -P data https://raw.githubusercontent.com/nlp-236299/data/master/Words2Num/test.src\n",
        "  wget -nv -N -P data https://raw.githubusercontent.com/nlp-236299/data/master/Words2Num/test.tgt\n",
        "\"\"\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaNFzGZFMDai"
      },
      "source": [
        "As before, we use `torchtext` to load data. We use two fields: `SRC` for processing the source side (the words) and `TGT` for processing the target side (the numbers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAqvfrsZMDaj"
      },
      "source": [
        "SRC = tt.data.Field(include_lengths=True,         # include lengths\n",
        "                    batch_first=False,            # batches will be max_len x bsz\n",
        "                    tokenize=lambda x: x.split(), # use split to tokenize\n",
        "                   ) \n",
        "TGT = tt.data.Field(include_lengths=False,\n",
        "                    batch_first=False,            # batches will be max_len x bsz\n",
        "                    tokenize=lambda x: x.split(), # use split to tokenize\n",
        "                    init_token=\"<bos>\",           # prepend <bos>\n",
        "                    eos_token=\"<eos>\")            # append <eos>\n",
        "fields = [('src', SRC), ('tgt', TGT)]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F734I_qyMDaj"
      },
      "source": [
        "As in lab 4-3, we prepended `<bos>` and appended `<eos>` to target sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtn-qvjYrk_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "690f86f9-d533-41bf-b135-49bb8b3b4810"
      },
      "source": [
        "# Make splits for data\n",
        "train_data, val_data, test_data = tt.datasets.TranslationDataset.splits(\n",
        "    ('.src', '.tgt'), fields, path='./data',\n",
        "    train='train', validation='dev', test='test')\n",
        "\n",
        "# Build vocabulary\n",
        "SRC.build_vocab(train_data.src)\n",
        "TGT.build_vocab(train_data.tgt)\n",
        "\n",
        "print (f\"Size of src vocab: {len(SRC.vocab)}\")\n",
        "print (f\"Size of tgt vocab: {len(TGT.vocab)}\")\n",
        "print (f\"Index for src padding: {SRC.vocab.stoi[SRC.pad_token]}\")\n",
        "print (f\"Index for tgt padding: {TGT.vocab.stoi[TGT.pad_token]}\")\n",
        "print (f\"Index for start of sequence token: {TGT.vocab.stoi[TGT.init_token]}\")\n",
        "print (f\"Index for end of sequence token: {TGT.vocab.stoi[TGT.eos_token]}\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of src vocab: 34\n",
            "Size of tgt vocab: 14\n",
            "Index for src padding: 1\n",
            "Index for tgt padding: 1\n",
            "Index for start of sequence token: 2\n",
            "Index for end of sequence token: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhuT9uHrMDal"
      },
      "source": [
        "We batch training and validation data into minibatches, but for the test set, we use a batch size of 1, to make decoding implementation easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLATojtUroS7"
      },
      "source": [
        "BATCH_SIZE = 32     # batch size for training and validation\n",
        "TEST_BATCH_SIZE = 1 # batch size for test; we use 1 to make implementation easier\n",
        "\n",
        "train_iter, val_iter = tt.data.BucketIterator.splits((train_data, val_data),\n",
        "                                                     batch_size=BATCH_SIZE, \n",
        "                                                     device=device,\n",
        "                                                     repeat=False, \n",
        "                                                     sort_key=lambda x: len(x.src), # sort by length to minimize padding\n",
        "                                                     sort_within_batch=True)\n",
        "test_iter = tt.data.BucketIterator(test_data, \n",
        "                                   batch_size=TEST_BATCH_SIZE, \n",
        "                                   device=device,\n",
        "                                   repeat=False, \n",
        "                                   sort=False, \n",
        "                                   train=False)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNPGZ7kNMDam"
      },
      "source": [
        "Let's take a look at a batch from these iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHaSMyjyrpVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ae3b2c-da82-457a-82d1-4fb59329875c"
      },
      "source": [
        "batch = next(iter(train_iter))\n",
        "src, src_lengths = batch.src\n",
        "print (f\"Size of src batch: {src.shape}\")\n",
        "print (f\"Third src sentence in batch: {src[:, 2]}\")\n",
        "print (f\"Length of the third src sentence in batch: {src_lengths[2]}\")\n",
        "print (f\"Converted back to string: {' '.join([SRC.vocab.itos[i] for i in src[:, 2]])}\")\n",
        "\n",
        "tgt = batch.tgt\n",
        "print (f\"Size of tgt batch: {tgt.shape}\")\n",
        "print (f\"Third tgt sentence in batch: {tgt[:, 2]}\")\n",
        "print (f\"Converted back to string: {' '.join([TGT.vocab.itos[i] for i in tgt[:, 2]])}\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of src batch: torch.Size([16, 32])\n",
            "Third src sentence in batch: tensor([13, 23,  6,  3,  2, 19, 13,  5,  8,  3,  2, 10,  4,  2, 19,  8],\n",
            "       device='cuda:0')\n",
            "Length of the third src sentence in batch: 16\n",
            "Converted back to string: eight billion two hundred and eighty eight million five hundred and nine thousand and eighty five\n",
            "Size of tgt batch: torch.Size([12, 32])\n",
            "Third tgt sentence in batch: tensor([ 2, 11,  7, 11, 11,  9, 13,  5, 13, 11,  9,  3], device='cuda:0')\n",
            "Converted back to string: <bos> 8 2 8 8 5 0 9 0 8 5 <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfXUL6h9MDan"
      },
      "source": [
        "## The attention mechanism\n",
        "\n",
        "Attention works by _querying_ a (dynamically sized) sequence of _keys_ associated with _values_. As usual, the query, keys, and values are represented as vectors. The query process provides a score that specifies how much each key should be attended to. The attention can then be summarized by taking an average of the values weighted by the attention score of the corresponding keys. This _context vector_ can then be used as another input to other processes.\n",
        "\n",
        "More formally, let's suppose we have a query vector $\\mathbf{q}\\in \\mathbb{R}^D$, a set of $S$ key-value pairs $\\{(\\mathbf{k}_i, \\mathbf{v}_i) \\in \\mathbb{R}^D \\times \\mathbb{R}^D: i \\in \\{1, 2, \\cdots, S\\} \\}$, where $D$ is the hidden size. What we want to do through the attention mechanism is to use the query to attend to the keys, and summarize those values associated with the \"relevant\" keys into a fixed-size context vector $\\mathbf{c}\\in\\mathbb{R}^D$. Note that this is different from directly compressing the key-value pairs into a fixed-size vector, since depending on the query, we might end up with different context vectors.\n",
        "\n",
        "To determine the score for a given query and key, it is standard to use a measure of similarity between the query and key. You've seen such similarity measures before, in labs 1-1 and 1-2. A good choice is simply the normalized dot product between query and key. We'll thus take the attention score for query $\\mathbf{q}$ and key $\\mathbf{k}_i$ to be \n",
        "$$\n",
        "a_i = \\frac{\\exp(\\mathbf{q} \\cdot \\mathbf{k}_i)}{Z},\n",
        "$$\n",
        "where $\\cdot$ denotes the dot product (inner product), and \n",
        "$$Z = \\sum_i \\exp(\\mathbf{q} \\cdot \\mathbf{k}_i)$$ \n",
        "is the normalizer to guarantee the scores all sum to one. (There are multiple ways of parameterizing the attention function, but the form we present here is the most popular one.) You might have noticed that the operation above is essentially a softmax over $\\mathbf{q}\\cdot\\mathbf{k}$.\n",
        "\n",
        "The attention scores $\\mathbf{a}$ lie on a simplex (meaninging $a_i\\ge 0$ and $\\sum_i a_i=1$), which lends it some interpretability: the closer $a_i$ is to 1, the more \"relevant\" a key $k_i$ is to the given query. We will observe this later in the lab: When we are about to predict the target word \"3\", $a_i$ is close to 1 for the source word $x_i=\\text{\"three\"}$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6ZJh0mJMDao"
      },
      "source": [
        "To compute the context vector $\\mathbf{c}$, we take the weighted sum of values using the corresponding attention scores as weights:\n",
        "$$\n",
        "\\mathbf{c} = \\sum_{i=1}^K a_i \\mathbf{v}_i\n",
        "$$\n",
        "The closer $a_i$ is to 1, the higher the weight $\\mathbf{v}_i$ receives: in the extreme, if there exists $i$ for which $a_i$ is 1, then $\\mathbf{c}$ will be $\\mathbf{v}_i$.\n",
        "\n",
        "In practice, instead of computing the context vector once for each query, we want to batch computations for different queries together for parallel processing on GPUs. This will become useful for the transformer implementation. We use a matrix $Q\\in\\mathbb{R}^{{T} \\times D}$ to store $T$ queries, a matrix $K\\in\\mathbb{R}^{S \\times D}$ to store $S$ keys, and a matrix $V\\in\\mathbb{R}^{S\\times D}$ to store the corresponding values. Then we can write down how we compute the attention scores $A\\in\\mathbb{R}^{T \\times S}$ in a matrix form:\n",
        "\n",
        "$$\n",
        "A = \\operatorname{softmax} (Q K^{\\top}, \\text{dim}=-1),\n",
        "$$\n",
        "where $A_{ij}$ stores the score of query $A_i$ attending to value $V_j$.\n",
        "\n",
        "To get the context matrix $C \\in \\mathbb{R}^{T \\times D}$:\n",
        "\n",
        "$$\n",
        "C = A V\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "QOvfwirhMDap"
      },
      "source": [
        "Your first job is to implement this calculation by finishing the attention function below, which takes the $Q$, $K$, and $V$ matrices and returns the $A$ and $C$ matrices. Note that for these matrices, there is one additional dimension for batch, so instead of $Q\\in \\mathbb{R}^{T \\times D}$, $K,V\\in \\mathbb{R}^{S \\times D}$, $A\\in \\mathbb{R}^{T \\times S}$, $C\\in \\mathbb{R}^{T \\times D}$, we have $Q\\in\\mathbb{R}^{T\\times B \\times D}$, $K,V\\in \\mathbb{R}^{S \\times B\\times D}$, $A\\in \\mathbb{R}^{B\\times T \\times S}$, $C\\in \\mathbb{R}^{T \\times B \\times D}$, where $B$ is the batch size.  In addition, the function below also takes an argument `mask` of size $\\mathbb{R}^{B\\times T \\times S}$ to mark where attentions are disallowed. This is useful not only in disallowing attending to padding symbols, but also in implementing the transformer model which we'll see later in this lab.\n",
        "\n",
        "> Hint: You might find [`torch.bmm`](https://pytorch.org/docs/stable/generated/torch.bmm.html) helpful for batched matrix multiplications. You might need to transpose and reshape tensors to be able to use this function.\n",
        "\n",
        "> Hint: As mentioned in the beginning of the lab, you might also find [`torch.transpose`](https://pytorch.org/docs/stable/generated/torch.transpose.html), [`torch.reshape`](https://pytorch.org/docs/stable/generated/torch.reshape.html), [`torch.masked_fill`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.masked_fill), and [`torch.softmax`](https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#softmax) useful.\n",
        "\n",
        "> Hint: A simple trick for masking an attention score is to set it to **negative infinity** before normalization.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: attention\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZDq2jwcMDap"
      },
      "source": [
        "#TODO - finish implementing this function.\n",
        "def attention(batched_Q, batched_K, batched_V, mask=None):\n",
        "  \"\"\"\n",
        "  Performs the attention operation and returns the attention matrix\n",
        "  `batched_A` and the context matrix `batched_C` using queries \n",
        "  `batched_Q`, keys `batched_K`, and values `batched_V`.\n",
        "  Arguments:\n",
        "      batched_Q: (q_len, bsz, D)\n",
        "      batched_K: (k_len, bsz, D)\n",
        "      batched_V: (k_len, bsz, D)\n",
        "      mask: (bsz, q_len, k_len). An optional boolean mask *disallowing* \n",
        "            attention where the mask value is *`False`*.\n",
        "  Returns:\n",
        "      batched_A: the normalized attention scores (bsz, q_len, k_ken)\n",
        "      batched_C: a tensor of size (q_len, bsz, D).\n",
        "  \"\"\"\n",
        "  # Check sizes\n",
        "  D = batched_Q.size(-1)\n",
        "  bsz = batched_Q.size(1)\n",
        "  q_len = batched_Q.size(0)\n",
        "  k_len = batched_K.size(0)\n",
        "  assert batched_K.size(-1) == D and batched_V.size(-1) == D\n",
        "  assert batched_K.size(1) == bsz and batched_V.size(1) == bsz\n",
        "  assert batched_V.size(0) == k_len\n",
        "  if mask is not None:\n",
        "    assert mask.size() == torch.Size([bsz, q_len, k_len])\n",
        "  Q = torch.transpose(batched_Q, dim0=0, dim1=1)\n",
        "  K  = torch.transpose(batched_K, dim0=0, dim1=1)\n",
        "  K = torch.transpose(K, dim0=1, dim1=2)\n",
        "  QK = Q @ K\n",
        "  if(mask is not None):\n",
        "    mask = mask == False\n",
        "    QK = QK.masked_fill_(mask, -float(\"Inf\"))\n",
        "  batched_A = nn.Softmax(dim=-1)(QK)\n",
        "  V = torch.transpose(batched_V, dim0=0, dim1=1)\n",
        "  batched_C = batched_A @ V\n",
        "  batched_C = torch.transpose(batched_C, dim0=0, dim1=1)\n",
        "  # Verify that things sum up to one properly.\n",
        "  assert torch.all(torch.isclose(batched_A.sum(-1), \n",
        "                                 torch.ones(bsz, q_len).to(device)))\n",
        "  return batched_A, batched_C"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "0LKTMIA9MDaq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "outputId": "6ec77729-8270-48c8-9222-66a0c4b76157"
      },
      "source": [
        "grader.check(\"attention\")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ],
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW6LOMT9MDaq"
      },
      "source": [
        "## Neural encoder-decoder models with attention\n",
        "\n",
        "Now we can add an attention mechanism to our encoder-decoder model. As in lab 4-3, we use a bidirectional LSTM as the encoder, and a unidirectional LSTM as the decoder, and initialize the decoder state with the encoder final state. However, instead of directly projecting the decoder hidden state to logits, we use it as a query vector and attend to all encoder outputs (used as both keys and values), and then concatanate the resulting context vector with the query vector, and project to logits. In addition, we add the context vector to the word embedding at the next time step, so that the LSTM can be aware of the attention results. \n",
        "\n",
        "<img src=\"https://github.com/nlp-course/data/raw/master/img/encoder_decoder_attn.png\" alt=\"encoder-decoder-attn illustration\" />\n",
        "\n",
        "In the above illustration, at the first time step, we use $q_1$ to denote the decoder output. Instead of directly projecting that to logits as in lab 4-3, we use $q_1$ as the query vector, and use it to attend to the memory bank (which is the sequence of encoder outputs) and get the context vector $c_1$. We concatenate $c_1$ with $q_1$, and project the result to the vocabulary size to get logits. At the next step, we first embed $y_1$ into embeddings, and then **add** $c_1$ to it (via componentwise addition) and use the sum as the decoder input. This process continues until an end-of-sequence is produced.\n",
        "\n",
        "You'll need to implement `forward_encoder`, `forward_decoder_incrementally`, and `forward` in the code below. The `forward_encoder` function will return a \"memory bank\" in addition to the final states. The \"memory bank\" is simply the encoder outputs at all time steps, which is the first returned value of `torch.nn.LSTM`.\n",
        "\n",
        "The `forward_decoder_incrementally` function forwards the LSTM cell for a single time step. It takes the initial decoder state, the memory bank, and the ground truth target input at the current time step and returns logits for this time step. In addition, it needs to return the context vector and the updated decoder state, which will be used for the next time step. Note that here you need to consider **batch sizes greater than 1**, as this function is used in `forward_decoder`, which is used during training.\n",
        "\n",
        "In summary, the steps in decoding are:\n",
        "\n",
        "1. Map the target words to word embeddings. Add the context vector from the previous time step if any. Use the result as the input to the decoder.\n",
        "\n",
        "2. Forward the decoder RNN for one time step. Use the decoder output as query, the memory bank as both keys and values, and compute the context vector through the attention mechanism. \n",
        "\n",
        "3. Concatenate the context vector with the decoder output, and project the concatenation to vocabulary size as (unnormalized) logits. Normalize them using `torch.log_softmax` if `normalize` is `True`.\n",
        "\n",
        "4. Update the decoder hidden state and the context vector, which will be used in the next time step.\n",
        "\n",
        "The `forward` function used the `forward_encoder` function to collect the \"memory bank\" and the final states, and then uses the `forward_decoder` function to get the logits. Remember to use the padding mask that's calculated inside the `forward` function.  \n",
        "\n",
        "Before proceeding, let's consider a simple question: in lab 4-3, we tried to avoid `for` loops, but if you read the code of `forward_decoder`, you might notice a `for` loop there. Is this unavoidable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "MMiOlmujMDaq"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "---\n",
        "**Question:** Why do we need a `for` loop in `forward_decoder` in this function?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_for_loop\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uqIzQnzMDar"
      },
      "source": [
        "Because we need to apply the attention mechanism on each step, and we need this for using the appropriate context for each predication, which is different for each word in the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "iFTrRdBfMDar"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "---\n",
        "Now let's implement `forward_encoder`, `forward_decoder_incrementally`, and `forward`. \n",
        "\n",
        "> Hint on using `pack`: if you use `pack` to handle paddings and pass the result as encoder inputs, you need to use `unpack` and extract the first returned value as the memory bank.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: attn_encoder_decoder\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VVZlQeWMDas"
      },
      "source": [
        "#TODO - implement `forward_encoder` and `forward_decoder_incrementally`.\n",
        "class AttnEncoderDecoder(nn.Module):\n",
        "  def __init__(self, src_field, tgt_field, hidden_size=64, layers=3):\n",
        "    \"\"\"\n",
        "    Initializer. Creates network modules and loss function.\n",
        "    Arguments:\n",
        "        src_field: src field\n",
        "        tgt_field: tgt field\n",
        "        hidden_size: hidden layer size of both encoder and decoder\n",
        "        layers: number of layers of both encoder and decoder\n",
        "    \"\"\"\n",
        "    super(AttnEncoderDecoder, self).__init__()\n",
        "    self.src_field = src_field\n",
        "    self.tgt_field = tgt_field\n",
        "    \n",
        "    # Keep the vocabulary sizes available\n",
        "    self.V_src = len(src_field.vocab.itos)\n",
        "    self.V_tgt = len(tgt_field.vocab.itos)\n",
        "    \n",
        "    # Get special word ids\n",
        "    self.padding_id_src = src_field.vocab.stoi[src_field.pad_token]\n",
        "    self.padding_id_tgt = tgt_field.vocab.stoi[tgt_field.pad_token]\n",
        "    self.bos_id = tgt_field.vocab.stoi[tgt_field.init_token]\n",
        "    self.eos_id = tgt_field.vocab.stoi[tgt_field.eos_token]\n",
        "\n",
        "    # Keep hyper-parameters available\n",
        "    self.embedding_size = hidden_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.layers = layers\n",
        "\n",
        "    # Create essential modules\n",
        "    self.word_embeddings_src = nn.Embedding(self.V_src, self.embedding_size)\n",
        "    self.word_embeddings_tgt = nn.Embedding(self.V_tgt, self.embedding_size)\n",
        "\n",
        "    # RNN cells\n",
        "    self.encoder_rnn = nn.LSTM(\n",
        "      input_size    = self.embedding_size,\n",
        "      hidden_size   = hidden_size // 2, # to match decoder hidden size\n",
        "      num_layers    = layers,\n",
        "      bidirectional = True              # bidirectional encoder\n",
        "    )\n",
        "    self.decoder_rnn = nn.LSTM(\n",
        "      input_size    = self.embedding_size,\n",
        "      hidden_size   = hidden_size,\n",
        "      num_layers    = layers,\n",
        "      bidirectional = False             # unidirectional decoder\n",
        "    )\n",
        "\n",
        "    # Final projection layer\n",
        "    self.hidden2output = nn.Linear(2*hidden_size, self.V_tgt) # project the concatenation to logits\n",
        "   \n",
        "    # Create loss function\n",
        "    self.loss_function = nn.CrossEntropyLoss(reduction='sum', \n",
        "                                             ignore_index=self.padding_id_tgt)\n",
        "\n",
        "  def forward_encoder(self, src, src_lengths):\n",
        "    \"\"\"\n",
        "    Encodes source words `src`.\n",
        "    Arguments:\n",
        "        src: src batch of size (max_src_len, bsz)\n",
        "        src_lengths: src lengths of size (bsz)\n",
        "    Returns:\n",
        "        memory_bank: a tensor of size (src_len, bsz, hidden_size)\n",
        "        (final_state, context): `final_state` is a tuple (h, c) where h/c is of size \n",
        "                                (layers, bsz, hidden_size), and `context` is `None`. \n",
        "    \"\"\"\n",
        "    # Compute word embeddings\n",
        "    src_embeddings = self.word_embeddings_src(src) # max_src_len, bsz, embedding_size\n",
        "    src_lengths = src_lengths.tolist()\n",
        "    # Deal with paddings\n",
        "    packed_src = pack(src_embeddings, src_lengths)\n",
        "    #TODO\n",
        "    packed_outputs, (h, c) = self.encoder_rnn(packed_src)\n",
        "\n",
        "    shape = h.shape\n",
        "    h = h.view(2, int(shape[0] / 2), shape[1], shape[2])\n",
        "    h = torch.cat([h[0], h[1]], dim=2)\n",
        "    c = c.view(2, int(shape[0] / 2), shape[1], shape[2])\n",
        "    c = torch.cat([c[0], c[1]], dim=2)\n",
        "\n",
        "    memory_bank, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "    final_state = (h,c)\n",
        "    context = None\n",
        "    return memory_bank, (final_state, context)\n",
        "\n",
        "  def forward_decoder(self, encoder_final_state, tgt_in, memory_bank, src_mask):\n",
        "    \"\"\"\n",
        "    Decodes based on encoder final state, memory bank, src_mask, and ground truth \n",
        "    target words.\n",
        "    Arguments:\n",
        "        encoder_final_state: (final_state, None) where final_state is the encoder\n",
        "                             final state used to initialize decoder. None is the\n",
        "                             initial context (there's no previous context at the\n",
        "                             first step).\n",
        "        tgt_in: a tensor of size (tgt_len, bsz)\n",
        "        memory_bank: a tensor of size (src_len, bsz, hidden_size), encoder outputs \n",
        "                     at every position\n",
        "        src_mask: a tensor of size (src_len, bsz): a boolean tensor, `False` where\n",
        "                  src is padding (we disallow decoder to attend to those places).\n",
        "    Returns:\n",
        "        Logits of size (tgt_len, bsz, V_tgt) (before the softmax operation)\n",
        "    \"\"\"\n",
        "    max_tgt_length = tgt_in.size(0)\n",
        "    \n",
        "    # Initialize decoder state, note that it's a tuple (state, context) here\n",
        "    decoder_states = encoder_final_state\n",
        "    \n",
        "    all_logits = []\n",
        "    for i in range(max_tgt_length):\n",
        "      logits, decoder_states, attn = \\\n",
        "        self.forward_decoder_incrementally(decoder_states, \n",
        "                                           tgt_in[i], \n",
        "                                           memory_bank,\n",
        "                                           src_mask,\n",
        "                                           normalize=False)\n",
        "      all_logits.append(logits)             # list of bsz, vocab_tgt\n",
        "    all_logits = torch.stack(all_logits, 0) # tgt_len, bsz, vocab_tgt\n",
        "    return all_logits\n",
        "\n",
        "  def forward(self, src, src_lengths, tgt_in):\n",
        "    \"\"\"\n",
        "    Performs forward computation, returns logits.\n",
        "    Arguments:\n",
        "        src: src batch of size (max_src_len, bsz)\n",
        "        src_lengths: src lengths of size (bsz)\n",
        "        tgt_in:  a tensor of size (tgt_len, bsz)\n",
        "    \"\"\"\n",
        "    # Create padding mask\n",
        "    src_mask = src.ne(self.padding_id_src) # max_src_len, bsz\n",
        "    # TODO \n",
        "    memory_bank, encoder_final_state = self.forward_encoder(src, src_lengths)\n",
        "    logits = self.forward_decoder(encoder_final_state, tgt_in, memory_bank, src_mask)\n",
        "    return logits\n",
        "\n",
        "  def forward_decoder_incrementally(self, prev_decoder_states, tgt_in_onestep, \n",
        "                                    memory_bank, src_mask,\n",
        "                                    normalize=True):\n",
        "    \"\"\"\n",
        "    Forward the decoder for a single step with token `tgt_in_onestep`.\n",
        "    This function will be used both in `forward_decoder` and in beam search.\n",
        "    Note that bsz can be greater than 1.\n",
        "    Arguments:\n",
        "        prev_decoder_states: a tuple (prev_decoder_state, prev_context). `prev_context`\n",
        "                             is `None` for the first step\n",
        "        tgt_in_onestep: a tensor of size (bsz), tokens at one step\n",
        "        memory_bank: a tensor of size (src_len, bsz, hidden_size), encoder outputs \n",
        "                     at every position\n",
        "        src_mask: a tensor of size (src_len, bsz): a boolean tensor, `False` where\n",
        "                  src is padding (we disallow decoder to attend to those places).\n",
        "        normalize: use log_softmax to normalize or not. Beam search needs to normalize,\n",
        "                   while `forward_decoder` does not\n",
        "    Returns:\n",
        "        logits: log probabilities for `tgt_in_token` of size (bsz, V_tgt)\n",
        "        decoder_states: (`decoder_state`, `context`) which will be used for the \n",
        "                        next incremental update\n",
        "        attn: normalized attention scores at this step (bsz, src_len)\n",
        "    \"\"\"\n",
        "    prev_decoder_state, prev_context = prev_decoder_states\n",
        "    #TODO\n",
        "    tgt_embeddings = self.word_embeddings_tgt(tgt_in_onestep.view(1, -1)) # max_len, batch_size, embedding_size\n",
        "    if prev_context != None:\n",
        "      tgt_embeddings += prev_context\n",
        "      prev_decoder_state = (prev_decoder_state, prev_context.repeat(3,1,1))\n",
        "    decoder_state, (h, c) = self.decoder_rnn(tgt_embeddings, prev_decoder_state)\n",
        "\n",
        "    #m = src_mask.T.view(src_mask.T.shape[0], src_mask.T.shape[1], 1)\n",
        "    #k_len = h.shape[0]\n",
        "    #mask = m.repeat(1, 1, k_len)\n",
        "    mask = src_mask.T.unsqueeze(1)\n",
        "    attn, context = attention(decoder_state, memory_bank, memory_bank, mask)\n",
        "\n",
        "    decoder_states = (decoder_state, context)\n",
        "    dec_out_e_cntxt = torch.cat(decoder_states, dim=2)\n",
        "    logits = self.hidden2output(dec_out_e_cntxt).squeeze(0)\n",
        "    attn = attn.squeeze(1)\n",
        "    decoder_states = (decoder_states[0].repeat(3,1,1), decoder_states[1])\n",
        "    if normalize:\n",
        "      logits = torch.log_softmax(logits, dim=-1)\n",
        "    return logits, decoder_states, attn\n",
        "\n",
        "  def evaluate_ppl(self, iterator):\n",
        "    \"\"\"Returns the model's perplexity on a given dataset `iterator`.\"\"\"\n",
        "    # Switch to eval mode\n",
        "    self.eval()\n",
        "    total_loss = 0\n",
        "    total_words = 0\n",
        "    for batch in iterator:\n",
        "      # Input and target\n",
        "      src, src_lengths = batch.src\n",
        "      tgt = batch.tgt # max_length_sql, bsz\n",
        "      tgt_in = tgt[:-1] # remove <eos> for decode input (y_0=<bos>, y_1, y_2)\n",
        "      tgt_out = tgt[1:] # remove <bos> as target        (y_1, y_2, y_3=<eos>)\n",
        "      # Forward to get logits\n",
        "      logits = self.forward(src, src_lengths, tgt_in)\n",
        "      # Compute cross entropy loss\n",
        "      loss = self.loss_function(logits.view(-1, self.V_tgt), tgt_out.view(-1))\n",
        "      total_loss += loss.item()\n",
        "      total_words += tgt_out.ne(self.padding_id_tgt).float().sum().item()\n",
        "    return math.exp(total_loss/total_words)\n",
        "\n",
        "  def train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001):\n",
        "    \"\"\"Train the model.\"\"\"\n",
        "    # Switch the module to training mode\n",
        "    self.train()\n",
        "    # Use Adam to optimize the parameters\n",
        "    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    best_validation_ppl = float('inf')\n",
        "    best_model = None\n",
        "    # Run the optimization for multiple epochs\n",
        "    for epoch in range(epochs): \n",
        "      total_words = 0\n",
        "      total_loss = 0.0\n",
        "      for batch in tqdm(train_iter):\n",
        "        # Zero the parameter gradients\n",
        "        self.zero_grad()\n",
        "        # Input and target\n",
        "        src, src_lengths = batch.src # text: max_src_length, bsz\n",
        "        tgt = batch.tgt # max_tgt_length, bsz\n",
        "        tgt_in = tgt[:-1] # Remove <eos> for decode input (y_0=<bos>, y_1, y_2)\n",
        "        tgt_out = tgt[1:] # Remove <bos> as target        (y_1, y_2, y_3=<eos>)\n",
        "        bsz = tgt.size(1)\n",
        "        # Run forward pass and compute loss along the way.\n",
        "        logits = self.forward(src, src_lengths, tgt_in)\n",
        "        loss = self.loss_function(logits.view(-1, self.V_tgt), tgt_out.view(-1))\n",
        "        # Training stats\n",
        "        num_tgt_words = tgt_out.ne(self.padding_id_tgt).float().sum().item()\n",
        "        total_words += num_tgt_words\n",
        "        total_loss += loss.item()\n",
        "        # Perform backpropagation\n",
        "        loss.div(bsz).backward()\n",
        "        optim.step()\n",
        "\n",
        "      # Evaluate and track improvements on the validation dataset\n",
        "      validation_ppl = self.evaluate_ppl(val_iter)\n",
        "      self.train()\n",
        "      if validation_ppl < best_validation_ppl:\n",
        "        best_validation_ppl = validation_ppl\n",
        "        self.best_model = copy.deepcopy(self.state_dict())\n",
        "      epoch_loss = total_loss / total_words\n",
        "      print (f'Epoch: {epoch} Training Perplexity: {math.exp(epoch_loss):.4f} '\n",
        "             f'Validation Perplexity: {validation_ppl:.4f}')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd3yy8GKMDat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e46850-c046-4db1-be8b-025d98947328"
      },
      "source": [
        "EPOCHS = 2 # epochs, we highly recommend starting with a smaller number like 1\n",
        "LEARNING_RATE = 2e-3 # learning rate\n",
        "\n",
        "# Instantiate and train classifier\n",
        "model = AttnEncoderDecoder(SRC, TGT,\n",
        "  hidden_size    = 64,\n",
        "  layers         = 3,\n",
        ").to(device)\n",
        "\n",
        "model.train_all(train_iter, val_iter, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
        "model.load_state_dict(model.best_model)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2032/2032 [01:12<00:00, 28.14it/s]\n",
            "  0%|          | 1/2032 [00:00<03:47,  8.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Training Perplexity: 1.4156 Validation Perplexity: 1.0093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2032/2032 [01:12<00:00, 27.95it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Training Perplexity: 1.0082 Validation Perplexity: 1.0186\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_LfQ2xCMDat"
      },
      "source": [
        "Since the task we consider here is very simple, we should expect a perplexity very close to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzF3IKuBMDat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "745bdc3d-3bbd-47e7-891d-d67793387d33"
      },
      "source": [
        "# Evaluate model performance, the expected value should be < 1.05\n",
        "print (f'Test perplexity: {model.evaluate_ppl(test_iter):.3f}')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test perplexity: 1.015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "zyjGBQ0SMDau",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "outputId": "5cbe006b-8dd6-45e8-b135-15e75ac98b07"
      },
      "source": [
        "grader.check(\"encoder_decoder_ppl\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ],
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWopwQI8MDax"
      },
      "source": [
        "## Beam search decoding\n",
        "\n",
        "We can reuse most of our beam search code in lab 4-3 here: we only need to modify the code a bit to pass in `memory_bank` and `src_mask`. For reference here is the same pseudo-code used in lab 4-3, where we want to decode a single example `x` of maximum length `max_T` using a beam size of `K`.\n",
        "\n",
        "```\n",
        " 1.  def beam_search(x, K, max_T):\n",
        " 2.      finished = []       # for storing completed hypotheses\n",
        "         # Initialize the beam\n",
        " 3.      beams = [Beam(hyp=(bos), score=0)] # initial hypothesis: bos, initial score: 0\n",
        " \n",
        " 4.      for t in [1..max_T]  # main body of search over time steps           \n",
        " 5.          hypotheses = []\n",
        " \n",
        "             # Expand each beam by all possible tokens y_{t+1}\n",
        " 6.          for beam in beams:\n",
        " 7.              y_{1:t}, score = beam.hyp, beam.score\n",
        " 8.              for y_{t+1} in V:\n",
        " 9.                  y_{1:t+1} = y_{1:t} + [y_{t+1}]\n",
        " 10.                 new_score = score + log P(y_{t+1} | y_{1:t}, x)\n",
        " 11.                 hypotheses.append(Beam(hyp=y_{1:t+1}, score=new_score))\n",
        " \n",
        "             # Find K best next beams\n",
        " 12.         beams = sorted(hypotheses, key=lambda beam: -beam.score)[:K]\n",
        " \n",
        "             # Set aside finished beams (those that end in <eos>)\n",
        " 13.         for beam in beams:\n",
        " 14.             y_{t+1} = beam.hyp[-1]\n",
        " 15.             if y_{t+1} == eos:\n",
        " 16.                 finished.append(beam)\n",
        " 17.                 beams.remove(beam)\n",
        "\n",
        "             # Break the loop if everything is finished\n",
        " 18.         if len(beams) == 0:\n",
        " 19.             break              \n",
        " 20.     return sorted(finished, key=lambda beam: -beam.score)[0] # return the best finished hypothesis\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "oaP5JleIMDay"
      },
      "source": [
        "Implement function `beam_search` in the code below. In addition to the predicted target sequence, this function also returns a list of attentions `all_attns`.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: beam_search\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGgjb9UGMDay"
      },
      "source": [
        "# max target length\n",
        "MAX_T = 15\n",
        "class Beam():\n",
        "  \"\"\"\n",
        "  Helper class for storing a hypothesis, its score and its decoder hidden state.\n",
        "  \"\"\"\n",
        "  def __init__(self, decoder_state, tokens, score):\n",
        "    self.decoder_state = decoder_state\n",
        "    self.tokens = tokens\n",
        "    self.score = score\n",
        "        \n",
        "class BeamSearcher():\n",
        "  \"\"\"\n",
        "  Main class for beam search.\n",
        "  \"\"\"\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "    self.bos_id = model.bos_id\n",
        "    self.eos_id = model.eos_id\n",
        "    self.padding_id_src = model.padding_id_src\n",
        "    self.V = model.V_tgt\n",
        "\n",
        "\n",
        "  def beam_search(self, src, src_lengths, K, max_T=MAX_T):\n",
        "    \"\"\"\n",
        "    Performs beam search decoding.\n",
        "    Arguments:\n",
        "        src: src batch of size (max_src_len, 1)\n",
        "        src_lengths: src lengths of size (1)\n",
        "        K: beam size\n",
        "        max_T: max possible target length considered\n",
        "    Returns:\n",
        "        a list of token ids and a list of attentions\n",
        "    \"\"\"\n",
        "    finished = []\n",
        "    all_attns = []\n",
        "    # Initialize the beam\n",
        "    self.model.eval()\n",
        "    #TODO - fill in `memory_bank`, `encoder_final_state`, and `init_beam` below\n",
        "    \n",
        "    (memory_bank, encoder_final_state) = self.model.forward_encoder(src, src_lengths)\n",
        "    bos = torch.tensor([[self.model.bos_id]], device=device)\n",
        "    init_beam = Beam(encoder_final_state, bos, 0)\n",
        "\n",
        "    beams = [init_beam]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for t in range(max_T): # main body of search over time steps\n",
        "        \n",
        "        # Expand each beam by all possible tokens y_{t+1}\n",
        "        all_total_scores = []\n",
        "        for beam in beams:\n",
        "          y_1_to_t, score, decoder_state = beam.tokens, beam.score, beam.decoder_state\n",
        "          y_t = y_1_to_t[-1]\n",
        "          #TODO - finish the code below\n",
        "          # Hint: you might want to use `model.forward_decoder_incrementally`\n",
        "          src_mask = src.ne(self.padding_id_src)\n",
        "\n",
        "          logits, decoder_state, attn = self.model.forward_decoder_incrementally(decoder_state, y_t, memory_bank, src_mask)\n",
        "          \n",
        "          total_scores = logits\n",
        "          all_total_scores.append(total_scores)\n",
        "          all_attns.append(attn) # keep attentions for visualization\n",
        "          beam.decoder_state = decoder_state # update decoder state in the beam\n",
        "        all_total_scores = torch.stack(all_total_scores) # (K, V) when t>0, (1, V) when t=0\n",
        "\n",
        "        # Find K best next beams\n",
        "        # The code below has the same functionality as line 6-12, but is more efficient\n",
        "        all_scores_flattened = all_total_scores.view(-1) # K*V when t>0, 1*V when t=0\n",
        "        topk_scores, topk_ids = all_scores_flattened.topk(K, 0)\n",
        "        beam_ids = topk_ids // self.V\n",
        "        next_tokens = topk_ids - beam_ids * self.V\n",
        "        new_beams = []\n",
        "        for k in range(K):\n",
        "          beam_id = beam_ids[k]       # which beam it comes from\n",
        "          y_t_plus_1 = next_tokens[k] # which y_{t+1}\n",
        "          score = topk_scores[k]\n",
        "          beam = beams[beam_id]\n",
        "          decoder_state = beam.decoder_state\n",
        "          y_1_to_t = beam.tokens\n",
        "          #TODO\n",
        "          y_t = torch.cat((y_1_to_t, y_t_plus_1.view(1,1)))\n",
        "          new_beam = Beam(decoder_state, y_t, beam.score+score)\n",
        "          new_beams.append(new_beam)\n",
        "        beams = new_beams\n",
        "\n",
        "        # Set aside completed beams\n",
        "        # TODO - move completed beams to `finished` (and remove them from `beams`)\n",
        "        new_beams = []\n",
        "        for beam in beams:\n",
        "            y_1_to_t, score, decoder_state = beam.tokens, beam.score, beam.decoder_state\n",
        "            y_t = y_1_to_t[-1]\n",
        "            if y_t.view(1) == self.model.eos_id:\n",
        "                finished.append(beam)\n",
        "            else:\n",
        "                new_beams.append(beam)\n",
        "        beams = new_beams\n",
        "        \n",
        "        # Break the loop if everything is completed\n",
        "        if len(beams) == 0:\n",
        "            break\n",
        "            \n",
        "    # Return the best hypothesis\n",
        "    if len(finished) > 0:\n",
        "      finished = sorted(finished, key=lambda beam: -beam.score)\n",
        "      return finished[0].tokens, all_attns\n",
        "    else: # when nothing is finished, return an unfinished hypothesis\n",
        "      return beams[0].tokens, all_attns"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "84k5OtdQMDaz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "outputId": "9aa57507-aad0-4d06-ec41-28c44f423fb6"
      },
      "source": [
        "grader.check(\"beam_search\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ],
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reZU880bMDaz"
      },
      "source": [
        "Now we can use beam search decoding to predict the outputs for the test set inputs using the trained model. You should expect an accuracy close to 100%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JifzNBRMDaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8922070e-e709-4b30-8956-5b1aedcb20ab"
      },
      "source": [
        "DEBUG = True # set to False to disable printing predictions\n",
        "K = 1 # beam size 1\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# create beam searcher\n",
        "beam_searcher = BeamSearcher(model)\n",
        "\n",
        "for batch in test_iter:\n",
        "  # Input and output\n",
        "  src, src_lengths = batch.src\n",
        "  # Predict\n",
        "  prediction, _ = beam_searcher.beam_search(src, src_lengths, K)\n",
        "  # Convert to string\n",
        "  prediction = ' '.join([TGT.vocab.itos[token] for token in prediction])\n",
        "  prediction = prediction.lstrip('<bos>').rstrip('<eos>').strip()\n",
        "  ground_truth = ' '.join([TGT.vocab.itos[token] for token in batch.tgt.view(-1)])\n",
        "  ground_truth = ground_truth.lstrip('<bos>').rstrip('<eos>').strip()\n",
        "  if DEBUG:\n",
        "    src = ' '.join([SRC.vocab.itos[item] for item in src.view(-1)])\n",
        "    print (f'Source: {src}')\n",
        "    print (f'Prediction:   {prediction}')\n",
        "    print (f'Ground truth: {ground_truth}')\n",
        "  if ground_truth == prediction:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "\n",
        "print (f'Accuracy: {correct/total:.2f}')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source: sixteen thousand eight hundred and thirty two\n",
            "Prediction:   1 6 8 3 2\n",
            "Ground truth: 1 6 8 3 2\n",
            "Source: sixty seven million six hundred and eighty five thousand two hundred and thirty\n",
            "Prediction:   6 7 6 8 5 2 3 0\n",
            "Ground truth: 6 7 6 8 5 2 3 0\n",
            "Source: six thousand two hundred and twelve\n",
            "Prediction:   6 2 1 2\n",
            "Ground truth: 6 2 1 2\n",
            "Source: seven hundred and ninety eight million three hundred and thirty one thousand eight hundred and eighteen\n",
            "Prediction:   7 9 8 3 3 1 8 1 8\n",
            "Ground truth: 7 9 8 3 3 1 8 1 8\n",
            "Source: eighty eight million four hundred and thirteen thousand nine hundred and eighteen\n",
            "Prediction:   8 8 4 1 3 9 1 8\n",
            "Ground truth: 8 8 4 1 3 9 1 8\n",
            "Source: three hundred and seventy four thousand two hundred and seventy\n",
            "Prediction:   3 7 4 2 7 0\n",
            "Ground truth: 3 7 4 2 7 0\n",
            "Source: ninety eight million three hundred and seventy thousand five hundred and forty five\n",
            "Prediction:   9 8 3 7 0 5 4 5\n",
            "Ground truth: 9 8 3 7 0 5 4 5\n",
            "Source: ninety seven thousand seven hundred and sixty two\n",
            "Prediction:   9 7 7 6 2\n",
            "Ground truth: 9 7 7 6 2\n",
            "Source: four hundred and ten thousand two hundred and three\n",
            "Prediction:   4 1 0 2 0 3\n",
            "Ground truth: 4 1 0 2 0 3\n",
            "Source: five hundred and ninety eight thousand three hundred and ninety seven\n",
            "Prediction:   5 9 8 3 9 7\n",
            "Ground truth: 5 9 8 3 9 7\n",
            "Source: eighty nine thousand and forty three\n",
            "Prediction:   8 9 0 4 3\n",
            "Ground truth: 8 9 0 4 3\n",
            "Source: eight hundred million five hundred and sixty two thousand seven hundred and twenty seven\n",
            "Prediction:   8 0 5 6 2 7 2 7\n",
            "Ground truth: 8 0 0 5 6 2 7 2 7\n",
            "Source: nine hundred and forty seven million seven hundred and twenty five thousand one hundred and eighty five\n",
            "Prediction:   9 4 7 7 2 5 1 8 5\n",
            "Ground truth: 9 4 7 7 2 5 1 8 5\n",
            "Source: four billion six hundred and fifty seven million seven hundred and sixty seven thousand one hundred and three\n",
            "Prediction:   4 6 5 7 7 6 7 1 0 3\n",
            "Ground truth: 4 6 5 7 7 6 7 1 0 3\n",
            "Source: thirty one million eight hundred and thirty thousand two hundred and fifty four\n",
            "Prediction:   3 1 8 3 0 2 5 4\n",
            "Ground truth: 3 1 8 3 0 2 5 4\n",
            "Source: three billion seven hundred and eighty two million one hundred and sixty two thousand three hundred and ninety\n",
            "Prediction:   3 7 8 2 1 6 2 3 9 0\n",
            "Ground truth: 3 7 8 2 1 6 2 3 9 0\n",
            "Source: thirty five million six hundred and twenty four thousand seven hundred\n",
            "Prediction:   3 5 6 2 4 7 0 0\n",
            "Ground truth: 3 5 6 2 4 7 0 0\n",
            "Source: six billion three hundred and eleven million nine hundred and eighty seven thousand six hundred and seventy four\n",
            "Prediction:   6 3 1 1 9 8 7 6 7 4\n",
            "Ground truth: 6 3 1 1 9 8 7 6 7 4\n",
            "Source: six hundred and fifty eight thousand six hundred and ninety eight\n",
            "Prediction:   6 5 8 6 9 8\n",
            "Ground truth: 6 5 8 6 9 8\n",
            "Source: eight billion nine hundred and thirty seven million six hundred and thirty four thousand three hundred and seven\n",
            "Prediction:   8 9 3 7 6 3 4 3 0 7\n",
            "Ground truth: 8 9 3 7 6 3 4 3 0 7\n",
            "Source: one hundred and three thousand six hundred and ninety four\n",
            "Prediction:   1 0 3 6 9 4\n",
            "Ground truth: 1 0 3 6 9 4\n",
            "Source: seven hundred and fifty eight million seven hundred and fifty nine thousand one hundred and twenty seven\n",
            "Prediction:   7 5 8 7 5 9 1 2 7\n",
            "Ground truth: 7 5 8 7 5 9 1 2 7\n",
            "Source: nine thousand four hundred and twelve\n",
            "Prediction:   9 4 1 2\n",
            "Ground truth: 9 4 1 2\n",
            "Source: two hundred and thirty one million six hundred and fifteen thousand nine hundred and twenty\n",
            "Prediction:   2 3 1 6 1 5 9 2 0\n",
            "Ground truth: 2 3 1 6 1 5 9 2 0\n",
            "Source: seven million one hundred and forty five thousand six hundred and eighteen\n",
            "Prediction:   7 1 4 5 6 1 8\n",
            "Ground truth: 7 1 4 5 6 1 8\n",
            "Source: fourteen thousand five hundred and sixty\n",
            "Prediction:   1 4 5 6 0\n",
            "Ground truth: 1 4 5 6 0\n",
            "Source: four billion two hundred and thirteen million six hundred and twenty six thousand nine hundred and sixty nine\n",
            "Prediction:   4 2 1 3 6 2 6 9 6 9\n",
            "Ground truth: 4 2 1 3 6 2 6 9 6 9\n",
            "Source: six million three hundred and sixty three thousand and fourteen\n",
            "Prediction:   6 3 6 3 0 1 4\n",
            "Ground truth: 6 3 6 3 0 1 4\n",
            "Source: eight million sixty eight thousand four hundred and eighty one\n",
            "Prediction:   8 0 6 8 4 8 1\n",
            "Ground truth: 8 0 6 8 4 8 1\n",
            "Source: nine billion one hundred and sixty seven million eighty eight thousand six hundred and seventy eight\n",
            "Prediction:   9 1 6 7 0 8 8 6 7 8\n",
            "Ground truth: 9 1 6 7 0 8 8 6 7 8\n",
            "Source: four million nine hundred and thirty one thousand five hundred and fifty seven\n",
            "Prediction:   4 9 3 1 5 5 7\n",
            "Ground truth: 4 9 3 1 5 5 7\n",
            "Source: six hundred and forty seven million six hundred and fourteen thousand two hundred and seventy seven\n",
            "Prediction:   6 4 7 6 1 4 2 7 7\n",
            "Ground truth: 6 4 7 6 1 4 2 7 7\n",
            "Source: three thousand four hundred and seventy seven\n",
            "Prediction:   3 4 7 7\n",
            "Ground truth: 3 4 7 7\n",
            "Source: forty six thousand six hundred and five\n",
            "Prediction:   4 6 6 0 5\n",
            "Ground truth: 4 6 6 0 5\n",
            "Source: eight thousand seven hundred and sixty eight\n",
            "Prediction:   8 7 6 8\n",
            "Ground truth: 8 7 6 8\n",
            "Source: three million four hundred and sixteen thousand five hundred and sixty six\n",
            "Prediction:   3 4 1 6 5 6 6\n",
            "Ground truth: 3 4 1 6 5 6 6\n",
            "Source: two hundred and forty seven thousand five hundred and twenty eight\n",
            "Prediction:   2 4 7 5 2 8\n",
            "Ground truth: 2 4 7 5 2 8\n",
            "Source: three hundred and four thousand three hundred and sixty four\n",
            "Prediction:   3 0 4 3 6 4\n",
            "Ground truth: 3 0 4 3 6 4\n",
            "Source: three hundred and seventy six million three hundred and ninety one thousand eight hundred and thirty two\n",
            "Prediction:   3 7 6 3 9 1 8 3 2\n",
            "Ground truth: 3 7 6 3 9 1 8 3 2\n",
            "Source: four hundred and nineteen million five hundred and eighty eight thousand three hundred and twenty one\n",
            "Prediction:   4 1 9 5 8 8 3 2 1\n",
            "Ground truth: 4 1 9 5 8 8 3 2 1\n",
            "Source: three hundred and eighty five million eight hundred and thirty two thousand two hundred and seventy two\n",
            "Prediction:   3 8 5 8 3 2 2 7 2\n",
            "Ground truth: 3 8 5 8 3 2 2 7 2\n",
            "Source: sixty six million two hundred and ninety five thousand and forty\n",
            "Prediction:   6 6 2 9 5 0 4 0\n",
            "Ground truth: 6 6 2 9 5 0 4 0\n",
            "Source: three hundred and forty five million five hundred and forty two thousand five hundred and twenty two\n",
            "Prediction:   3 4 5 5 4 2 5 2 2\n",
            "Ground truth: 3 4 5 5 4 2 5 2 2\n",
            "Source: one hundred and ten thousand six hundred and fifty five\n",
            "Prediction:   1 1 0 6 5 5\n",
            "Ground truth: 1 1 0 6 5 5\n",
            "Source: five million two hundred and eighteen thousand six hundred and eleven\n",
            "Prediction:   5 2 1 8 6 1 1\n",
            "Ground truth: 5 2 1 8 6 1 1\n",
            "Source: seven thousand eight hundred and fourteen\n",
            "Prediction:   7 8 1 4\n",
            "Ground truth: 7 8 1 4\n",
            "Source: five million seven hundred and sixty two thousand eight hundred and eighty nine\n",
            "Prediction:   5 7 6 2 8 8 9\n",
            "Ground truth: 5 7 6 2 8 8 9\n",
            "Source: two million sixty nine thousand three hundred and seventy three\n",
            "Prediction:   2 0 6 9 3 7 3\n",
            "Ground truth: 2 0 6 9 3 7 3\n",
            "Source: three hundred and seven million nine hundred and twenty one thousand four hundred and fourteen\n",
            "Prediction:   3 0 7 9 2 1 4 1 4\n",
            "Ground truth: 3 0 7 9 2 1 4 1 4\n",
            "Source: two billion nine hundred and eighty seven million one hundred and thirty thousand eight hundred and seventy six\n",
            "Prediction:   2 9 8 7 1 3 0 8 7 6\n",
            "Ground truth: 2 9 8 7 1 3 0 8 7 6\n",
            "Source: eight million two hundred and ninety thousand two hundred and sixty nine\n",
            "Prediction:   8 2 9 0 2 6 9\n",
            "Ground truth: 8 2 9 0 2 6 9\n",
            "Source: eight billion one million six hundred and forty four thousand nine hundred and twenty nine\n",
            "Prediction:   8 1 1 6 4 4 9 2 9\n",
            "Ground truth: 8 0 0 1 6 4 4 9 2 9\n",
            "Source: one hundred and eighty thousand four hundred and forty one\n",
            "Prediction:   1 8 0 4 4 1\n",
            "Ground truth: 1 8 0 4 4 1\n",
            "Source: five hundred and fifty three million six hundred and five thousand four hundred and sixty one\n",
            "Prediction:   5 5 3 6 0 5 4 6 1\n",
            "Ground truth: 5 5 3 6 0 5 4 6 1\n",
            "Source: nine hundred and twenty seven million two hundred and seventy three thousand three hundred and thirty eight\n",
            "Prediction:   9 2 7 2 7 3 3 3 8\n",
            "Ground truth: 9 2 7 2 7 3 3 3 8\n",
            "Source: four thousand three hundred and twenty one\n",
            "Prediction:   4 3 2 1\n",
            "Ground truth: 4 3 2 1\n",
            "Source: fifty two million five hundred and fifty six thousand six hundred and sixty six\n",
            "Prediction:   5 2 5 5 6 6 6 6\n",
            "Ground truth: 5 2 5 5 6 6 6 6\n",
            "Source: four thousand three hundred and fifty six\n",
            "Prediction:   4 3 5 6\n",
            "Ground truth: 4 3 5 6\n",
            "Source: four million one hundred and forty three thousand five hundred and one\n",
            "Prediction:   4 1 4 3 5 0 1\n",
            "Ground truth: 4 1 4 3 5 0 1\n",
            "Source: eighty million three hundred and four thousand four hundred and sixty six\n",
            "Prediction:   8 0 3 0 4 4 6 6\n",
            "Ground truth: 8 0 3 0 4 4 6 6\n",
            "Source: nine hundred and thirty thousand three hundred and thirty four\n",
            "Prediction:   9 3 0 3 3 4\n",
            "Ground truth: 9 3 0 3 3 4\n",
            "Source: three thousand nine hundred and fifty four\n",
            "Prediction:   3 9 5 4\n",
            "Ground truth: 3 9 5 4\n",
            "Source: forty three thousand three hundred and thirty six\n",
            "Prediction:   4 3 3 3 6\n",
            "Ground truth: 4 3 3 3 6\n",
            "Source: seventy one thousand three hundred and eighteen\n",
            "Prediction:   7 1 3 1 8\n",
            "Ground truth: 7 1 3 1 8\n",
            "Source: four hundred and six thousand four hundred and seven\n",
            "Prediction:   4 0 6 4 0 7\n",
            "Ground truth: 4 0 6 4 0 7\n",
            "Source: two thousand three hundred and ninety three\n",
            "Prediction:   2 3 9 3\n",
            "Ground truth: 2 3 9 3\n",
            "Source: eighteen million four hundred and eighty five thousand two hundred and ten\n",
            "Prediction:   1 8 4 8 5 2 1 0\n",
            "Ground truth: 1 8 4 8 5 2 1 0\n",
            "Source: one billion six hundred and forty five million eight hundred and twenty nine thousand nine hundred and eleven\n",
            "Prediction:   1 6 4 5 8 2 9 9 1 1\n",
            "Ground truth: 1 6 4 5 8 2 9 9 1 1\n",
            "Source: three thousand six hundred and seventeen\n",
            "Prediction:   3 6 1 7\n",
            "Ground truth: 3 6 1 7\n",
            "Source: two thousand four hundred and sixty four\n",
            "Prediction:   2 4 6 4\n",
            "Ground truth: 2 4 6 4\n",
            "Source: four thousand one hundred and ninety three\n",
            "Prediction:   4 1 9 3\n",
            "Ground truth: 4 1 9 3\n",
            "Source: five billion eight hundred and seventy three million four hundred and twenty nine thousand nine hundred and fifty six\n",
            "Prediction:   5 8 7 3 4 2 9 9 5 6\n",
            "Ground truth: 5 8 7 3 4 2 9 9 5 6\n",
            "Source: six hundred and ninety thousand one hundred and eighteen\n",
            "Prediction:   6 9 0 1 1 8\n",
            "Ground truth: 6 9 0 1 1 8\n",
            "Source: ninety four thousand two hundred and twenty two\n",
            "Prediction:   9 4 2 2 2\n",
            "Ground truth: 9 4 2 2 2\n",
            "Source: eight thousand nine hundred and twenty six\n",
            "Prediction:   8 9 2 6\n",
            "Ground truth: 8 9 2 6\n",
            "Source: two billion eight hundred and ninety six million six hundred and eighty six thousand and thirty two\n",
            "Prediction:   2 8 9 6 6 8 6 0 3 2\n",
            "Ground truth: 2 8 9 6 6 8 6 0 3 2\n",
            "Source: two million nine hundred and ten thousand nine hundred and eighty four\n",
            "Prediction:   2 9 1 0 9 8 4\n",
            "Ground truth: 2 9 1 0 9 8 4\n",
            "Source: ninety two million seven hundred thousand three hundred and eighty\n",
            "Prediction:   9 2 7 0 0 3 8 0\n",
            "Ground truth: 9 2 7 0 0 3 8 0\n",
            "Source: seventy four million six hundred and fifty thousand two hundred and forty seven\n",
            "Prediction:   7 4 6 5 0 2 4 7\n",
            "Ground truth: 7 4 6 5 0 2 4 7\n",
            "Source: thirty seven million two hundred and sixty two thousand four hundred and fifty four\n",
            "Prediction:   3 7 2 6 2 4 5 4\n",
            "Ground truth: 3 7 2 6 2 4 5 4\n",
            "Source: thirty nine thousand five hundred and thirty eight\n",
            "Prediction:   3 9 5 3 8\n",
            "Ground truth: 3 9 5 3 8\n",
            "Source: four hundred and seventy nine thousand four hundred and forty six\n",
            "Prediction:   4 7 9 4 4 6\n",
            "Ground truth: 4 7 9 4 4 6\n",
            "Source: three thousand five hundred and three\n",
            "Prediction:   3 5 0 3\n",
            "Ground truth: 3 5 0 3\n",
            "Source: ninety two thousand seven hundred and thirty seven\n",
            "Prediction:   9 2 7 3 7\n",
            "Ground truth: 9 2 7 3 7\n",
            "Source: one hundred and eight million three hundred and forty eight thousand six hundred and eighteen\n",
            "Prediction:   1 0 8 3 4 8 6 1 8\n",
            "Ground truth: 1 0 8 3 4 8 6 1 8\n",
            "Source: eight thousand three hundred and thirteen\n",
            "Prediction:   8 3 1 3\n",
            "Ground truth: 8 3 1 3\n",
            "Source: nine billion eight hundred and twenty seven million eight hundred and eighty four thousand nine hundred and ninety\n",
            "Prediction:   9 8 2 7 8 8 4 9 9 0\n",
            "Ground truth: 9 8 2 7 8 8 4 9 9 0\n",
            "Source: four million eight hundred and twenty thousand one hundred and sixteen\n",
            "Prediction:   4 8 2 0 1 1 6\n",
            "Ground truth: 4 8 2 0 1 1 6\n",
            "Source: seventy five thousand three hundred and fifty eight\n",
            "Prediction:   7 5 3 5 8\n",
            "Ground truth: 7 5 3 5 8\n",
            "Source: two hundred and ninety one million forty four thousand two hundred and thirty\n",
            "Prediction:   2 9 1 0 4 4 2 3 0\n",
            "Ground truth: 2 9 1 0 4 4 2 3 0\n",
            "Source: two thousand seven hundred and thirteen\n",
            "Prediction:   2 7 1 3\n",
            "Ground truth: 2 7 1 3\n",
            "Source: one hundred and fifty million nine thousand five hundred and thirty one\n",
            "Prediction:   1 5 0 0 9 5 3 1\n",
            "Ground truth: 1 5 0 0 0 9 5 3 1\n",
            "Source: nine billion two hundred and forty five million forty seven thousand one hundred and seventy six\n",
            "Prediction:   9 2 4 5 0 4 7 1 7 6\n",
            "Ground truth: 9 2 4 5 0 4 7 1 7 6\n",
            "Source: one million three hundred and sixty three thousand three hundred and ninety eight\n",
            "Prediction:   1 3 6 3 3 9 8\n",
            "Ground truth: 1 3 6 3 3 9 8\n",
            "Source: two billion twelve million one hundred and forty eight thousand five hundred and twenty eight\n",
            "Prediction:   2 0 0 2 1 4 8 5 2 8\n",
            "Ground truth: 2 0 1 2 1 4 8 5 2 8\n",
            "Source: one hundred and forty eight thousand six hundred and sixty three\n",
            "Prediction:   1 4 8 6 6 3\n",
            "Ground truth: 1 4 8 6 6 3\n",
            "Source: ninety two thousand nine hundred and eighty four\n",
            "Prediction:   9 2 9 8 4\n",
            "Ground truth: 9 2 9 8 4\n",
            "Source: six billion eight hundred and thirty six million two hundred and twelve thousand five hundred and forty eight\n",
            "Prediction:   6 8 3 6 2 1 2 5 4 8\n",
            "Ground truth: 6 8 3 6 2 1 2 5 4 8\n",
            "Source: fifty one million four hundred and thirty one thousand seven hundred and fifty\n",
            "Prediction:   5 1 4 3 1 7 5 0\n",
            "Ground truth: 5 1 4 3 1 7 5 0\n",
            "Source: eight hundred and sixty one thousand nine hundred and fifty seven\n",
            "Prediction:   8 6 1 9 5 7\n",
            "Ground truth: 8 6 1 9 5 7\n",
            "Source: six million two hundred and eighty thousand nine hundred and sixty eight\n",
            "Prediction:   6 2 8 0 9 6 8\n",
            "Ground truth: 6 2 8 0 9 6 8\n",
            "Source: eight hundred and forty five million four hundred and forty two thousand seven hundred and sixty seven\n",
            "Prediction:   8 4 5 4 4 2 7 6 7\n",
            "Ground truth: 8 4 5 4 4 2 7 6 7\n",
            "Source: eighty seven thousand three hundred and thirteen\n",
            "Prediction:   8 7 3 1 3\n",
            "Ground truth: 8 7 3 1 3\n",
            "Source: seven billion four hundred and ninety two million eight hundred and twenty five thousand two hundred and sixty one\n",
            "Prediction:   7 4 9 2 8 2 5 2 6 1\n",
            "Ground truth: 7 4 9 2 8 2 5 2 6 1\n",
            "Source: seven hundred and nine million four hundred and twenty seven thousand two hundred and eighty nine\n",
            "Prediction:   7 0 9 4 2 7 2 8 9\n",
            "Ground truth: 7 0 9 4 2 7 2 8 9\n",
            "Source: eight million four hundred and seventy thousand two hundred and seventy five\n",
            "Prediction:   8 4 7 0 2 7 5\n",
            "Ground truth: 8 4 7 0 2 7 5\n",
            "Source: two thousand five hundred and seventy\n",
            "Prediction:   2 5 7 0\n",
            "Ground truth: 2 5 7 0\n",
            "Source: six hundred and eighteen thousand seven hundred and sixty six\n",
            "Prediction:   6 1 8 7 6 6\n",
            "Ground truth: 6 1 8 7 6 6\n",
            "Source: three billion four hundred and sixty one million seventy one thousand one hundred and fourteen\n",
            "Prediction:   3 4 6 1 0 7 1 1 1 4\n",
            "Ground truth: 3 4 6 1 0 7 1 1 1 4\n",
            "Source: seventy million seven hundred and thirty five thousand six hundred and forty seven\n",
            "Prediction:   7 0 7 3 5 6 4 7\n",
            "Ground truth: 7 0 7 3 5 6 4 7\n",
            "Source: one billion six hundred and ninety seven million two hundred and eighty six thousand one hundred and eighty two\n",
            "Prediction:   1 6 9 7 2 8 6 1 8 2\n",
            "Ground truth: 1 6 9 7 2 8 6 1 8 2\n",
            "Source: ten thousand nine hundred and seventy seven\n",
            "Prediction:   1 0 9 7 7\n",
            "Ground truth: 1 0 9 7 7\n",
            "Source: fifty seven million nine hundred and forty five thousand nine hundred and fifty three\n",
            "Prediction:   5 7 9 4 5 9 5 3\n",
            "Ground truth: 5 7 9 4 5 9 5 3\n",
            "Source: seven thousand nine hundred and sixty two\n",
            "Prediction:   7 9 6 2\n",
            "Ground truth: 7 9 6 2\n",
            "Source: sixty thousand nine hundred and thirteen\n",
            "Prediction:   6 0 9 1 3\n",
            "Ground truth: 6 0 9 1 3\n",
            "Source: one hundred and forty eight million six hundred and thirty five thousand two hundred and twenty seven\n",
            "Prediction:   1 4 8 6 3 5 2 2 7\n",
            "Ground truth: 1 4 8 6 3 5 2 2 7\n",
            "Source: seven hundred and thirty eight thousand one hundred and seventy three\n",
            "Prediction:   7 3 8 1 7 3\n",
            "Ground truth: 7 3 8 1 7 3\n",
            "Source: eight hundred and ninety eight million seven hundred and fifty thousand two hundred and fifty one\n",
            "Prediction:   8 9 8 7 5 0 2 5 1\n",
            "Ground truth: 8 9 8 7 5 0 2 5 1\n",
            "Source: two hundred and forty nine million eight hundred and seventeen thousand three hundred and two\n",
            "Prediction:   2 4 9 8 1 7 3 0 2\n",
            "Ground truth: 2 4 9 8 1 7 3 0 2\n",
            "Source: nineteen million three hundred and nineteen thousand and seventy four\n",
            "Prediction:   1 9 3 1 9 0 7 4\n",
            "Ground truth: 1 9 3 1 9 0 7 4\n",
            "Source: eight billion two hundred and thirteen million five hundred and eighty three thousand three hundred and fifty six\n",
            "Prediction:   8 2 1 3 5 8 3 3 5 6\n",
            "Ground truth: 8 2 1 3 5 8 3 3 5 6\n",
            "Source: fifty nine thousand eight hundred and seventy one\n",
            "Prediction:   5 9 8 7 1\n",
            "Ground truth: 5 9 8 7 1\n",
            "Source: nine hundred and ninety two thousand three hundred and five\n",
            "Prediction:   9 9 2 3 0 5\n",
            "Ground truth: 9 9 2 3 0 5\n",
            "Source: three billion eight hundred and eleven million seven hundred and eighty thousand and ninety\n",
            "Prediction:   3 8 1 1 7 8 0 0 9 0\n",
            "Ground truth: 3 8 1 1 7 8 0 0 9 0\n",
            "Source: three hundred and fifty five million two hundred and thirty five thousand nine hundred and seventy four\n",
            "Prediction:   3 5 5 2 3 5 9 7 4\n",
            "Ground truth: 3 5 5 2 3 5 9 7 4\n",
            "Source: nine hundred and seventy five million three hundred and two thousand nine hundred and thirty four\n",
            "Prediction:   9 7 5 3 0 2 9 3 4\n",
            "Ground truth: 9 7 5 3 0 2 9 3 4\n",
            "Source: four thousand nine hundred and thirty five\n",
            "Prediction:   4 9 3 5\n",
            "Ground truth: 4 9 3 5\n",
            "Source: eight hundred and twenty eight thousand three hundred and thirty eight\n",
            "Prediction:   8 2 8 3 3 8\n",
            "Ground truth: 8 2 8 3 3 8\n",
            "Source: two thousand four hundred and thirty\n",
            "Prediction:   2 4 3 0\n",
            "Ground truth: 2 4 3 0\n",
            "Source: nine hundred and ninety five thousand eight hundred and three\n",
            "Prediction:   9 9 5 8 0 3\n",
            "Ground truth: 9 9 5 8 0 3\n",
            "Source: six hundred and seventy four thousand nine hundred and ninety three\n",
            "Prediction:   6 7 4 9 9 3\n",
            "Ground truth: 6 7 4 9 9 3\n",
            "Source: seven hundred and thirty nine million three hundred and twenty one thousand four hundred and fifty three\n",
            "Prediction:   7 3 9 3 2 1 4 5 3\n",
            "Ground truth: 7 3 9 3 2 1 4 5 3\n",
            "Source: nine billion eight hundred and eighty nine million six hundred and fifty seven thousand eight hundred and seventy four\n",
            "Prediction:   9 8 8 9 6 5 7 8 7 4\n",
            "Ground truth: 9 8 8 9 6 5 7 8 7 4\n",
            "Source: six thousand seven hundred and forty five\n",
            "Prediction:   6 7 4 5\n",
            "Ground truth: 6 7 4 5\n",
            "Source: ninety two thousand eight hundred and twelve\n",
            "Prediction:   9 2 8 1 2\n",
            "Ground truth: 9 2 8 1 2\n",
            "Source: five million three hundred and seventy nine thousand two hundred and eighty six\n",
            "Prediction:   5 3 7 9 2 8 6\n",
            "Ground truth: 5 3 7 9 2 8 6\n",
            "Source: seventy four million six hundred and five thousand nine hundred and thirty six\n",
            "Prediction:   7 4 6 0 5 9 3 6\n",
            "Ground truth: 7 4 6 0 5 9 3 6\n",
            "Source: eight million ninety three thousand nine hundred and seventy two\n",
            "Prediction:   8 0 9 3 9 7 2\n",
            "Ground truth: 8 0 9 3 9 7 2\n",
            "Source: four hundred and thirty four thousand three hundred and eighty five\n",
            "Prediction:   4 3 4 3 8 5\n",
            "Ground truth: 4 3 4 3 8 5\n",
            "Source: eight hundred and sixty four million one thousand four hundred and twenty five\n",
            "Prediction:   8 6 4 0 1 4 2 5\n",
            "Ground truth: 8 6 4 0 0 1 4 2 5\n",
            "Source: nine million one hundred thousand one hundred and seventy\n",
            "Prediction:   9 1 0 0 1 7 0\n",
            "Ground truth: 9 1 0 0 1 7 0\n",
            "Source: five hundred and seventy seven thousand seven hundred and sixty two\n",
            "Prediction:   5 7 7 7 6 2\n",
            "Ground truth: 5 7 7 7 6 2\n",
            "Source: seven million two hundred and thirty nine thousand three hundred and thirty one\n",
            "Prediction:   7 2 3 9 3 3 1\n",
            "Ground truth: 7 2 3 9 3 3 1\n",
            "Source: eight thousand four hundred and sixty eight\n",
            "Prediction:   8 4 6 8\n",
            "Ground truth: 8 4 6 8\n",
            "Source: ninety six thousand eight hundred and seventeen\n",
            "Prediction:   9 6 8 1 7\n",
            "Ground truth: 9 6 8 1 7\n",
            "Source: five hundred and nineteen thousand seven hundred and one\n",
            "Prediction:   5 1 9 7 0 1\n",
            "Ground truth: 5 1 9 7 0 1\n",
            "Source: twenty nine million seven hundred and sixty two thousand two hundred and thirty nine\n",
            "Prediction:   2 9 7 6 2 2 3 9\n",
            "Ground truth: 2 9 7 6 2 2 3 9\n",
            "Source: thirty six thousand one hundred and sixty two\n",
            "Prediction:   3 6 1 6 2\n",
            "Ground truth: 3 6 1 6 2\n",
            "Source: five hundred and sixty four million two hundred and ninety nine thousand two hundred and twenty\n",
            "Prediction:   5 6 4 2 9 9 2 2 0\n",
            "Ground truth: 5 6 4 2 9 9 2 2 0\n",
            "Source: one hundred and sixty eight million thirty five thousand eight hundred and twenty two\n",
            "Prediction:   1 6 8 0 3 5 8 2 2\n",
            "Ground truth: 1 6 8 0 3 5 8 2 2\n",
            "Source: three hundred and fifty two million ten thousand seven hundred and fifty two\n",
            "Prediction:   3 5 2 0 1 0 7 5 2\n",
            "Ground truth: 3 5 2 0 1 0 7 5 2\n",
            "Source: eight hundred and ninety five thousand six hundred and three\n",
            "Prediction:   8 9 5 6 0 3\n",
            "Ground truth: 8 9 5 6 0 3\n",
            "Source: five hundred and forty five million four hundred and seventy nine thousand eight hundred and eighty eight\n",
            "Prediction:   5 4 5 4 7 9 8 8 8\n",
            "Ground truth: 5 4 5 4 7 9 8 8 8\n",
            "Source: one million two hundred and four thousand and twenty three\n",
            "Prediction:   1 2 0 4 0 2 3\n",
            "Ground truth: 1 2 0 4 0 2 3\n",
            "Source: seventy three thousand nine hundred and eight\n",
            "Prediction:   7 3 9 0 8\n",
            "Ground truth: 7 3 9 0 8\n",
            "Source: four million three hundred and fifty thousand five hundred and eighty four\n",
            "Prediction:   4 3 5 0 5 8 4\n",
            "Ground truth: 4 3 5 0 5 8 4\n",
            "Source: seven billion nine hundred and forty one million eight hundred and twenty three thousand nine hundred and thirty five\n",
            "Prediction:   7 9 4 1 8 2 3 9 3 5\n",
            "Ground truth: 7 9 4 1 8 2 3 9 3 5\n",
            "Source: nine billion three hundred and sixty five million seven hundred and fifty four thousand two hundred and seventy seven\n",
            "Prediction:   9 3 6 5 7 5 4 2 7 7\n",
            "Ground truth: 9 3 6 5 7 5 4 2 7 7\n",
            "Source: eight hundred and twenty four million eight hundred and forty nine thousand eight hundred and six\n",
            "Prediction:   8 2 4 8 4 9 8 0 6\n",
            "Ground truth: 8 2 4 8 4 9 8 0 6\n",
            "Source: nine hundred and seven thousand and twenty one\n",
            "Prediction:   9 0 7 0 2 1\n",
            "Ground truth: 9 0 7 0 2 1\n",
            "Source: five hundred and six thousand nine hundred and forty four\n",
            "Prediction:   5 0 6 9 4 4\n",
            "Ground truth: 5 0 6 9 4 4\n",
            "Source: eight thousand six hundred and eighty three\n",
            "Prediction:   8 6 8 3\n",
            "Ground truth: 8 6 8 3\n",
            "Source: seven million one hundred and ninety three thousand five hundred and eighty\n",
            "Prediction:   7 1 9 3 5 8 0\n",
            "Ground truth: 7 1 9 3 5 8 0\n",
            "Source: three million seven thousand nine hundred and fifty four\n",
            "Prediction:   3 0 0 7 9 5 4\n",
            "Ground truth: 3 0 0 7 9 5 4\n",
            "Source: ninety seven thousand four hundred and thirty three\n",
            "Prediction:   9 7 4 3 3\n",
            "Ground truth: 9 7 4 3 3\n",
            "Source: sixty five million seven hundred and sixty five thousand eight hundred and seventeen\n",
            "Prediction:   6 5 7 6 5 8 1 7\n",
            "Ground truth: 6 5 7 6 5 8 1 7\n",
            "Source: forty three million eight hundred and twenty eight thousand one hundred and twenty\n",
            "Prediction:   4 3 8 2 8 1 2 0\n",
            "Ground truth: 4 3 8 2 8 1 2 0\n",
            "Source: nine billion four hundred and fifty million two hundred and four thousand one hundred and sixty eight\n",
            "Prediction:   9 4 5 0 2 0 4 1 6 8\n",
            "Ground truth: 9 4 5 0 2 0 4 1 6 8\n",
            "Source: ninety two thousand three hundred and ninety eight\n",
            "Prediction:   9 2 3 9 8\n",
            "Ground truth: 9 2 3 9 8\n",
            "Source: two hundred and forty three million three hundred and forty three thousand nine hundred and twenty nine\n",
            "Prediction:   2 4 3 3 4 3 9 2 9\n",
            "Ground truth: 2 4 3 3 4 3 9 2 9\n",
            "Source: seven billion three hundred and seventeen million three hundred and thirty one thousand nine hundred and forty five\n",
            "Prediction:   7 3 1 7 3 3 1 9 4 5\n",
            "Ground truth: 7 3 1 7 3 3 1 9 4 5\n",
            "Source: seven billion nine hundred and eighty seven million one hundred and fifty seven thousand eight hundred and fifty\n",
            "Prediction:   7 9 8 7 1 5 7 8 5 0\n",
            "Ground truth: 7 9 8 7 1 5 7 8 5 0\n",
            "Source: seventy three million eight hundred and sixty thousand and thirty five\n",
            "Prediction:   7 3 8 6 0 0 3 5\n",
            "Ground truth: 7 3 8 6 0 0 3 5\n",
            "Source: five hundred and twenty two thousand eight hundred and twenty four\n",
            "Prediction:   5 2 2 8 2 4\n",
            "Ground truth: 5 2 2 8 2 4\n",
            "Source: seven billion five hundred and fifty three million five hundred and twenty seven thousand two hundred and seventy one\n",
            "Prediction:   7 5 5 3 5 2 7 2 7 1\n",
            "Ground truth: 7 5 5 3 5 2 7 2 7 1\n",
            "Source: one hundred and forty five thousand seven hundred and ninety three\n",
            "Prediction:   1 4 5 7 9 3\n",
            "Ground truth: 1 4 5 7 9 3\n",
            "Source: six million four hundred and nineteen thousand one hundred and seventy six\n",
            "Prediction:   6 4 1 9 1 7 6\n",
            "Ground truth: 6 4 1 9 1 7 6\n",
            "Source: eight hundred and seventy eight thousand two hundred and eleven\n",
            "Prediction:   8 7 8 2 1 1\n",
            "Ground truth: 8 7 8 2 1 1\n",
            "Source: sixty one million three hundred and sixty seven thousand six hundred and thirty six\n",
            "Prediction:   6 1 3 6 7 6 3 6\n",
            "Ground truth: 6 1 3 6 7 6 3 6\n",
            "Source: six billion nine hundred and forty six million seven hundred and eighty two thousand one hundred and sixty two\n",
            "Prediction:   6 9 4 6 7 8 2 1 6 2\n",
            "Ground truth: 6 9 4 6 7 8 2 1 6 2\n",
            "Source: five million two hundred and thirty nine thousand four hundred and three\n",
            "Prediction:   5 2 3 9 4 0 3\n",
            "Ground truth: 5 2 3 9 4 0 3\n",
            "Source: seventy nine million six hundred and sixty four thousand three hundred and fifty eight\n",
            "Prediction:   7 9 6 6 4 3 5 8\n",
            "Ground truth: 7 9 6 6 4 3 5 8\n",
            "Source: six thousand three hundred and ninety eight\n",
            "Prediction:   6 3 9 8\n",
            "Ground truth: 6 3 9 8\n",
            "Source: five thousand and twenty eight\n",
            "Prediction:   5 0 2 8\n",
            "Ground truth: 5 0 2 8\n",
            "Source: three billion one hundred and eighty million nine hundred and eighty one thousand three hundred and seventy\n",
            "Prediction:   3 1 8 0 9 8 1 3 7 0\n",
            "Ground truth: 3 1 8 0 9 8 1 3 7 0\n",
            "Source: sixteen million eight hundred and twenty three thousand eight hundred and seventy eight\n",
            "Prediction:   1 6 8 2 3 8 7 8\n",
            "Ground truth: 1 6 8 2 3 8 7 8\n",
            "Source: eighty seven thousand four hundred and twenty six\n",
            "Prediction:   8 7 4 2 6\n",
            "Ground truth: 8 7 4 2 6\n",
            "Source: six hundred and forty four thousand six hundred and fifty eight\n",
            "Prediction:   6 4 4 6 5 8\n",
            "Ground truth: 6 4 4 6 5 8\n",
            "Source: eighty five million one hundred and ninety thousand two hundred and forty seven\n",
            "Prediction:   8 5 1 9 0 2 4 7\n",
            "Ground truth: 8 5 1 9 0 2 4 7\n",
            "Source: thirty eight million two hundred and thirty four thousand four hundred and seventy two\n",
            "Prediction:   3 8 2 3 4 4 7 2\n",
            "Ground truth: 3 8 2 3 4 4 7 2\n",
            "Source: nine hundred and forty four thousand and eighty\n",
            "Prediction:   9 4 4 0 8 0\n",
            "Ground truth: 9 4 4 0 8 0\n",
            "Source: four million four hundred and forty two thousand six hundred and thirty three\n",
            "Prediction:   4 4 4 2 6 3 3\n",
            "Ground truth: 4 4 4 2 6 3 3\n",
            "Source: six hundred and seventy two thousand one hundred and sixty two\n",
            "Prediction:   6 7 2 1 6 2\n",
            "Ground truth: 6 7 2 1 6 2\n",
            "Source: five hundred and eighty three million six hundred and fifteen thousand and seventy nine\n",
            "Prediction:   5 8 3 6 1 5 0 7 9\n",
            "Ground truth: 5 8 3 6 1 5 0 7 9\n",
            "Source: two hundred and ninety eight million nine hundred and thirteen thousand eight hundred and forty one\n",
            "Prediction:   2 9 8 9 1 3 8 4 1\n",
            "Ground truth: 2 9 8 9 1 3 8 4 1\n",
            "Source: three million one hundred and thirty thousand nine hundred and three\n",
            "Prediction:   3 1 3 0 9 0 3\n",
            "Ground truth: 3 1 3 0 9 0 3\n",
            "Source: seven million eight hundred and eight thousand five hundred and thirty one\n",
            "Prediction:   7 8 0 8 5 3 1\n",
            "Ground truth: 7 8 0 8 5 3 1\n",
            "Source: seventy seven million seven hundred and eight thousand nine hundred and fifty eight\n",
            "Prediction:   7 7 7 0 8 9 5 8\n",
            "Ground truth: 7 7 7 0 8 9 5 8\n",
            "Source: sixty two million seventy thousand eight hundred and ten\n",
            "Prediction:   6 2 0 7 0 8 1 0\n",
            "Ground truth: 6 2 0 7 0 8 1 0\n",
            "Source: seven billion five hundred and twelve million seven hundred and forty eight thousand two hundred and thirty eight\n",
            "Prediction:   7 5 1 2 7 4 8 2 3 8\n",
            "Ground truth: 7 5 1 2 7 4 8 2 3 8\n",
            "Source: two hundred and sixty three thousand seven hundred and ninety three\n",
            "Prediction:   2 6 3 7 9 3\n",
            "Ground truth: 2 6 3 7 9 3\n",
            "Source: seven billion two hundred and seventy one million twenty one thousand eight hundred and twenty one\n",
            "Prediction:   7 2 7 1 0 2 1 8 2 1\n",
            "Ground truth: 7 2 7 1 0 2 1 8 2 1\n",
            "Source: eight million four hundred and eighty five thousand five hundred and seventy six\n",
            "Prediction:   8 4 8 5 5 7 6\n",
            "Ground truth: 8 4 8 5 5 7 6\n",
            "Source: three million nine hundred and twenty one thousand six hundred and thirty three\n",
            "Prediction:   3 9 2 1 6 3 3\n",
            "Ground truth: 3 9 2 1 6 3 3\n",
            "Source: ninety nine million four hundred and eighty five thousand seven hundred and ninety seven\n",
            "Prediction:   9 9 4 8 5 7 9 7\n",
            "Ground truth: 9 9 4 8 5 7 9 7\n",
            "Source: five million six hundred and twenty nine thousand six hundred and sixty five\n",
            "Prediction:   5 6 2 9 6 6 5\n",
            "Ground truth: 5 6 2 9 6 6 5\n",
            "Source: two thousand six hundred and fifty three\n",
            "Prediction:   2 6 5 3\n",
            "Ground truth: 2 6 5 3\n",
            "Source: three hundred and thirty thousand seven hundred and sixty four\n",
            "Prediction:   3 3 0 7 6 4\n",
            "Ground truth: 3 3 0 7 6 4\n",
            "Source: seven billion one hundred and ninety one million nine hundred and forty seven thousand one hundred and forty two\n",
            "Prediction:   7 1 9 1 9 4 7 1 4 2\n",
            "Ground truth: 7 1 9 1 9 4 7 1 4 2\n",
            "Source: seven hundred and sixty two thousand six hundred and eighty eight\n",
            "Prediction:   7 6 2 6 8 8\n",
            "Ground truth: 7 6 2 6 8 8\n",
            "Source: thirty seven thousand five hundred and sixty nine\n",
            "Prediction:   3 7 5 6 9\n",
            "Ground truth: 3 7 5 6 9\n",
            "Source: nine hundred and sixty one million five hundred and forty seven thousand seven hundred and sixty one\n",
            "Prediction:   9 6 1 5 4 7 7 6 1\n",
            "Ground truth: 9 6 1 5 4 7 7 6 1\n",
            "Source: thirteen thousand six hundred and seventy six\n",
            "Prediction:   1 3 6 7 6\n",
            "Ground truth: 1 3 6 7 6\n",
            "Source: fifty five million nine hundred and sixty nine thousand eight hundred and twenty one\n",
            "Prediction:   5 5 9 6 9 8 2 1\n",
            "Ground truth: 5 5 9 6 9 8 2 1\n",
            "Source: two hundred and twenty nine million four hundred and thirty four thousand and seven\n",
            "Prediction:   2 2 9 4 3 4 0 0 7\n",
            "Ground truth: 2 2 9 4 3 4 0 0 7\n",
            "Source: eighty six million one hundred and eighty thousand five hundred and ninety seven\n",
            "Prediction:   8 6 1 8 0 5 9 7\n",
            "Ground truth: 8 6 1 8 0 5 9 7\n",
            "Source: seventy million forty nine thousand five hundred and thirty\n",
            "Prediction:   7 0 0 4 9 5 3 0\n",
            "Ground truth: 7 0 0 4 9 5 3 0\n",
            "Source: two billion five hundred and ninety four million five hundred and eighty thousand and sixty\n",
            "Prediction:   2 5 9 4 5 8 0 0 6 0\n",
            "Ground truth: 2 5 9 4 5 8 0 0 6 0\n",
            "Source: three billion five hundred and thirty nine million three hundred and fifty eight thousand six hundred and forty one\n",
            "Prediction:   3 5 3 9 3 5 8 6 4 1\n",
            "Ground truth: 3 5 3 9 3 5 8 6 4 1\n",
            "Source: twelve million eight hundred and sixty six thousand five hundred and seventy one\n",
            "Prediction:   1 2 8 6 6 5 7 1\n",
            "Ground truth: 1 2 8 6 6 5 7 1\n",
            "Source: forty three thousand eight hundred and twenty seven\n",
            "Prediction:   4 3 8 2 7\n",
            "Ground truth: 4 3 8 2 7\n",
            "Source: six million forty four thousand seven hundred and twenty\n",
            "Prediction:   6 0 4 4 7 2 0\n",
            "Ground truth: 6 0 4 4 7 2 0\n",
            "Source: five thousand six hundred and thirty three\n",
            "Prediction:   5 6 3 3\n",
            "Ground truth: 5 6 3 3\n",
            "Source: nine thousand and forty six\n",
            "Prediction:   9 0 4 6\n",
            "Ground truth: 9 0 4 6\n",
            "Source: fifty nine million three hundred and eighty thousand three hundred and eighty two\n",
            "Prediction:   5 9 3 8 0 3 8 2\n",
            "Ground truth: 5 9 3 8 0 3 8 2\n",
            "Source: one billion eleven million three hundred and twenty five thousand three hundred and sixty three\n",
            "Prediction:   1 0 1 1 3 2 5 3 6 3\n",
            "Ground truth: 1 0 1 1 3 2 5 3 6 3\n",
            "Source: four thousand nine hundred and seventy nine\n",
            "Prediction:   4 9 7 9\n",
            "Ground truth: 4 9 7 9\n",
            "Source: seventy one million one hundred and sixty two thousand seven hundred and fifty nine\n",
            "Prediction:   7 1 1 6 2 7 5 9\n",
            "Ground truth: 7 1 1 6 2 7 5 9\n",
            "Source: two thousand six hundred and twenty one\n",
            "Prediction:   2 6 2 1\n",
            "Ground truth: 2 6 2 1\n",
            "Source: five million seven hundred and forty thousand three hundred and fifty three\n",
            "Prediction:   5 7 4 0 3 5 3\n",
            "Ground truth: 5 7 4 0 3 5 3\n",
            "Source: five hundred and twenty two thousand six hundred\n",
            "Prediction:   5 2 2 6 0 0\n",
            "Ground truth: 5 2 2 6 0 0\n",
            "Source: nine billion four hundred and thirty nine million forty three thousand eight hundred and eighty eight\n",
            "Prediction:   9 4 3 9 0 4 3 8 8 8\n",
            "Ground truth: 9 4 3 9 0 4 3 8 8 8\n",
            "Source: thirty seven million five hundred and twenty nine thousand four hundred and sixty\n",
            "Prediction:   3 7 5 2 9 4 6 0\n",
            "Ground truth: 3 7 5 2 9 4 6 0\n",
            "Source: seventy eight million seven thousand two hundred and sixty five\n",
            "Prediction:   7 8 0 0 7 2 6 5\n",
            "Ground truth: 7 8 0 0 7 2 6 5\n",
            "Source: sixty one million two hundred and four thousand three hundred and thirty five\n",
            "Prediction:   6 1 2 0 4 3 3 5\n",
            "Ground truth: 6 1 2 0 4 3 3 5\n",
            "Source: seven million three hundred and forty nine thousand six hundred and ninety six\n",
            "Prediction:   7 3 4 9 6 9 6\n",
            "Ground truth: 7 3 4 9 6 9 6\n",
            "Source: three billion two hundred and seventy six million three hundred and sixty one thousand six hundred and twenty two\n",
            "Prediction:   3 2 7 6 3 6 1 6 2 2\n",
            "Ground truth: 3 2 7 6 3 6 1 6 2 2\n",
            "Source: six hundred and forty seven million one hundred and eighty eight thousand and twenty nine\n",
            "Prediction:   6 4 7 1 8 8 0 2 9\n",
            "Ground truth: 6 4 7 1 8 8 0 2 9\n",
            "Source: four billion forty five million four hundred and eleven thousand three hundred\n",
            "Prediction:   4 0 4 5 4 1 1 3 0 0\n",
            "Ground truth: 4 0 4 5 4 1 1 3 0 0\n",
            "Source: eight thousand one hundred and forty two\n",
            "Prediction:   8 1 4 2\n",
            "Ground truth: 8 1 4 2\n",
            "Source: five billion six hundred and nine million four hundred and thirteen thousand nine hundred and ninety two\n",
            "Prediction:   5 6 0 9 4 1 3 9 9 2\n",
            "Ground truth: 5 6 0 9 4 1 3 9 9 2\n",
            "Source: one hundred and fifteen million seven hundred and twenty eight thousand nine hundred and sixteen\n",
            "Prediction:   1 1 5 7 2 8 9 1 6\n",
            "Ground truth: 1 1 5 7 2 8 9 1 6\n",
            "Source: two hundred and twenty one thousand six hundred and forty five\n",
            "Prediction:   2 2 1 6 4 5\n",
            "Ground truth: 2 2 1 6 4 5\n",
            "Source: seven thousand two hundred and four\n",
            "Prediction:   7 2 0 4\n",
            "Ground truth: 7 2 0 4\n",
            "Source: six thousand nine hundred and thirty nine\n",
            "Prediction:   6 9 3 9\n",
            "Ground truth: 6 9 3 9\n",
            "Source: three billion four hundred and eighty six million nine hundred and eighteen thousand seven hundred and eleven\n",
            "Prediction:   3 4 8 6 9 1 8 7 1 1\n",
            "Ground truth: 3 4 8 6 9 1 8 7 1 1\n",
            "Source: seven billion one hundred and eighty eight million one hundred and twelve thousand and seventy eight\n",
            "Prediction:   7 1 8 8 1 1 2 0 7 8\n",
            "Ground truth: 7 1 8 8 1 1 2 0 7 8\n",
            "Source: six million three hundred and eighty nine thousand and sixty two\n",
            "Prediction:   6 3 8 9 0 6 2\n",
            "Ground truth: 6 3 8 9 0 6 2\n",
            "Source: two hundred and six million five hundred and forty seven thousand seven hundred and eighty six\n",
            "Prediction:   2 0 6 5 4 7 7 8 6\n",
            "Ground truth: 2 0 6 5 4 7 7 8 6\n",
            "Source: four million two hundred and eighty four thousand nine hundred and forty eight\n",
            "Prediction:   4 2 8 4 9 4 8\n",
            "Ground truth: 4 2 8 4 9 4 8\n",
            "Source: eight thousand eight hundred and fifty two\n",
            "Prediction:   8 8 5 2\n",
            "Ground truth: 8 8 5 2\n",
            "Source: ninety four thousand and thirty six\n",
            "Prediction:   9 4 0 3 6\n",
            "Ground truth: 9 4 0 3 6\n",
            "Source: twenty one thousand one hundred and twenty six\n",
            "Prediction:   2 1 1 2 6\n",
            "Ground truth: 2 1 1 2 6\n",
            "Source: twelve thousand one hundred\n",
            "Prediction:   1 2 1 0 0\n",
            "Ground truth: 1 2 1 0 0\n",
            "Source: five million three hundred and thirty five thousand one hundred and thirty eight\n",
            "Prediction:   5 3 3 5 1 3 8\n",
            "Ground truth: 5 3 3 5 1 3 8\n",
            "Source: four million three hundred and eighty eight thousand three hundred and nine\n",
            "Prediction:   4 3 8 8 3 0 9\n",
            "Ground truth: 4 3 8 8 3 0 9\n",
            "Source: one thousand and eighty eight\n",
            "Prediction:   1 0 8 8\n",
            "Ground truth: 1 0 8 8\n",
            "Source: ninety one thousand seven hundred and thirty three\n",
            "Prediction:   9 1 7 3 3\n",
            "Ground truth: 9 1 7 3 3\n",
            "Source: two million four hundred and thirty six thousand five hundred and sixty nine\n",
            "Prediction:   2 4 3 6 5 6 9\n",
            "Ground truth: 2 4 3 6 5 6 9\n",
            "Source: eight million five hundred and sixty three thousand one hundred and seventy one\n",
            "Prediction:   8 5 6 3 1 7 1\n",
            "Ground truth: 8 5 6 3 1 7 1\n",
            "Source: three hundred and eighteen thousand and eighty\n",
            "Prediction:   3 1 8 0 8 0\n",
            "Ground truth: 3 1 8 0 8 0\n",
            "Source: three million nine hundred and sixty three thousand one hundred and thirty two\n",
            "Prediction:   3 9 6 3 1 3 2\n",
            "Ground truth: 3 9 6 3 1 3 2\n",
            "Source: six million six hundred and seventy thousand six hundred and sixty\n",
            "Prediction:   6 6 7 0 6 6 0\n",
            "Ground truth: 6 6 7 0 6 6 0\n",
            "Source: twenty eight million one hundred and thirty two thousand three hundred and ninety two\n",
            "Prediction:   2 8 1 3 2 3 9 2\n",
            "Ground truth: 2 8 1 3 2 3 9 2\n",
            "Source: fifteen million nine hundred and eighty seven thousand four hundred and fifty nine\n",
            "Prediction:   1 5 9 8 7 4 5 9\n",
            "Ground truth: 1 5 9 8 7 4 5 9\n",
            "Source: five hundred and forty four thousand five hundred\n",
            "Prediction:   5 4 4 5 0 0\n",
            "Ground truth: 5 4 4 5 0 0\n",
            "Source: nine thousand three hundred and seventy six\n",
            "Prediction:   9 3 7 6\n",
            "Ground truth: 9 3 7 6\n",
            "Source: seventy six thousand nine hundred and twenty eight\n",
            "Prediction:   7 6 9 2 8\n",
            "Ground truth: 7 6 9 2 8\n",
            "Source: eight hundred and thirty three thousand and eighteen\n",
            "Prediction:   8 3 3 0 1 8\n",
            "Ground truth: 8 3 3 0 1 8\n",
            "Source: six billion four hundred and forty four million eight hundred and thirty two thousand and eleven\n",
            "Prediction:   6 4 4 4 8 3 2 0 1 1\n",
            "Ground truth: 6 4 4 4 8 3 2 0 1 1\n",
            "Source: twenty one thousand three hundred and thirty three\n",
            "Prediction:   2 1 3 3 3\n",
            "Ground truth: 2 1 3 3 3\n",
            "Source: thirty thousand three hundred and eighty three\n",
            "Prediction:   3 0 3 8 3\n",
            "Ground truth: 3 0 3 8 3\n",
            "Source: fifty million four hundred and ninety nine thousand and ninety four\n",
            "Prediction:   5 0 4 9 9 0 9 4\n",
            "Ground truth: 5 0 4 9 9 0 9 4\n",
            "Source: twenty one thousand and twenty two\n",
            "Prediction:   2 1 0 2 2\n",
            "Ground truth: 2 1 0 2 2\n",
            "Source: eight thousand and ninety two\n",
            "Prediction:   8 0 9 2\n",
            "Ground truth: 8 0 9 2\n",
            "Source: six thousand nine hundred and twenty nine\n",
            "Prediction:   6 9 2 9\n",
            "Ground truth: 6 9 2 9\n",
            "Source: forty five million five hundred and thirty nine thousand five hundred and thirty\n",
            "Prediction:   4 5 5 3 9 5 3 0\n",
            "Ground truth: 4 5 5 3 9 5 3 0\n",
            "Source: eighty million eight hundred and sixty nine thousand and ninety two\n",
            "Prediction:   8 0 8 6 9 0 9 2\n",
            "Ground truth: 8 0 8 6 9 0 9 2\n",
            "Source: eighty seven million one hundred and seventy three thousand two hundred and ninety two\n",
            "Prediction:   8 7 1 7 3 2 9 2\n",
            "Ground truth: 8 7 1 7 3 2 9 2\n",
            "Source: fifty four thousand five hundred and fifty three\n",
            "Prediction:   5 4 5 5 3\n",
            "Ground truth: 5 4 5 5 3\n",
            "Source: six thousand six hundred and eighty\n",
            "Prediction:   6 6 8 0\n",
            "Ground truth: 6 6 8 0\n",
            "Source: forty five thousand seven hundred and sixty four\n",
            "Prediction:   4 5 7 6 4\n",
            "Ground truth: 4 5 7 6 4\n",
            "Source: six hundred and eighty six million two hundred and three thousand four hundred and two\n",
            "Prediction:   6 8 6 2 0 3 4 0 2\n",
            "Ground truth: 6 8 6 2 0 3 4 0 2\n",
            "Source: three hundred and fifty five million eight hundred and seventy nine thousand one hundred and twenty five\n",
            "Prediction:   3 5 5 8 7 9 1 2 5\n",
            "Ground truth: 3 5 5 8 7 9 1 2 5\n",
            "Source: one billion eight hundred and four million five hundred and ninety nine thousand three hundred and sixty seven\n",
            "Prediction:   1 8 0 4 5 9 9 3 6 7\n",
            "Ground truth: 1 8 0 4 5 9 9 3 6 7\n",
            "Source: six billion two hundred and fifty four million forty one thousand two hundred and seventy one\n",
            "Prediction:   6 2 5 4 0 4 1 2 7 1\n",
            "Ground truth: 6 2 5 4 0 4 1 2 7 1\n",
            "Source: nine hundred and fifty three million six hundred and thirty six thousand eight hundred and eighty five\n",
            "Prediction:   9 5 3 6 3 6 8 8 5\n",
            "Ground truth: 9 5 3 6 3 6 8 8 5\n",
            "Source: six hundred and eighty six thousand six hundred and seventy two\n",
            "Prediction:   6 8 6 6 7 2\n",
            "Ground truth: 6 8 6 6 7 2\n",
            "Source: ninety eight million three hundred and seventy six thousand five hundred and thirty five\n",
            "Prediction:   9 8 3 7 6 5 3 5\n",
            "Ground truth: 9 8 3 7 6 5 3 5\n",
            "Source: three thousand five hundred and ninety six\n",
            "Prediction:   3 5 9 6\n",
            "Ground truth: 3 5 9 6\n",
            "Source: nine billion six hundred million one hundred and ninety two thousand four hundred and forty eight\n",
            "Prediction:   9 6 0 1 9 2 4 4 8\n",
            "Ground truth: 9 6 0 0 1 9 2 4 4 8\n",
            "Source: six billion four hundred and nine million five hundred and sixty six thousand four hundred and eighty\n",
            "Prediction:   6 4 0 9 5 6 6 4 8 0\n",
            "Ground truth: 6 4 0 9 5 6 6 4 8 0\n",
            "Source: one billion five hundred and sixty million nine hundred and twenty four thousand three hundred and twenty four\n",
            "Prediction:   1 5 6 0 9 2 4 3 2 4\n",
            "Ground truth: 1 5 6 0 9 2 4 3 2 4\n",
            "Source: four thousand two hundred and forty\n",
            "Prediction:   4 2 4 0\n",
            "Ground truth: 4 2 4 0\n",
            "Source: thirty seven thousand eight hundred and ninety eight\n",
            "Prediction:   3 7 8 9 8\n",
            "Ground truth: 3 7 8 9 8\n",
            "Source: five billion two hundred and thirty six million seventy eight thousand one hundred and fifty five\n",
            "Prediction:   5 2 3 6 0 7 8 1 5 5\n",
            "Ground truth: 5 2 3 6 0 7 8 1 5 5\n",
            "Source: ninety nine million two hundred and eighty one thousand four hundred and twenty three\n",
            "Prediction:   9 9 2 8 1 4 2 3\n",
            "Ground truth: 9 9 2 8 1 4 2 3\n",
            "Source: eight hundred and forty six million two hundred and thirty three thousand six hundred and twenty eight\n",
            "Prediction:   8 4 6 2 3 3 6 2 8\n",
            "Ground truth: 8 4 6 2 3 3 6 2 8\n",
            "Source: nine million six hundred and twenty two thousand three hundred and fourteen\n",
            "Prediction:   9 6 2 2 3 1 4\n",
            "Ground truth: 9 6 2 2 3 1 4\n",
            "Source: four thousand one hundred and sixty three\n",
            "Prediction:   4 1 6 3\n",
            "Ground truth: 4 1 6 3\n",
            "Source: one hundred and twenty nine thousand six hundred and forty seven\n",
            "Prediction:   1 2 9 6 4 7\n",
            "Ground truth: 1 2 9 6 4 7\n",
            "Source: twenty eight thousand nine hundred and ninety eight\n",
            "Prediction:   2 8 9 9 8\n",
            "Ground truth: 2 8 9 9 8\n",
            "Source: nine thousand one hundred and ninety\n",
            "Prediction:   9 1 9 0\n",
            "Ground truth: 9 1 9 0\n",
            "Source: nine thousand four hundred and eighty three\n",
            "Prediction:   9 4 8 3\n",
            "Ground truth: 9 4 8 3\n",
            "Source: three billion one hundred and sixty five million two hundred and fifty thousand four hundred and eighty eight\n",
            "Prediction:   3 1 6 5 2 5 0 4 8 8\n",
            "Ground truth: 3 1 6 5 2 5 0 4 8 8\n",
            "Source: seven thousand three hundred and ninety four\n",
            "Prediction:   7 3 9 4\n",
            "Ground truth: 7 3 9 4\n",
            "Source: fifty nine million seven hundred and six thousand two hundred and eighty five\n",
            "Prediction:   5 9 7 0 6 2 8 5\n",
            "Ground truth: 5 9 7 0 6 2 8 5\n",
            "Source: two hundred and eighty nine million seven hundred and thirteen thousand and sixty eight\n",
            "Prediction:   2 8 9 7 1 3 0 6 8\n",
            "Ground truth: 2 8 9 7 1 3 0 6 8\n",
            "Source: sixty one thousand seven hundred and forty seven\n",
            "Prediction:   6 1 7 4 7\n",
            "Ground truth: 6 1 7 4 7\n",
            "Source: three hundred and sixty thousand four hundred\n",
            "Prediction:   3 6 0 4 0 0\n",
            "Ground truth: 3 6 0 4 0 0\n",
            "Source: fifty seven thousand nine hundred and thirty seven\n",
            "Prediction:   5 7 9 3 7\n",
            "Ground truth: 5 7 9 3 7\n",
            "Source: forty nine million nine hundred and ten thousand seven hundred and seventy eight\n",
            "Prediction:   4 9 9 1 0 7 7 8\n",
            "Ground truth: 4 9 9 1 0 7 7 8\n",
            "Source: twelve million one hundred and sixty three thousand three hundred and seventy seven\n",
            "Prediction:   1 2 1 6 3 3 7 7\n",
            "Ground truth: 1 2 1 6 3 3 7 7\n",
            "Source: six thousand six hundred and thirty three\n",
            "Prediction:   6 6 3 3\n",
            "Ground truth: 6 6 3 3\n",
            "Source: six hundred and twenty nine thousand three hundred and sixteen\n",
            "Prediction:   6 2 9 3 1 6\n",
            "Ground truth: 6 2 9 3 1 6\n",
            "Source: five thousand seven hundred and seventy three\n",
            "Prediction:   5 7 7 3\n",
            "Ground truth: 5 7 7 3\n",
            "Source: eighty five million six hundred and three thousand five hundred and eighty two\n",
            "Prediction:   8 5 6 0 3 5 8 2\n",
            "Ground truth: 8 5 6 0 3 5 8 2\n",
            "Source: eight million one hundred and twenty three thousand six hundred and forty one\n",
            "Prediction:   8 1 2 3 6 4 1\n",
            "Ground truth: 8 1 2 3 6 4 1\n",
            "Source: seven hundred and fifty one million nine hundred and thirty nine thousand eight hundred and forty two\n",
            "Prediction:   7 5 1 9 3 9 8 4 2\n",
            "Ground truth: 7 5 1 9 3 9 8 4 2\n",
            "Source: ninety two thousand and twenty nine\n",
            "Prediction:   9 2 0 2 9\n",
            "Ground truth: 9 2 0 2 9\n",
            "Source: fifty five thousand three hundred and twenty six\n",
            "Prediction:   5 5 3 2 6\n",
            "Ground truth: 5 5 3 2 6\n",
            "Source: seven billion one hundred and thirty six million two hundred and seventy five thousand eight hundred and eighty seven\n",
            "Prediction:   7 1 3 6 2 7 5 8 8 7\n",
            "Ground truth: 7 1 3 6 2 7 5 8 8 7\n",
            "Source: six hundred and eighteen million eight hundred and eighty one thousand seven hundred and eighty nine\n",
            "Prediction:   6 1 8 8 8 1 7 8 9\n",
            "Ground truth: 6 1 8 8 8 1 7 8 9\n",
            "Source: sixty seven thousand four hundred and fifty seven\n",
            "Prediction:   6 7 4 5 7\n",
            "Ground truth: 6 7 4 5 7\n",
            "Source: seven hundred and ninety one thousand eight hundred and twenty three\n",
            "Prediction:   7 9 1 8 2 3\n",
            "Ground truth: 7 9 1 8 2 3\n",
            "Source: five hundred and seventy four million three hundred and seventy three thousand eight hundred and eighty eight\n",
            "Prediction:   5 7 4 3 7 3 8 8 8\n",
            "Ground truth: 5 7 4 3 7 3 8 8 8\n",
            "Source: eight thousand seven hundred and seven\n",
            "Prediction:   8 7 0 7\n",
            "Ground truth: 8 7 0 7\n",
            "Source: eighty six million five hundred and sixty nine thousand two hundred and eighty six\n",
            "Prediction:   8 6 5 6 9 2 8 6\n",
            "Ground truth: 8 6 5 6 9 2 8 6\n",
            "Source: forty eight thousand two hundred and eighty four\n",
            "Prediction:   4 8 2 8 4\n",
            "Ground truth: 4 8 2 8 4\n",
            "Source: eighty two thousand two hundred and sixty four\n",
            "Prediction:   8 2 2 6 4\n",
            "Ground truth: 8 2 2 6 4\n",
            "Source: three thousand four hundred and forty one\n",
            "Prediction:   3 4 4 1\n",
            "Ground truth: 3 4 4 1\n",
            "Source: five hundred and sixty two thousand two hundred and forty\n",
            "Prediction:   5 6 2 2 4 0\n",
            "Ground truth: 5 6 2 2 4 0\n",
            "Source: two thousand and ninety three\n",
            "Prediction:   2 0 9 3\n",
            "Ground truth: 2 0 9 3\n",
            "Source: seventy million six thousand four hundred and forty seven\n",
            "Prediction:   7 0 0 0 6 4 4 7\n",
            "Ground truth: 7 0 0 0 6 4 4 7\n",
            "Source: seventy nine thousand and twenty two\n",
            "Prediction:   7 9 0 2 2\n",
            "Ground truth: 7 9 0 2 2\n",
            "Source: eight hundred and sixty eight thousand three hundred and twenty one\n",
            "Prediction:   8 6 8 3 2 1\n",
            "Ground truth: 8 6 8 3 2 1\n",
            "Source: seven hundred and thirty three million six hundred and eight thousand five hundred and two\n",
            "Prediction:   7 3 3 6 0 8 5 0 2\n",
            "Ground truth: 7 3 3 6 0 8 5 0 2\n",
            "Source: four hundred and fifty three thousand six hundred and forty six\n",
            "Prediction:   4 5 3 6 4 6\n",
            "Ground truth: 4 5 3 6 4 6\n",
            "Source: eighty three million five hundred and five thousand nine hundred and eighty three\n",
            "Prediction:   8 3 5 0 5 9 8 3\n",
            "Ground truth: 8 3 5 0 5 9 8 3\n",
            "Source: three hundred and seventy five million nine hundred and forty four thousand two hundred and thirty\n",
            "Prediction:   3 7 5 9 4 4 2 3 0\n",
            "Ground truth: 3 7 5 9 4 4 2 3 0\n",
            "Source: twenty three million three hundred and twenty thousand seven hundred and forty five\n",
            "Prediction:   2 3 3 2 0 7 4 5\n",
            "Ground truth: 2 3 3 2 0 7 4 5\n",
            "Source: six thousand three hundred and fifty one\n",
            "Prediction:   6 3 5 1\n",
            "Ground truth: 6 3 5 1\n",
            "Source: nine hundred and thirty thousand three hundred and ninety seven\n",
            "Prediction:   9 3 0 3 9 7\n",
            "Ground truth: 9 3 0 3 9 7\n",
            "Source: six hundred and fifty two thousand three hundred and forty\n",
            "Prediction:   6 5 2 3 4 0\n",
            "Ground truth: 6 5 2 3 4 0\n",
            "Source: six million two hundred and thirty five thousand eight hundred and forty four\n",
            "Prediction:   6 2 3 5 8 4 4\n",
            "Ground truth: 6 2 3 5 8 4 4\n",
            "Source: five billion nine hundred and eighty five million two hundred and five thousand four hundred and eighty seven\n",
            "Prediction:   5 9 8 5 2 0 5 4 8 7\n",
            "Ground truth: 5 9 8 5 2 0 5 4 8 7\n",
            "Source: seven billion three hundred and fifty seven million five hundred and one thousand eight hundred and twenty\n",
            "Prediction:   7 3 5 7 5 0 1 8 2 0\n",
            "Ground truth: 7 3 5 7 5 0 1 8 2 0\n",
            "Source: three million one hundred and ten thousand eight hundred and seventy\n",
            "Prediction:   3 1 1 0 8 7 0\n",
            "Ground truth: 3 1 1 0 8 7 0\n",
            "Source: four million three hundred and eighty five thousand one hundred and fifty one\n",
            "Prediction:   4 3 8 5 1 5 1\n",
            "Ground truth: 4 3 8 5 1 5 1\n",
            "Source: eight hundred and two thousand two hundred and nine\n",
            "Prediction:   8 0 2 2 0 9\n",
            "Ground truth: 8 0 2 2 0 9\n",
            "Source: one million four hundred and thirty one thousand eight hundred and sixty three\n",
            "Prediction:   1 4 3 1 8 6 3\n",
            "Ground truth: 1 4 3 1 8 6 3\n",
            "Source: forty four million two hundred and two thousand five hundred and sixty eight\n",
            "Prediction:   4 4 2 0 2 5 6 8\n",
            "Ground truth: 4 4 2 0 2 5 6 8\n",
            "Source: eight million nine hundred and twenty one thousand seven hundred and thirty one\n",
            "Prediction:   8 9 2 1 7 3 1\n",
            "Ground truth: 8 9 2 1 7 3 1\n",
            "Source: forty five million four hundred and twenty six thousand one hundred and fifty\n",
            "Prediction:   4 5 4 2 6 1 5 0\n",
            "Ground truth: 4 5 4 2 6 1 5 0\n",
            "Source: forty two million five hundred and sixty nine thousand five hundred and eighty two\n",
            "Prediction:   4 2 5 6 9 5 8 2\n",
            "Ground truth: 4 2 5 6 9 5 8 2\n",
            "Source: five hundred and sixty eight million eighty three thousand four hundred and thirty nine\n",
            "Prediction:   5 6 8 0 8 3 4 3 9\n",
            "Ground truth: 5 6 8 0 8 3 4 3 9\n",
            "Source: sixteen thousand six hundred and thirteen\n",
            "Prediction:   1 6 6 1 3\n",
            "Ground truth: 1 6 6 1 3\n",
            "Source: eighty million seventy thousand six hundred and thirty one\n",
            "Prediction:   8 0 0 7 0 6 3 1\n",
            "Ground truth: 8 0 0 7 0 6 3 1\n",
            "Source: forty seven million four hundred and ninety nine thousand two hundred and forty\n",
            "Prediction:   4 7 4 9 9 2 4 0\n",
            "Ground truth: 4 7 4 9 9 2 4 0\n",
            "Source: four billion eight hundred and seventy one million five hundred and twenty three thousand and forty five\n",
            "Prediction:   4 8 7 1 5 2 3 0 4 5\n",
            "Ground truth: 4 8 7 1 5 2 3 0 4 5\n",
            "Source: eight million six hundred and sixty four thousand seven hundred and thirty eight\n",
            "Prediction:   8 6 6 4 7 3 8\n",
            "Ground truth: 8 6 6 4 7 3 8\n",
            "Source: one million six hundred and eighty nine thousand seven hundred and seventy seven\n",
            "Prediction:   1 6 8 9 7 7 7\n",
            "Ground truth: 1 6 8 9 7 7 7\n",
            "Source: two hundred and fifty five million nine hundred and forty six thousand six hundred and seventy four\n",
            "Prediction:   2 5 5 9 4 6 6 7 4\n",
            "Ground truth: 2 5 5 9 4 6 6 7 4\n",
            "Source: nine hundred and eleven thousand one hundred and twenty nine\n",
            "Prediction:   9 1 1 1 2 9\n",
            "Ground truth: 9 1 1 1 2 9\n",
            "Source: seventy nine thousand six hundred and thirty\n",
            "Prediction:   7 9 6 3 0\n",
            "Ground truth: 7 9 6 3 0\n",
            "Source: seventy three thousand four hundred and seventy two\n",
            "Prediction:   7 3 4 7 2\n",
            "Ground truth: 7 3 4 7 2\n",
            "Source: six million eight hundred and seventy five thousand one hundred and sixty six\n",
            "Prediction:   6 8 7 5 1 6 6\n",
            "Ground truth: 6 8 7 5 1 6 6\n",
            "Source: six billion two hundred and twelve million twenty nine thousand nine hundred and forty three\n",
            "Prediction:   6 2 1 2 0 2 9 9 4 3\n",
            "Ground truth: 6 2 1 2 0 2 9 9 4 3\n",
            "Source: fifty five million four hundred and eighty three thousand one hundred and three\n",
            "Prediction:   5 5 4 8 3 1 0 3\n",
            "Ground truth: 5 5 4 8 3 1 0 3\n",
            "Source: thirty five million nine hundred and seventy four thousand six hundred and three\n",
            "Prediction:   3 5 9 7 4 6 0 3\n",
            "Ground truth: 3 5 9 7 4 6 0 3\n",
            "Source: nine hundred and twenty eight thousand five hundred and fifty four\n",
            "Prediction:   9 2 8 5 5 4\n",
            "Ground truth: 9 2 8 5 5 4\n",
            "Source: twenty four thousand three hundred and ninety nine\n",
            "Prediction:   2 4 3 9 9\n",
            "Ground truth: 2 4 3 9 9\n",
            "Source: seven hundred and fifty six thousand eight hundred and sixty eight\n",
            "Prediction:   7 5 6 8 6 8\n",
            "Ground truth: 7 5 6 8 6 8\n",
            "Source: seven billion seven hundred and four million nine hundred and forty nine thousand and eighty three\n",
            "Prediction:   7 7 0 4 9 4 9 0 8 3\n",
            "Ground truth: 7 7 0 4 9 4 9 0 8 3\n",
            "Source: nine million eight hundred and ninety three thousand nine hundred and forty\n",
            "Prediction:   9 8 9 3 9 4 0\n",
            "Ground truth: 9 8 9 3 9 4 0\n",
            "Source: seven billion seven hundred and thirty nine million two hundred and five thousand and three\n",
            "Prediction:   7 7 3 9 2 0 5 0 0 3\n",
            "Ground truth: 7 7 3 9 2 0 5 0 0 3\n",
            "Source: nine hundred and forty thousand eight hundred and ninety three\n",
            "Prediction:   9 4 0 8 9 3\n",
            "Ground truth: 9 4 0 8 9 3\n",
            "Source: one hundred and twenty eight thousand three hundred and thirty one\n",
            "Prediction:   1 2 8 3 3 1\n",
            "Ground truth: 1 2 8 3 3 1\n",
            "Source: thirty three million thirty six thousand four hundred and ninety two\n",
            "Prediction:   3 3 0 3 6 4 9 2\n",
            "Ground truth: 3 3 0 3 6 4 9 2\n",
            "Source: seven hundred and sixty one thousand four hundred and twenty nine\n",
            "Prediction:   7 6 1 4 2 9\n",
            "Ground truth: 7 6 1 4 2 9\n",
            "Source: seventy four million two hundred and twenty seven thousand and fifty one\n",
            "Prediction:   7 4 2 2 7 0 5 1\n",
            "Ground truth: 7 4 2 2 7 0 5 1\n",
            "Source: seven hundred thousand eight hundred and seven\n",
            "Prediction:   7 0 0 8 0 7\n",
            "Ground truth: 7 0 0 8 0 7\n",
            "Source: twenty one thousand five hundred and ninety\n",
            "Prediction:   2 1 5 9 0\n",
            "Ground truth: 2 1 5 9 0\n",
            "Source: seventy four million forty seven thousand five hundred and five\n",
            "Prediction:   7 4 0 4 7 5 0 5\n",
            "Ground truth: 7 4 0 4 7 5 0 5\n",
            "Source: seven hundred and forty two thousand five hundred and seventy seven\n",
            "Prediction:   7 4 2 5 7 7\n",
            "Ground truth: 7 4 2 5 7 7\n",
            "Source: eight hundred and twenty six million five hundred and ninety three thousand two hundred and seventy\n",
            "Prediction:   8 2 6 5 9 3 2 7 0\n",
            "Ground truth: 8 2 6 5 9 3 2 7 0\n",
            "Source: five thousand nine hundred and ninety four\n",
            "Prediction:   5 9 9 4\n",
            "Ground truth: 5 9 9 4\n",
            "Source: twenty seven thousand six hundred and nine\n",
            "Prediction:   2 7 6 0 9\n",
            "Ground truth: 2 7 6 0 9\n",
            "Source: one million two hundred and twenty thousand eight hundred and twenty six\n",
            "Prediction:   1 2 2 0 8 2 6\n",
            "Ground truth: 1 2 2 0 8 2 6\n",
            "Source: two thousand nine hundred and ninety\n",
            "Prediction:   2 9 9 0\n",
            "Ground truth: 2 9 9 0\n",
            "Source: three hundred million two hundred and seventy four thousand nine hundred and twenty\n",
            "Prediction:   3 0 0 2 7 4 9 2 0\n",
            "Ground truth: 3 0 0 2 7 4 9 2 0\n",
            "Source: one million seven hundred and sixty thousand nine hundred and nineteen\n",
            "Prediction:   1 7 6 0 9 1 9\n",
            "Ground truth: 1 7 6 0 9 1 9\n",
            "Source: two hundred and sixty two thousand nine hundred and forty eight\n",
            "Prediction:   2 6 2 9 4 8\n",
            "Ground truth: 2 6 2 9 4 8\n",
            "Source: four thousand six hundred and seventy three\n",
            "Prediction:   4 6 7 3\n",
            "Ground truth: 4 6 7 3\n",
            "Source: six hundred and fifty one million eight hundred and fifty thousand nine hundred and eighty one\n",
            "Prediction:   6 5 1 8 5 0 9 8 1\n",
            "Ground truth: 6 5 1 8 5 0 9 8 1\n",
            "Source: ten thousand two hundred and ninety five\n",
            "Prediction:   1 0 2 9 5\n",
            "Ground truth: 1 0 2 9 5\n",
            "Source: five million seven hundred and eighteen thousand three hundred and seventy two\n",
            "Prediction:   5 7 1 8 3 7 2\n",
            "Ground truth: 5 7 1 8 3 7 2\n",
            "Source: forty one million eight hundred and twenty eight thousand five hundred and eighty three\n",
            "Prediction:   4 1 8 2 8 5 8 3\n",
            "Ground truth: 4 1 8 2 8 5 8 3\n",
            "Source: nine hundred and ninety five thousand six hundred and sixty six\n",
            "Prediction:   9 9 5 6 6 6\n",
            "Ground truth: 9 9 5 6 6 6\n",
            "Source: two billion seven hundred and seventy million nine hundred and twenty six thousand six hundred\n",
            "Prediction:   2 7 7 0 9 2 6 6 0 0\n",
            "Ground truth: 2 7 7 0 9 2 6 6 0 0\n",
            "Source: eight hundred and twenty million two hundred and twelve thousand and ten\n",
            "Prediction:   8 2 0 2 1 2 0 1 0\n",
            "Ground truth: 8 2 0 2 1 2 0 1 0\n",
            "Source: three million four hundred and eighty six thousand two hundred and sixty eight\n",
            "Prediction:   3 4 8 6 2 6 8\n",
            "Ground truth: 3 4 8 6 2 6 8\n",
            "Source: five thousand four hundred and one\n",
            "Prediction:   5 4 0 1\n",
            "Ground truth: 5 4 0 1\n",
            "Source: sixty three million five hundred and fifteen thousand one hundred and twenty five\n",
            "Prediction:   6 3 5 1 5 1 2 5\n",
            "Ground truth: 6 3 5 1 5 1 2 5\n",
            "Source: one billion five hundred and ninety two million eight hundred and two thousand seven hundred and sixty one\n",
            "Prediction:   1 5 9 2 8 0 2 7 6 1\n",
            "Ground truth: 1 5 9 2 8 0 2 7 6 1\n",
            "Source: six hundred and fifty two million eight hundred and forty eight thousand three hundred and twenty four\n",
            "Prediction:   6 5 2 8 4 8 3 2 4\n",
            "Ground truth: 6 5 2 8 4 8 3 2 4\n",
            "Source: fifty million three hundred and fifty seven thousand two hundred and seventy two\n",
            "Prediction:   5 0 3 5 7 2 7 2\n",
            "Ground truth: 5 0 3 5 7 2 7 2\n",
            "Source: one hundred and six million four hundred and seventy eight thousand three hundred and seventy seven\n",
            "Prediction:   1 0 6 4 7 8 3 7 7\n",
            "Ground truth: 1 0 6 4 7 8 3 7 7\n",
            "Source: nine hundred and fifty three thousand two hundred\n",
            "Prediction:   9 5 3 2 0 0\n",
            "Ground truth: 9 5 3 2 0 0\n",
            "Source: six million five hundred and seventy thousand seven hundred and eighty five\n",
            "Prediction:   6 5 7 0 7 8 5\n",
            "Ground truth: 6 5 7 0 7 8 5\n",
            "Source: seven thousand eight hundred and one\n",
            "Prediction:   7 8 0 1\n",
            "Ground truth: 7 8 0 1\n",
            "Source: eight million five hundred and fifty five thousand nine hundred and ninety three\n",
            "Prediction:   8 5 5 5 9 9 3\n",
            "Ground truth: 8 5 5 5 9 9 3\n",
            "Source: sixty nine million fifty three thousand three hundred and five\n",
            "Prediction:   6 9 0 5 3 3 0 5\n",
            "Ground truth: 6 9 0 5 3 3 0 5\n",
            "Source: seven hundred and eighty six thousand five hundred and eleven\n",
            "Prediction:   7 8 6 5 1 1\n",
            "Ground truth: 7 8 6 5 1 1\n",
            "Source: four billion eight hundred and sixty eight million seven hundred and twenty one thousand four hundred and sixty six\n",
            "Prediction:   4 8 6 8 7 2 1 4 6 6\n",
            "Ground truth: 4 8 6 8 7 2 1 4 6 6\n",
            "Source: eight thousand three hundred and forty seven\n",
            "Prediction:   8 3 4 7\n",
            "Ground truth: 8 3 4 7\n",
            "Source: one thousand two hundred and nine\n",
            "Prediction:   1 2 0 9\n",
            "Ground truth: 1 2 0 9\n",
            "Source: one billion four hundred and thirty seven million five hundred and seventy eight thousand one hundred and seventy eight\n",
            "Prediction:   1 4 3 7 5 7 8 1 7 8\n",
            "Ground truth: 1 4 3 7 5 7 8 1 7 8\n",
            "Source: eighty six thousand eight hundred and forty four\n",
            "Prediction:   8 6 8 4 4\n",
            "Ground truth: 8 6 8 4 4\n",
            "Source: six hundred and sixty five million nine hundred and thirty eight thousand nine hundred and forty four\n",
            "Prediction:   6 6 5 9 3 8 9 4 4\n",
            "Ground truth: 6 6 5 9 3 8 9 4 4\n",
            "Source: one hundred and seven thousand six hundred and sixty seven\n",
            "Prediction:   1 0 7 6 6 7\n",
            "Ground truth: 1 0 7 6 6 7\n",
            "Source: six thousand one hundred and twenty eight\n",
            "Prediction:   6 1 2 8\n",
            "Ground truth: 6 1 2 8\n",
            "Source: ninety four thousand five hundred and seventy six\n",
            "Prediction:   9 4 5 7 6\n",
            "Ground truth: 9 4 5 7 6\n",
            "Source: five thousand four hundred and twenty seven\n",
            "Prediction:   5 4 2 7\n",
            "Ground truth: 5 4 2 7\n",
            "Source: ninety two thousand eight hundred and thirty nine\n",
            "Prediction:   9 2 8 3 9\n",
            "Ground truth: 9 2 8 3 9\n",
            "Source: four hundred and thirty seven million four hundred and seventy one thousand one hundred and forty seven\n",
            "Prediction:   4 3 7 4 7 1 1 4 7\n",
            "Ground truth: 4 3 7 4 7 1 1 4 7\n",
            "Source: two hundred and sixty nine thousand and forty\n",
            "Prediction:   2 6 9 0 4 0\n",
            "Ground truth: 2 6 9 0 4 0\n",
            "Source: two million eighty six thousand nine hundred and twenty six\n",
            "Prediction:   2 0 8 6 9 2 6\n",
            "Ground truth: 2 0 8 6 9 2 6\n",
            "Source: five hundred and seventeen million two hundred and fifty eight thousand and eighty one\n",
            "Prediction:   5 1 7 2 5 8 0 8 1\n",
            "Ground truth: 5 1 7 2 5 8 0 8 1\n",
            "Source: nine hundred and seventeen million one hundred and sixty seven thousand and forty nine\n",
            "Prediction:   9 1 7 1 6 7 0 4 9\n",
            "Ground truth: 9 1 7 1 6 7 0 4 9\n",
            "Source: six billion one hundred and forty two million two hundred and fifty three thousand two hundred and five\n",
            "Prediction:   6 1 4 2 2 5 3 2 0 5\n",
            "Ground truth: 6 1 4 2 2 5 3 2 0 5\n",
            "Source: nine million one hundred and six thousand seven hundred and thirty one\n",
            "Prediction:   9 1 0 6 7 3 1\n",
            "Ground truth: 9 1 0 6 7 3 1\n",
            "Source: twenty five million one hundred and forty seven thousand two hundred and sixty three\n",
            "Prediction:   2 5 1 4 7 2 6 3\n",
            "Ground truth: 2 5 1 4 7 2 6 3\n",
            "Source: sixty seven thousand and fourteen\n",
            "Prediction:   6 7 0 1 4\n",
            "Ground truth: 6 7 0 1 4\n",
            "Source: eighty one thousand eight hundred and fourteen\n",
            "Prediction:   8 1 8 1 4\n",
            "Ground truth: 8 1 8 1 4\n",
            "Source: seventy nine million seven hundred and thirty three thousand three hundred and eighty five\n",
            "Prediction:   7 9 7 3 3 3 8 5\n",
            "Ground truth: 7 9 7 3 3 3 8 5\n",
            "Source: two hundred and thirteen million nine hundred and sixteen thousand six hundred and sixty eight\n",
            "Prediction:   2 1 3 9 1 6 6 6 8\n",
            "Ground truth: 2 1 3 9 1 6 6 6 8\n",
            "Source: thirty eight thousand three hundred and ten\n",
            "Prediction:   3 8 3 1 0\n",
            "Ground truth: 3 8 3 1 0\n",
            "Source: two hundred and seventy four million eighteen thousand five hundred and fifteen\n",
            "Prediction:   2 7 4 0 0 8 5 1 5\n",
            "Ground truth: 2 7 4 0 1 8 5 1 5\n",
            "Source: five hundred and fifty three thousand two hundred and thirty nine\n",
            "Prediction:   5 5 3 2 3 9\n",
            "Ground truth: 5 5 3 2 3 9\n",
            "Source: one billion eight hundred and fifty eight million four hundred and twenty five thousand five hundred and eighteen\n",
            "Prediction:   1 8 5 8 4 2 5 5 1 8\n",
            "Ground truth: 1 8 5 8 4 2 5 5 1 8\n",
            "Source: four billion eight hundred and sixty six million one hundred and seventy five thousand one hundred and twenty six\n",
            "Prediction:   4 8 6 6 1 7 5 1 2 6\n",
            "Ground truth: 4 8 6 6 1 7 5 1 2 6\n",
            "Source: eight billion seven hundred and fifty million fifty four thousand five hundred\n",
            "Prediction:   8 7 5 0 0 5 4 5 0 0\n",
            "Ground truth: 8 7 5 0 0 5 4 5 0 0\n",
            "Source: three hundred and thirty one thousand nine hundred and eighty eight\n",
            "Prediction:   3 3 1 9 8 8\n",
            "Ground truth: 3 3 1 9 8 8\n",
            "Source: six hundred and seventy two million three hundred and twenty four thousand seven hundred and fourteen\n",
            "Prediction:   6 7 2 3 2 4 7 1 4\n",
            "Ground truth: 6 7 2 3 2 4 7 1 4\n",
            "Source: three thousand five hundred and eleven\n",
            "Prediction:   3 5 1 1\n",
            "Ground truth: 3 5 1 1\n",
            "Source: five hundred and seven million five hundred and eighty five thousand five hundred and sixty one\n",
            "Prediction:   5 0 7 5 8 5 5 6 1\n",
            "Ground truth: 5 0 7 5 8 5 5 6 1\n",
            "Source: one hundred and eighty nine million eight hundred and thirty three thousand nine hundred and seventy seven\n",
            "Prediction:   1 8 9 8 3 3 9 7 7\n",
            "Ground truth: 1 8 9 8 3 3 9 7 7\n",
            "Source: seven million five hundred and fifty five thousand eight hundred and fifty four\n",
            "Prediction:   7 5 5 5 8 5 4\n",
            "Ground truth: 7 5 5 5 8 5 4\n",
            "Source: five billion nine hundred and forty five million one hundred and twenty one thousand two hundred and twenty three\n",
            "Prediction:   5 9 4 5 1 2 1 2 2 3\n",
            "Ground truth: 5 9 4 5 1 2 1 2 2 3\n",
            "Source: four thousand and seventy one\n",
            "Prediction:   4 0 7 1\n",
            "Ground truth: 4 0 7 1\n",
            "Source: two hundred and eleven million six hundred and sixty two thousand five hundred and eighty six\n",
            "Prediction:   2 1 1 6 6 2 5 8 6\n",
            "Ground truth: 2 1 1 6 6 2 5 8 6\n",
            "Source: seven hundred and seven million nine hundred and sixty six thousand eight hundred and thirteen\n",
            "Prediction:   7 0 7 9 6 6 8 1 3\n",
            "Ground truth: 7 0 7 9 6 6 8 1 3\n",
            "Source: eighty nine thousand two hundred and seventeen\n",
            "Prediction:   8 9 2 1 7\n",
            "Ground truth: 8 9 2 1 7\n",
            "Source: six hundred and thirteen thousand two hundred and sixty\n",
            "Prediction:   6 1 3 2 6 0\n",
            "Ground truth: 6 1 3 2 6 0\n",
            "Source: thirty two million two hundred and thirty thousand four hundred and twenty seven\n",
            "Prediction:   3 2 2 3 0 4 2 7\n",
            "Ground truth: 3 2 2 3 0 4 2 7\n",
            "Source: three billion eight hundred and sixty four million six hundred and twenty nine thousand and ninety\n",
            "Prediction:   3 8 6 4 6 2 9 0 9 0\n",
            "Ground truth: 3 8 6 4 6 2 9 0 9 0\n",
            "Source: nine thousand four hundred and eighty\n",
            "Prediction:   9 4 8 0\n",
            "Ground truth: 9 4 8 0\n",
            "Source: nine billion sixty four million seven hundred and four thousand nine hundred and one\n",
            "Prediction:   9 0 6 4 7 0 4 9 0 1\n",
            "Ground truth: 9 0 6 4 7 0 4 9 0 1\n",
            "Source: nine million five hundred and sixty one thousand and forty\n",
            "Prediction:   9 5 6 1 0 4 0\n",
            "Ground truth: 9 5 6 1 0 4 0\n",
            "Source: one thousand seven hundred and twenty seven\n",
            "Prediction:   1 7 2 7\n",
            "Ground truth: 1 7 2 7\n",
            "Source: forty one million four hundred and twenty eight thousand four hundred and thirty\n",
            "Prediction:   4 1 4 2 8 4 3 0\n",
            "Ground truth: 4 1 4 2 8 4 3 0\n",
            "Source: seventy seven thousand and eighteen\n",
            "Prediction:   7 7 0 1 8\n",
            "Ground truth: 7 7 0 1 8\n",
            "Source: seven hundred and twenty five thousand four hundred and fifty two\n",
            "Prediction:   7 2 5 4 5 2\n",
            "Ground truth: 7 2 5 4 5 2\n",
            "Source: seven billion nine hundred and eighty nine million eight hundred and fifty six thousand one hundred and five\n",
            "Prediction:   7 9 8 9 8 5 6 1 0 5\n",
            "Ground truth: 7 9 8 9 8 5 6 1 0 5\n",
            "Source: ninety one million four hundred and twenty eight thousand four hundred and ninety one\n",
            "Prediction:   9 1 4 2 8 4 9 1\n",
            "Ground truth: 9 1 4 2 8 4 9 1\n",
            "Source: forty nine million six hundred and ninety eight thousand eight hundred and seven\n",
            "Prediction:   4 9 6 9 8 8 0 7\n",
            "Ground truth: 4 9 6 9 8 8 0 7\n",
            "Source: eighty six million three hundred and seventy one thousand three hundred and twenty seven\n",
            "Prediction:   8 6 3 7 1 3 2 7\n",
            "Ground truth: 8 6 3 7 1 3 2 7\n",
            "Source: twenty eight million five hundred and thirty seven thousand seven hundred and three\n",
            "Prediction:   2 8 5 3 7 7 0 3\n",
            "Ground truth: 2 8 5 3 7 7 0 3\n",
            "Source: fifty thousand four hundred and fifty six\n",
            "Prediction:   5 0 4 5 6\n",
            "Ground truth: 5 0 4 5 6\n",
            "Source: four hundred and seventeen thousand three hundred and thirty six\n",
            "Prediction:   4 1 7 3 3 6\n",
            "Ground truth: 4 1 7 3 3 6\n",
            "Source: two thousand two hundred and seventy three\n",
            "Prediction:   2 2 7 3\n",
            "Ground truth: 2 2 7 3\n",
            "Source: seven hundred and seventeen thousand seven hundred and fifteen\n",
            "Prediction:   7 1 7 7 1 5\n",
            "Ground truth: 7 1 7 7 1 5\n",
            "Source: thirty five thousand four hundred and fifty five\n",
            "Prediction:   3 5 4 5 5\n",
            "Ground truth: 3 5 4 5 5\n",
            "Source: two billion one hundred and twenty eight million five hundred and sixty seven thousand three hundred and seventy nine\n",
            "Prediction:   2 1 2 8 5 6 7 3 7 9\n",
            "Ground truth: 2 1 2 8 5 6 7 3 7 9\n",
            "Source: one million four hundred and twenty three thousand one hundred and seventy two\n",
            "Prediction:   1 4 2 3 1 7 2\n",
            "Ground truth: 1 4 2 3 1 7 2\n",
            "Source: four billion nine hundred and twenty five million four hundred and fifty nine thousand three hundred and twenty six\n",
            "Prediction:   4 9 2 5 4 5 9 3 2 6\n",
            "Ground truth: 4 9 2 5 4 5 9 3 2 6\n",
            "Source: eighty million ninety two thousand and forty seven\n",
            "Prediction:   8 0 0 9 2 0 4 7\n",
            "Ground truth: 8 0 0 9 2 0 4 7\n",
            "Source: five hundred and ninety nine million eight hundred and forty six thousand three hundred and thirty eight\n",
            "Prediction:   5 9 9 8 4 6 3 3 8\n",
            "Ground truth: 5 9 9 8 4 6 3 3 8\n",
            "Source: two hundred and twenty six million two hundred and ninety seven thousand five hundred and twenty five\n",
            "Prediction:   2 2 6 2 9 7 5 2 5\n",
            "Ground truth: 2 2 6 2 9 7 5 2 5\n",
            "Source: four hundred and ninety one thousand eight hundred and thirty eight\n",
            "Prediction:   4 9 1 8 3 8\n",
            "Ground truth: 4 9 1 8 3 8\n",
            "Source: two thousand seven hundred and thirty one\n",
            "Prediction:   2 7 3 1\n",
            "Ground truth: 2 7 3 1\n",
            "Source: one hundred and eighty four million eight hundred and one thousand seven hundred and fifty one\n",
            "Prediction:   1 8 4 8 0 1 7 5 1\n",
            "Ground truth: 1 8 4 8 0 1 7 5 1\n",
            "Source: one billion five hundred and thirteen million two hundred and eighty six thousand four hundred and sixty eight\n",
            "Prediction:   1 5 1 3 2 8 6 4 6 8\n",
            "Ground truth: 1 5 1 3 2 8 6 4 6 8\n",
            "Source: six hundred and eighty million two hundred and forty two thousand eight hundred and forty nine\n",
            "Prediction:   6 8 0 2 4 2 8 4 9\n",
            "Ground truth: 6 8 0 2 4 2 8 4 9\n",
            "Source: nine thousand four hundred and fifteen\n",
            "Prediction:   9 4 1 5\n",
            "Ground truth: 9 4 1 5\n",
            "Source: one million two hundred and forty two thousand eight hundred and ninety six\n",
            "Prediction:   1 2 4 2 8 9 6\n",
            "Ground truth: 1 2 4 2 8 9 6\n",
            "Source: forty eight million three hundred and ninety three thousand and thirty two\n",
            "Prediction:   4 8 3 9 3 0 3 2\n",
            "Ground truth: 4 8 3 9 3 0 3 2\n",
            "Source: sixty thousand eight hundred and fifty\n",
            "Prediction:   6 0 8 5 0\n",
            "Ground truth: 6 0 8 5 0\n",
            "Source: ninety nine thousand six hundred and forty eight\n",
            "Prediction:   9 9 6 4 8\n",
            "Ground truth: 9 9 6 4 8\n",
            "Source: six billion seventy six million two hundred and eighteen thousand and ninety\n",
            "Prediction:   6 0 7 6 2 1 8 0 9 0\n",
            "Ground truth: 6 0 7 6 2 1 8 0 9 0\n",
            "Source: one billion nine hundred and twenty four million seven hundred and eighteen thousand four hundred and eighty three\n",
            "Prediction:   1 9 2 4 7 1 8 4 8 3\n",
            "Ground truth: 1 9 2 4 7 1 8 4 8 3\n",
            "Source: nineteen thousand two hundred and sixteen\n",
            "Prediction:   1 9 2 1 6\n",
            "Ground truth: 1 9 2 1 6\n",
            "Source: three hundred and eighty one million nine hundred and seventy nine thousand five hundred and ninety two\n",
            "Prediction:   3 8 1 9 7 9 5 9 2\n",
            "Ground truth: 3 8 1 9 7 9 5 9 2\n",
            "Source: three hundred and eighty two thousand six hundred and seventy six\n",
            "Prediction:   3 8 2 6 7 6\n",
            "Ground truth: 3 8 2 6 7 6\n",
            "Source: two million six hundred and ninety six thousand five hundred and sixty\n",
            "Prediction:   2 6 9 6 5 6 0\n",
            "Ground truth: 2 6 9 6 5 6 0\n",
            "Source: six hundred and fifty three million seven hundred and twenty six thousand seven hundred and eighty seven\n",
            "Prediction:   6 5 3 7 2 6 7 8 7\n",
            "Ground truth: 6 5 3 7 2 6 7 8 7\n",
            "Source: four million seven hundred and fourteen thousand four hundred and twenty one\n",
            "Prediction:   4 7 1 4 4 2 1\n",
            "Ground truth: 4 7 1 4 4 2 1\n",
            "Source: forty six million one hundred and thirty nine thousand eight hundred and seventy two\n",
            "Prediction:   4 6 1 3 9 8 7 2\n",
            "Ground truth: 4 6 1 3 9 8 7 2\n",
            "Source: eighty four thousand six hundred and thirty four\n",
            "Prediction:   8 4 6 3 4\n",
            "Ground truth: 8 4 6 3 4\n",
            "Source: one million four hundred and ninety two thousand five hundred and thirty two\n",
            "Prediction:   1 4 9 2 5 3 2\n",
            "Ground truth: 1 4 9 2 5 3 2\n",
            "Source: two hundred and seventy seven million six hundred and sixteen thousand six hundred and two\n",
            "Prediction:   2 7 7 6 1 6 6 0 2\n",
            "Ground truth: 2 7 7 6 1 6 6 0 2\n",
            "Source: two million one hundred and fifty one thousand eight hundred and fifty nine\n",
            "Prediction:   2 1 5 1 8 5 9\n",
            "Ground truth: 2 1 5 1 8 5 9\n",
            "Source: two million one hundred and seventeen thousand two hundred and twenty five\n",
            "Prediction:   2 1 1 7 2 2 5\n",
            "Ground truth: 2 1 1 7 2 2 5\n",
            "Source: seven thousand eight hundred and sixty three\n",
            "Prediction:   7 8 6 3\n",
            "Ground truth: 7 8 6 3\n",
            "Source: seventy seven thousand and seventy three\n",
            "Prediction:   7 7 0 7 3\n",
            "Ground truth: 7 7 0 7 3\n",
            "Source: one hundred and seventy nine thousand two hundred and thirty nine\n",
            "Prediction:   1 7 9 2 3 9\n",
            "Ground truth: 1 7 9 2 3 9\n",
            "Source: fifty two thousand five hundred and seventy three\n",
            "Prediction:   5 2 5 7 3\n",
            "Ground truth: 5 2 5 7 3\n",
            "Source: six billion three hundred and fifty nine million one hundred and forty one thousand three hundred and fifty two\n",
            "Prediction:   6 3 5 9 1 4 1 3 5 2\n",
            "Ground truth: 6 3 5 9 1 4 1 3 5 2\n",
            "Source: five hundred and fifty five million six hundred and thirty four thousand four hundred and thirty\n",
            "Prediction:   5 5 5 6 3 4 4 3 0\n",
            "Ground truth: 5 5 5 6 3 4 4 3 0\n",
            "Source: thirty nine million six hundred and fifty four thousand and eighty seven\n",
            "Prediction:   3 9 6 5 4 0 8 7\n",
            "Ground truth: 3 9 6 5 4 0 8 7\n",
            "Source: fifty two thousand six hundred and thirty nine\n",
            "Prediction:   5 2 6 3 9\n",
            "Ground truth: 5 2 6 3 9\n",
            "Source: fifty five million three hundred and thirty four thousand nine hundred and eighty one\n",
            "Prediction:   5 5 3 3 4 9 8 1\n",
            "Ground truth: 5 5 3 3 4 9 8 1\n",
            "Source: eight million nine hundred and ninety three thousand one hundred and seventy one\n",
            "Prediction:   8 9 9 3 1 7 1\n",
            "Ground truth: 8 9 9 3 1 7 1\n",
            "Source: six hundred and eighty five million one hundred and eighty nine thousand six hundred and forty four\n",
            "Prediction:   6 8 5 1 8 9 6 4 4\n",
            "Ground truth: 6 8 5 1 8 9 6 4 4\n",
            "Source: nineteen million three hundred and sixty seven thousand nine hundred and thirty\n",
            "Prediction:   1 9 3 6 7 9 3 0\n",
            "Ground truth: 1 9 3 6 7 9 3 0\n",
            "Source: two billion one hundred and thirty four million eight hundred and nineteen thousand and seventy six\n",
            "Prediction:   2 1 3 4 8 1 9 0 7 6\n",
            "Ground truth: 2 1 3 4 8 1 9 0 7 6\n",
            "Source: seven hundred and fifteen thousand and sixty four\n",
            "Prediction:   7 1 5 0 6 4\n",
            "Ground truth: 7 1 5 0 6 4\n",
            "Source: fifty four thousand seven hundred and eighty\n",
            "Prediction:   5 4 7 8 0\n",
            "Ground truth: 5 4 7 8 0\n",
            "Source: nine hundred and forty thousand six hundred and eighty five\n",
            "Prediction:   9 4 0 6 8 5\n",
            "Ground truth: 9 4 0 6 8 5\n",
            "Source: eighteen million seven hundred and forty seven thousand and twenty six\n",
            "Prediction:   1 8 7 4 7 0 2 6\n",
            "Ground truth: 1 8 7 4 7 0 2 6\n",
            "Source: three hundred and ninety thousand seven hundred and eighty six\n",
            "Prediction:   3 9 0 7 8 6\n",
            "Ground truth: 3 9 0 7 8 6\n",
            "Source: eight hundred and forty two thousand three hundred and twenty three\n",
            "Prediction:   8 4 2 3 2 3\n",
            "Ground truth: 8 4 2 3 2 3\n",
            "Source: nine thousand one hundred and seventy nine\n",
            "Prediction:   9 1 7 9\n",
            "Ground truth: 9 1 7 9\n",
            "Source: fourteen thousand nine hundred and forty seven\n",
            "Prediction:   1 4 9 4 7\n",
            "Ground truth: 1 4 9 4 7\n",
            "Source: three billion sixty seven million four hundred and ten thousand seven hundred and fifteen\n",
            "Prediction:   3 0 6 7 4 1 0 7 1 5\n",
            "Ground truth: 3 0 6 7 4 1 0 7 1 5\n",
            "Source: seven million ninety six thousand eight hundred and eighty three\n",
            "Prediction:   7 0 9 6 8 8 3\n",
            "Ground truth: 7 0 9 6 8 8 3\n",
            "Source: nine million eight hundred and forty one thousand eight hundred and twenty five\n",
            "Prediction:   9 8 4 1 8 2 5\n",
            "Ground truth: 9 8 4 1 8 2 5\n",
            "Source: thirty two thousand two hundred and eighty one\n",
            "Prediction:   3 2 2 8 1\n",
            "Ground truth: 3 2 2 8 1\n",
            "Source: seventeen million seven hundred and sixty seven thousand and sixty five\n",
            "Prediction:   1 7 7 6 7 0 6 5\n",
            "Ground truth: 1 7 7 6 7 0 6 5\n",
            "Source: seventy nine thousand one hundred and fifty eight\n",
            "Prediction:   7 9 1 5 8\n",
            "Ground truth: 7 9 1 5 8\n",
            "Source: six thousand four hundred and forty two\n",
            "Prediction:   6 4 4 2\n",
            "Ground truth: 6 4 4 2\n",
            "Source: five million seven hundred and fifty two thousand and seventy eight\n",
            "Prediction:   5 7 5 2 0 7 8\n",
            "Ground truth: 5 7 5 2 0 7 8\n",
            "Source: four million nine hundred and fifty four thousand six hundred and thirty\n",
            "Prediction:   4 9 5 4 6 3 0\n",
            "Ground truth: 4 9 5 4 6 3 0\n",
            "Source: eight thousand four hundred and eighty three\n",
            "Prediction:   8 4 8 3\n",
            "Ground truth: 8 4 8 3\n",
            "Source: eight hundred and eleven thousand and seventy four\n",
            "Prediction:   8 1 1 0 7 4\n",
            "Ground truth: 8 1 1 0 7 4\n",
            "Source: one thousand one hundred and thirty nine\n",
            "Prediction:   1 1 3 9\n",
            "Ground truth: 1 1 3 9\n",
            "Source: five thousand five hundred and ninety nine\n",
            "Prediction:   5 5 9 9\n",
            "Ground truth: 5 5 9 9\n",
            "Source: two thousand seven hundred and eighty six\n",
            "Prediction:   2 7 8 6\n",
            "Ground truth: 2 7 8 6\n",
            "Source: four hundred million seven hundred and fifty five thousand six hundred and twenty\n",
            "Prediction:   4 0 0 7 5 5 6 2 0\n",
            "Ground truth: 4 0 0 7 5 5 6 2 0\n",
            "Source: forty three million five hundred and forty thousand eight hundred and fifty four\n",
            "Prediction:   4 3 5 4 0 8 5 4\n",
            "Ground truth: 4 3 5 4 0 8 5 4\n",
            "Source: six billion one hundred and eighty two million one hundred and fifty six thousand eight hundred and eighty four\n",
            "Prediction:   6 1 8 2 1 5 6 8 8 4\n",
            "Ground truth: 6 1 8 2 1 5 6 8 8 4\n",
            "Source: eight million eight hundred and ninety three thousand seven hundred and seventy nine\n",
            "Prediction:   8 8 9 3 7 7 9\n",
            "Ground truth: 8 8 9 3 7 7 9\n",
            "Source: ninety one million nine hundred and sixty seven thousand one hundred and eighty one\n",
            "Prediction:   9 1 9 6 7 1 8 1\n",
            "Ground truth: 9 1 9 6 7 1 8 1\n",
            "Source: four hundred and fifty three million three hundred and thirty thousand nine hundred and seventy three\n",
            "Prediction:   4 5 3 3 3 0 9 7 3\n",
            "Ground truth: 4 5 3 3 3 0 9 7 3\n",
            "Source: twenty three thousand nine hundred and eighty three\n",
            "Prediction:   2 3 9 8 3\n",
            "Ground truth: 2 3 9 8 3\n",
            "Source: eight thousand three hundred and eighty eight\n",
            "Prediction:   8 3 8 8\n",
            "Ground truth: 8 3 8 8\n",
            "Source: nine hundred and eighty three thousand seven hundred and five\n",
            "Prediction:   9 8 3 7 0 5\n",
            "Ground truth: 9 8 3 7 0 5\n",
            "Source: six billion one hundred and six million seven hundred and ninety one thousand four hundred and thirty one\n",
            "Prediction:   6 1 0 6 7 9 1 4 3 1\n",
            "Ground truth: 6 1 0 6 7 9 1 4 3 1\n",
            "Source: two billion forty six million nine hundred and twelve thousand one hundred and fourteen\n",
            "Prediction:   2 0 4 6 9 1 2 1 1 4\n",
            "Ground truth: 2 0 4 6 9 1 2 1 1 4\n",
            "Source: seventy eight million eight hundred and nineteen thousand four hundred and fifty seven\n",
            "Prediction:   7 8 8 1 9 4 5 7\n",
            "Ground truth: 7 8 8 1 9 4 5 7\n",
            "Source: forty four thousand five hundred and eighty four\n",
            "Prediction:   4 4 5 8 4\n",
            "Ground truth: 4 4 5 8 4\n",
            "Source: nine hundred and seventy six thousand two hundred and ninety seven\n",
            "Prediction:   9 7 6 2 9 7\n",
            "Ground truth: 9 7 6 2 9 7\n",
            "Source: five million seventy two thousand eight hundred and fifty\n",
            "Prediction:   5 0 7 2 8 5 0\n",
            "Ground truth: 5 0 7 2 8 5 0\n",
            "Source: six thousand one hundred and six\n",
            "Prediction:   6 1 0 6\n",
            "Ground truth: 6 1 0 6\n",
            "Source: one hundred and ninety one million one hundred and fifty four thousand five hundred and forty two\n",
            "Prediction:   1 9 1 1 5 4 5 4 2\n",
            "Ground truth: 1 9 1 1 5 4 5 4 2\n",
            "Source: eight billion eight hundred and ninety three million seven hundred and thirty eight thousand seven hundred and seventy four\n",
            "Prediction:   8 8 9 3 7 3 8 7 7 4\n",
            "Ground truth: 8 8 9 3 7 3 8 7 7 4\n",
            "Source: five billion nine hundred and twenty one million one hundred and thirty four thousand one hundred and fifty eight\n",
            "Prediction:   5 9 2 1 1 3 4 1 5 8\n",
            "Ground truth: 5 9 2 1 1 3 4 1 5 8\n",
            "Source: seventy six thousand seven hundred and eight\n",
            "Prediction:   7 6 7 0 8\n",
            "Ground truth: 7 6 7 0 8\n",
            "Source: six billion two hundred and eighty eight million nine hundred and thirty three thousand four hundred and sixteen\n",
            "Prediction:   6 2 8 8 9 3 3 4 1 6\n",
            "Ground truth: 6 2 8 8 9 3 3 4 1 6\n",
            "Source: nine billion seven hundred and fourteen million six hundred and fifty five thousand two hundred and thirty one\n",
            "Prediction:   9 7 1 4 6 5 5 2 3 1\n",
            "Ground truth: 9 7 1 4 6 5 5 2 3 1\n",
            "Source: three billion four hundred and seventeen million nine hundred and thirty thousand four hundred and twelve\n",
            "Prediction:   3 4 1 7 9 3 0 4 1 2\n",
            "Ground truth: 3 4 1 7 9 3 0 4 1 2\n",
            "Source: sixty three thousand four hundred and nineteen\n",
            "Prediction:   6 3 4 1 9\n",
            "Ground truth: 6 3 4 1 9\n",
            "Source: nine billion eight hundred and two million three hundred and thirty seven thousand seven hundred and eighty two\n",
            "Prediction:   9 8 0 2 3 3 7 7 8 2\n",
            "Ground truth: 9 8 0 2 3 3 7 7 8 2\n",
            "Source: two hundred and sixty seven thousand two hundred and eighteen\n",
            "Prediction:   2 6 7 2 1 8\n",
            "Ground truth: 2 6 7 2 1 8\n",
            "Source: one billion seven hundred and fifty nine million three hundred and seventy three thousand three hundred and one\n",
            "Prediction:   1 7 5 9 3 7 3 3 0 1\n",
            "Ground truth: 1 7 5 9 3 7 3 3 0 1\n",
            "Source: five hundred and twelve million two hundred and forty two thousand and thirty four\n",
            "Prediction:   5 1 2 2 4 2 0 3 4\n",
            "Ground truth: 5 1 2 2 4 2 0 3 4\n",
            "Source: five thousand two hundred and ninety\n",
            "Prediction:   5 2 9 0\n",
            "Ground truth: 5 2 9 0\n",
            "Source: nine hundred and twenty four million nine hundred and seventy seven thousand six hundred and eighty three\n",
            "Prediction:   9 2 4 9 7 7 6 8 3\n",
            "Ground truth: 9 2 4 9 7 7 6 8 3\n",
            "Source: five hundred and fourteen million one hundred and ten thousand seven hundred and sixty one\n",
            "Prediction:   5 1 4 1 1 0 7 6 1\n",
            "Ground truth: 5 1 4 1 1 0 7 6 1\n",
            "Source: eight billion four hundred and fourteen million two hundred and thirty thousand four hundred and ninety two\n",
            "Prediction:   8 4 1 4 2 3 0 4 9 2\n",
            "Ground truth: 8 4 1 4 2 3 0 4 9 2\n",
            "Source: eight billion five hundred and ninety seven million seven hundred and sixty nine thousand one hundred and twenty\n",
            "Prediction:   8 5 9 7 7 6 9 1 2 0\n",
            "Ground truth: 8 5 9 7 7 6 9 1 2 0\n",
            "Source: ninety four thousand six hundred and seventy seven\n",
            "Prediction:   9 4 6 7 7\n",
            "Ground truth: 9 4 6 7 7\n",
            "Source: three hundred and ninety three thousand three hundred and seventy four\n",
            "Prediction:   3 9 3 3 7 4\n",
            "Ground truth: 3 9 3 3 7 4\n",
            "Source: eighty eight thousand and nine\n",
            "Prediction:   8 8 0 0 9\n",
            "Ground truth: 8 8 0 0 9\n",
            "Source: ninety nine million three hundred and ninety five thousand three hundred and eighty eight\n",
            "Prediction:   9 9 3 9 5 3 8 8\n",
            "Ground truth: 9 9 3 9 5 3 8 8\n",
            "Source: thirty nine million seven hundred and fifty five thousand two hundred and twenty five\n",
            "Prediction:   3 9 7 5 5 2 2 5\n",
            "Ground truth: 3 9 7 5 5 2 2 5\n",
            "Source: eighteen million two hundred and eighty two thousand two hundred and nine\n",
            "Prediction:   1 8 2 8 2 2 0 9\n",
            "Ground truth: 1 8 2 8 2 2 0 9\n",
            "Source: seventy three million four hundred and fifty three thousand nine hundred and twenty two\n",
            "Prediction:   7 3 4 5 3 9 2 2\n",
            "Ground truth: 7 3 4 5 3 9 2 2\n",
            "Source: four billion eight hundred and sixty two million six hundred and eighty six thousand two hundred and sixty eight\n",
            "Prediction:   4 8 6 2 6 8 6 2 6 8\n",
            "Ground truth: 4 8 6 2 6 8 6 2 6 8\n",
            "Source: eight million six hundred and forty four thousand four hundred and two\n",
            "Prediction:   8 6 4 4 4 0 2\n",
            "Ground truth: 8 6 4 4 4 0 2\n",
            "Source: seventy one million two hundred and forty nine thousand and two\n",
            "Prediction:   7 1 2 4 9 0 0 2\n",
            "Ground truth: 7 1 2 4 9 0 0 2\n",
            "Source: ten thousand five hundred\n",
            "Prediction:   1 0 5 0 0\n",
            "Ground truth: 1 0 5 0 0\n",
            "Source: sixty nine million three hundred and five thousand nine hundred and fifty nine\n",
            "Prediction:   6 9 3 0 5 9 5 9\n",
            "Ground truth: 6 9 3 0 5 9 5 9\n",
            "Source: three billion eight hundred and twenty five million three hundred and fifty three thousand six hundred and ninety seven\n",
            "Prediction:   3 8 2 5 3 5 3 6 9 7\n",
            "Ground truth: 3 8 2 5 3 5 3 6 9 7\n",
            "Source: four million six hundred and fifty two thousand four hundred and sixty three\n",
            "Prediction:   4 6 5 2 4 6 3\n",
            "Ground truth: 4 6 5 2 4 6 3\n",
            "Source: three thousand three hundred and thirty three\n",
            "Prediction:   3 3 3 3\n",
            "Ground truth: 3 3 3 3\n",
            "Source: nine thousand eight hundred and fifty two\n",
            "Prediction:   9 8 5 2\n",
            "Ground truth: 9 8 5 2\n",
            "Source: seven hundred and sixty five million fifty nine thousand six hundred and thirty three\n",
            "Prediction:   7 6 5 0 5 9 6 3 3\n",
            "Ground truth: 7 6 5 0 5 9 6 3 3\n",
            "Source: seventy two thousand two hundred and ninety two\n",
            "Prediction:   7 2 2 9 2\n",
            "Ground truth: 7 2 2 9 2\n",
            "Source: one hundred and thirty three million seven hundred and fifty seven thousand eight hundred and eighty six\n",
            "Prediction:   1 3 3 7 5 7 8 8 6\n",
            "Ground truth: 1 3 3 7 5 7 8 8 6\n",
            "Source: nine hundred and sixty eight million three hundred and ninety five thousand seven hundred and thirty three\n",
            "Prediction:   9 6 8 3 9 5 7 3 3\n",
            "Ground truth: 9 6 8 3 9 5 7 3 3\n",
            "Source: eight thousand seven hundred and twenty five\n",
            "Prediction:   8 7 2 5\n",
            "Ground truth: 8 7 2 5\n",
            "Source: eight billion one hundred and seventy one million seven hundred and sixty seven thousand and seventy one\n",
            "Prediction:   8 1 7 1 7 6 7 0 7 1\n",
            "Ground truth: 8 1 7 1 7 6 7 0 7 1\n",
            "Source: two billion seven hundred and thirty five million nine hundred and sixty thousand five hundred and eighty one\n",
            "Prediction:   2 7 3 5 9 6 0 5 8 1\n",
            "Ground truth: 2 7 3 5 9 6 0 5 8 1\n",
            "Source: fourteen thousand eight hundred and thirty four\n",
            "Prediction:   1 4 8 3 4\n",
            "Ground truth: 1 4 8 3 4\n",
            "Source: eight million four hundred and eighty two thousand two hundred and forty\n",
            "Prediction:   8 4 8 2 2 4 0\n",
            "Ground truth: 8 4 8 2 2 4 0\n",
            "Source: nine million nine hundred and twenty thousand one hundred and seventy five\n",
            "Prediction:   9 9 2 0 1 7 5\n",
            "Ground truth: 9 9 2 0 1 7 5\n",
            "Source: nine hundred and forty four thousand two hundred and ninety six\n",
            "Prediction:   9 4 4 2 9 6\n",
            "Ground truth: 9 4 4 2 9 6\n",
            "Source: seventy thousand eight hundred and ninety six\n",
            "Prediction:   7 0 8 9 6\n",
            "Ground truth: 7 0 8 9 6\n",
            "Source: two billion five hundred and forty nine million eight hundred and twenty four thousand one hundred and seventy eight\n",
            "Prediction:   2 5 4 9 8 2 4 1 7 8\n",
            "Ground truth: 2 5 4 9 8 2 4 1 7 8\n",
            "Source: eight million nine hundred and ninety two thousand four hundred and seventy six\n",
            "Prediction:   8 9 9 2 4 7 6\n",
            "Ground truth: 8 9 9 2 4 7 6\n",
            "Source: forty six million five hundred and ninety six thousand one hundred and thirty five\n",
            "Prediction:   4 6 5 9 6 1 3 5\n",
            "Ground truth: 4 6 5 9 6 1 3 5\n",
            "Source: eight hundred and fifty eight thousand four hundred and twenty\n",
            "Prediction:   8 5 8 4 2 0\n",
            "Ground truth: 8 5 8 4 2 0\n",
            "Source: fifty six thousand six hundred and eighty seven\n",
            "Prediction:   5 6 6 8 7\n",
            "Ground truth: 5 6 6 8 7\n",
            "Source: five thousand eight hundred and thirty\n",
            "Prediction:   5 8 3 0\n",
            "Ground truth: 5 8 3 0\n",
            "Source: four million four hundred and thirty one thousand six hundred and sixty\n",
            "Prediction:   4 4 3 1 6 6 0\n",
            "Ground truth: 4 4 3 1 6 6 0\n",
            "Source: seven million four hundred and thirty seven thousand eight hundred and thirty nine\n",
            "Prediction:   7 4 3 7 8 3 9\n",
            "Ground truth: 7 4 3 7 8 3 9\n",
            "Source: twelve thousand six hundred and sixty eight\n",
            "Prediction:   1 2 6 6 8\n",
            "Ground truth: 1 2 6 6 8\n",
            "Source: seventy two thousand four hundred and sixty nine\n",
            "Prediction:   7 2 4 6 9\n",
            "Ground truth: 7 2 4 6 9\n",
            "Source: one hundred and ninety nine million five hundred and sixty four thousand six hundred and eleven\n",
            "Prediction:   1 9 9 5 6 4 6 1 1\n",
            "Ground truth: 1 9 9 5 6 4 6 1 1\n",
            "Source: two million three hundred and fifty three thousand six hundred and twenty three\n",
            "Prediction:   2 3 5 3 6 2 3\n",
            "Ground truth: 2 3 5 3 6 2 3\n",
            "Source: four thousand six hundred and five\n",
            "Prediction:   4 6 0 5\n",
            "Ground truth: 4 6 0 5\n",
            "Source: five billion five hundred and twenty eight million one hundred and seventy four thousand one hundred and sixty two\n",
            "Prediction:   5 5 2 8 1 7 4 1 6 2\n",
            "Ground truth: 5 5 2 8 1 7 4 1 6 2\n",
            "Source: one billion sixteen million one hundred and nine thousand one hundred and seventy five\n",
            "Prediction:   1 1 6 1 0 9 1 7 5\n",
            "Ground truth: 1 0 1 6 1 0 9 1 7 5\n",
            "Source: one hundred and ninety nine thousand eight hundred and nineteen\n",
            "Prediction:   1 9 9 8 1 9\n",
            "Ground truth: 1 9 9 8 1 9\n",
            "Source: eight hundred and forty seven thousand six hundred and fifty two\n",
            "Prediction:   8 4 7 6 5 2\n",
            "Ground truth: 8 4 7 6 5 2\n",
            "Source: sixty two thousand two hundred and ninety five\n",
            "Prediction:   6 2 2 9 5\n",
            "Ground truth: 6 2 2 9 5\n",
            "Source: six hundred and sixty four thousand four hundred and eighty four\n",
            "Prediction:   6 6 4 4 8 4\n",
            "Ground truth: 6 6 4 4 8 4\n",
            "Source: one hundred and twenty one million four hundred and forty four thousand five hundred and twenty three\n",
            "Prediction:   1 2 1 4 4 4 5 2 3\n",
            "Ground truth: 1 2 1 4 4 4 5 2 3\n",
            "Source: five hundred and forty one million nine hundred and one thousand six hundred and forty five\n",
            "Prediction:   5 4 1 9 0 1 6 4 5\n",
            "Ground truth: 5 4 1 9 0 1 6 4 5\n",
            "Source: nine million three hundred and eighty seven thousand one hundred and sixty four\n",
            "Prediction:   9 3 8 7 1 6 4\n",
            "Ground truth: 9 3 8 7 1 6 4\n",
            "Source: fifty seven thousand eight hundred and ninety one\n",
            "Prediction:   5 7 8 9 1\n",
            "Ground truth: 5 7 8 9 1\n",
            "Source: seven hundred and ninety six million seven hundred and sixty six thousand and forty eight\n",
            "Prediction:   7 9 6 7 6 6 0 4 8\n",
            "Ground truth: 7 9 6 7 6 6 0 4 8\n",
            "Source: five million five hundred and forty two thousand two hundred and fifty five\n",
            "Prediction:   5 5 4 2 2 5 5\n",
            "Ground truth: 5 5 4 2 2 5 5\n",
            "Source: eight thousand three hundred and eighty four\n",
            "Prediction:   8 3 8 4\n",
            "Ground truth: 8 3 8 4\n",
            "Source: nine thousand five hundred and eighty three\n",
            "Prediction:   9 5 8 3\n",
            "Ground truth: 9 5 8 3\n",
            "Source: ninety nine thousand four hundred and eighty eight\n",
            "Prediction:   9 9 4 8 8\n",
            "Ground truth: 9 9 4 8 8\n",
            "Source: five hundred and fifty three million six hundred and thirty four thousand eight hundred and thirty one\n",
            "Prediction:   5 5 3 6 3 4 8 3 1\n",
            "Ground truth: 5 5 3 6 3 4 8 3 1\n",
            "Source: five hundred and fifty four million one hundred and thirty thousand seven hundred and sixty four\n",
            "Prediction:   5 5 4 1 3 0 7 6 4\n",
            "Ground truth: 5 5 4 1 3 0 7 6 4\n",
            "Source: nine million sixty six thousand five hundred and seventeen\n",
            "Prediction:   9 0 6 6 5 1 7\n",
            "Ground truth: 9 0 6 6 5 1 7\n",
            "Source: three thousand and forty seven\n",
            "Prediction:   3 0 4 7\n",
            "Ground truth: 3 0 4 7\n",
            "Source: fifty five thousand four hundred and seven\n",
            "Prediction:   5 5 4 0 7\n",
            "Ground truth: 5 5 4 0 7\n",
            "Source: seven billion one hundred and fifty million seven hundred and fifty thousand nine hundred and one\n",
            "Prediction:   7 1 5 0 7 5 0 9 0 1\n",
            "Ground truth: 7 1 5 0 7 5 0 9 0 1\n",
            "Source: one thousand and eighty four\n",
            "Prediction:   1 0 8 4\n",
            "Ground truth: 1 0 8 4\n",
            "Source: eight hundred and eighteen thousand three hundred and thirty five\n",
            "Prediction:   8 1 8 3 3 5\n",
            "Ground truth: 8 1 8 3 3 5\n",
            "Source: two thousand nine hundred and three\n",
            "Prediction:   2 9 0 3\n",
            "Ground truth: 2 9 0 3\n",
            "Source: two thousand seven hundred and seventy three\n",
            "Prediction:   2 7 7 3\n",
            "Ground truth: 2 7 7 3\n",
            "Source: seven thousand seven hundred and sixty five\n",
            "Prediction:   7 7 6 5\n",
            "Ground truth: 7 7 6 5\n",
            "Source: one billion ninety one million five hundred and forty five thousand nine hundred and twenty four\n",
            "Prediction:   1 0 9 1 5 4 5 9 2 4\n",
            "Ground truth: 1 0 9 1 5 4 5 9 2 4\n",
            "Source: seven billion seven hundred and twenty eight million one hundred and fifty nine thousand two hundred and seven\n",
            "Prediction:   7 7 2 8 1 5 9 2 0 7\n",
            "Ground truth: 7 7 2 8 1 5 9 2 0 7\n",
            "Source: nine hundred and sixty thousand three hundred and two\n",
            "Prediction:   9 6 0 3 0 2\n",
            "Ground truth: 9 6 0 3 0 2\n",
            "Source: six hundred and forty two thousand five hundred and eleven\n",
            "Prediction:   6 4 2 5 1 1\n",
            "Ground truth: 6 4 2 5 1 1\n",
            "Source: five hundred and ninety two thousand seven hundred and seventy four\n",
            "Prediction:   5 9 2 7 7 4\n",
            "Ground truth: 5 9 2 7 7 4\n",
            "Source: two hundred and forty million seven hundred and fifty two thousand nine hundred and eighty nine\n",
            "Prediction:   2 4 0 7 5 2 9 8 9\n",
            "Ground truth: 2 4 0 7 5 2 9 8 9\n",
            "Source: eight hundred and ninety two thousand eight hundred and fifty five\n",
            "Prediction:   8 9 2 8 5 5\n",
            "Ground truth: 8 9 2 8 5 5\n",
            "Source: eighty six million six hundred and forty seven thousand seven hundred and sixty seven\n",
            "Prediction:   8 6 6 4 7 7 6 7\n",
            "Ground truth: 8 6 6 4 7 7 6 7\n",
            "Source: ninety two thousand one hundred and ninety\n",
            "Prediction:   9 2 1 9 0\n",
            "Ground truth: 9 2 1 9 0\n",
            "Source: two million two hundred and fifty six\n",
            "Prediction:   2 2 5 6 6\n",
            "Ground truth: 2 0 0 0 2 5 6\n",
            "Source: nine hundred and one thousand two hundred and four\n",
            "Prediction:   9 0 1 2 0 4\n",
            "Ground truth: 9 0 1 2 0 4\n",
            "Source: eight hundred and fourteen million seven hundred and ninety one thousand and eighty two\n",
            "Prediction:   8 1 4 7 9 1 0 8 2\n",
            "Ground truth: 8 1 4 7 9 1 0 8 2\n",
            "Source: seven million seven hundred and fifty eight thousand six hundred and forty\n",
            "Prediction:   7 7 5 8 6 4 0\n",
            "Ground truth: 7 7 5 8 6 4 0\n",
            "Source: four thousand five hundred and ninety one\n",
            "Prediction:   4 5 9 1\n",
            "Ground truth: 4 5 9 1\n",
            "Source: forty thousand four hundred and one\n",
            "Prediction:   4 0 4 0 1\n",
            "Ground truth: 4 0 4 0 1\n",
            "Source: forty six million two hundred and sixty five thousand two hundred and thirty three\n",
            "Prediction:   4 6 2 6 5 2 3 3\n",
            "Ground truth: 4 6 2 6 5 2 3 3\n",
            "Source: five million nine hundred and seventy three thousand eight hundred and ten\n",
            "Prediction:   5 9 7 3 8 1 0\n",
            "Ground truth: 5 9 7 3 8 1 0\n",
            "Source: six million one hundred and sixty thousand seven hundred and eighty three\n",
            "Prediction:   6 1 6 0 7 8 3\n",
            "Ground truth: 6 1 6 0 7 8 3\n",
            "Source: forty eight million four hundred and eighty four thousand two hundred and thirty\n",
            "Prediction:   4 8 4 8 4 2 3 0\n",
            "Ground truth: 4 8 4 8 4 2 3 0\n",
            "Source: seven hundred and fifty seven million five hundred and eighty five thousand six hundred and fifty six\n",
            "Prediction:   7 5 7 5 8 5 6 5 6\n",
            "Ground truth: 7 5 7 5 8 5 6 5 6\n",
            "Source: fifty nine thousand and seventy three\n",
            "Prediction:   5 9 0 7 3\n",
            "Ground truth: 5 9 0 7 3\n",
            "Source: six thousand three hundred and nineteen\n",
            "Prediction:   6 3 1 9\n",
            "Ground truth: 6 3 1 9\n",
            "Source: sixty six thousand five hundred and eighty\n",
            "Prediction:   6 6 5 8 0\n",
            "Ground truth: 6 6 5 8 0\n",
            "Source: nine hundred and fifty million eighty five thousand one hundred and thirty two\n",
            "Prediction:   9 5 0 0 8 5 1 3 2\n",
            "Ground truth: 9 5 0 0 8 5 1 3 2\n",
            "Source: six thousand eight hundred and twenty\n",
            "Prediction:   6 8 2 0\n",
            "Ground truth: 6 8 2 0\n",
            "Source: four hundred and seventy five million five hundred and seventy six thousand and seventy nine\n",
            "Prediction:   4 7 5 5 7 6 0 7 9\n",
            "Ground truth: 4 7 5 5 7 6 0 7 9\n",
            "Source: forty seven thousand two hundred and seventy nine\n",
            "Prediction:   4 7 2 7 9\n",
            "Ground truth: 4 7 2 7 9\n",
            "Source: five thousand nine hundred and thirty one\n",
            "Prediction:   5 9 3 1\n",
            "Ground truth: 5 9 3 1\n",
            "Source: three hundred and six million six hundred and fifty one thousand eight hundred and eight\n",
            "Prediction:   3 0 6 6 5 1 8 0 8\n",
            "Ground truth: 3 0 6 6 5 1 8 0 8\n",
            "Source: three hundred and eighty one thousand four hundred and ninety two\n",
            "Prediction:   3 8 1 4 9 2\n",
            "Ground truth: 3 8 1 4 9 2\n",
            "Source: seven billion six hundred and sixty five million three hundred and ninety eight thousand five hundred and seven\n",
            "Prediction:   7 6 6 5 3 9 8 5 0 7\n",
            "Ground truth: 7 6 6 5 3 9 8 5 0 7\n",
            "Source: two hundred and seventy two thousand one hundred and ninety five\n",
            "Prediction:   2 7 2 1 9 5\n",
            "Ground truth: 2 7 2 1 9 5\n",
            "Source: twenty seven million three hundred and forty five thousand seven hundred and seventy five\n",
            "Prediction:   2 7 3 4 5 7 7 5\n",
            "Ground truth: 2 7 3 4 5 7 7 5\n",
            "Source: six thousand four hundred and thirty seven\n",
            "Prediction:   6 4 3 7\n",
            "Ground truth: 6 4 3 7\n",
            "Source: eighty three thousand eight hundred and seventy six\n",
            "Prediction:   8 3 8 7 6\n",
            "Ground truth: 8 3 8 7 6\n",
            "Source: eighteen thousand and sixty five\n",
            "Prediction:   1 8 0 6 5\n",
            "Ground truth: 1 8 0 6 5\n",
            "Source: four thousand six hundred and twenty nine\n",
            "Prediction:   4 6 2 9\n",
            "Ground truth: 4 6 2 9\n",
            "Source: one hundred and twenty six thousand four hundred and seventy two\n",
            "Prediction:   1 2 6 4 7 2\n",
            "Ground truth: 1 2 6 4 7 2\n",
            "Source: one hundred and eighty one thousand two hundred and fifty eight\n",
            "Prediction:   1 8 1 2 5 8\n",
            "Ground truth: 1 8 1 2 5 8\n",
            "Source: eighty five thousand eight hundred and sixty three\n",
            "Prediction:   8 5 8 6 3\n",
            "Ground truth: 8 5 8 6 3\n",
            "Source: four hundred and forty seven million four hundred and fifty eight thousand seven hundred and fifty eight\n",
            "Prediction:   4 4 7 4 5 8 7 5 8\n",
            "Ground truth: 4 4 7 4 5 8 7 5 8\n",
            "Source: six million nine hundred and ninety four thousand five hundred and eighty eight\n",
            "Prediction:   6 9 9 4 5 8 8\n",
            "Ground truth: 6 9 9 4 5 8 8\n",
            "Source: three thousand and sixty eight\n",
            "Prediction:   3 0 6 8\n",
            "Ground truth: 3 0 6 8\n",
            "Source: three million three hundred and fifty thousand six hundred and twenty five\n",
            "Prediction:   3 3 5 0 6 2 5\n",
            "Ground truth: 3 3 5 0 6 2 5\n",
            "Source: nine thousand two hundred and ten\n",
            "Prediction:   9 2 1 0\n",
            "Ground truth: 9 2 1 0\n",
            "Source: eight million nine hundred and three thousand five hundred and forty seven\n",
            "Prediction:   8 9 0 3 5 4 7\n",
            "Ground truth: 8 9 0 3 5 4 7\n",
            "Source: one million eight hundred and seven thousand six hundred and eleven\n",
            "Prediction:   1 8 0 7 6 1 1\n",
            "Ground truth: 1 8 0 7 6 1 1\n",
            "Source: eight million nine hundred and twenty five thousand five hundred and seventy one\n",
            "Prediction:   8 9 2 5 5 7 1\n",
            "Ground truth: 8 9 2 5 5 7 1\n",
            "Source: nine hundred and seventy million eight hundred and sixty nine thousand two hundred and thirty four\n",
            "Prediction:   9 7 0 8 6 9 2 3 4\n",
            "Ground truth: 9 7 0 8 6 9 2 3 4\n",
            "Source: four million nine hundred and seven thousand eight hundred and sixty two\n",
            "Prediction:   4 9 0 7 8 6 2\n",
            "Ground truth: 4 9 0 7 8 6 2\n",
            "Source: ninety million nine hundred and ninety nine thousand eight hundred and seventy\n",
            "Prediction:   9 0 9 9 9 8 7 0\n",
            "Ground truth: 9 0 9 9 9 8 7 0\n",
            "Source: two million eight hundred and seventy eight thousand nine hundred and fifteen\n",
            "Prediction:   2 8 7 8 9 1 5\n",
            "Ground truth: 2 8 7 8 9 1 5\n",
            "Source: eighty nine million eighty seven thousand six hundred and ninety four\n",
            "Prediction:   8 9 0 8 7 6 9 4\n",
            "Ground truth: 8 9 0 8 7 6 9 4\n",
            "Source: six billion five hundred and eighteen million seven hundred and forty thousand six hundred and twenty seven\n",
            "Prediction:   6 5 1 8 7 4 0 6 2 7\n",
            "Ground truth: 6 5 1 8 7 4 0 6 2 7\n",
            "Source: sixty two thousand two hundred and sixty six\n",
            "Prediction:   6 2 2 6 6\n",
            "Ground truth: 6 2 2 6 6\n",
            "Source: one thousand two hundred and ninety six\n",
            "Prediction:   1 2 9 6\n",
            "Ground truth: 1 2 9 6\n",
            "Source: four thousand seven hundred and thirty four\n",
            "Prediction:   4 7 3 4\n",
            "Ground truth: 4 7 3 4\n",
            "Source: three million nine hundred and fifty two thousand nine hundred and eighty seven\n",
            "Prediction:   3 9 5 2 9 8 7\n",
            "Ground truth: 3 9 5 2 9 8 7\n",
            "Source: one hundred and seventy four thousand and thirty seven\n",
            "Prediction:   1 7 4 0 3 7\n",
            "Ground truth: 1 7 4 0 3 7\n",
            "Source: two hundred and eight million eight hundred and ninety one thousand one hundred and seven\n",
            "Prediction:   2 0 8 8 9 1 1 0 7\n",
            "Ground truth: 2 0 8 8 9 1 1 0 7\n",
            "Source: three thousand nine hundred and twelve\n",
            "Prediction:   3 9 1 2\n",
            "Ground truth: 3 9 1 2\n",
            "Accuracy: 0.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg3mVb59MDa0"
      },
      "source": [
        "## Visualize Attentions\n",
        "\n",
        "We can visualize how each query distributes its attention scores over each source word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMs3yAH5MDa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "50a4f3cb-1ea1-4195-a05b-a036a3b50d49"
      },
      "source": [
        "K = 1 # this code only works for beam size 1\n",
        "\n",
        "# Create beam searcher\n",
        "beam_searcher = BeamSearcher(model)\n",
        "batch = next(iter(test_iter))\n",
        "# Input and output\n",
        "src, src_lengths = batch.src\n",
        "# Predict and get attentions\n",
        "prediction, all_attns = beam_searcher.beam_search(src, src_lengths, K)\n",
        "all_attns = torch.stack(all_attns, 0)\n",
        "# Convert to string\n",
        "prediction = ' '.join([TGT.vocab.itos[token] for token in prediction])\n",
        "prediction = prediction.lstrip('<bos>').rstrip('<eos>').strip()\n",
        "ground_truth = ' '.join([TGT.vocab.itos[token] for token in batch.tgt.view(-1)])\n",
        "ground_truth = ground_truth.lstrip('<bos>').rstrip('<eos>').strip()\n",
        "src = ' '.join([SRC.vocab.itos[item] for item in src.view(-1)])\n",
        "print (f'Source: {src}')\n",
        "print (f'Prediction:   {prediction}')\n",
        "print (f'Ground truth: {ground_truth}')\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "ax.imshow(all_attns[:,0,:].detach().cpu())\n",
        "ax.set_yticks(list(range(1+len(prediction.split()))));\n",
        "ax.set_yticklabels(prediction.split() + ['eos']);\n",
        "ax.set_xticks(list(range(len(src.split()))));\n",
        "ax.set_xticklabels(src.split());\n",
        "\n",
        "# Uncomment the line below if the plot does not show up\n",
        "# Make sure to comment that before submitting to gradescope\n",
        "# since there would be some autograder issues with plt.show()\n",
        "#plt.show()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source: sixteen thousand eight hundred and thirty two\n",
            "Prediction:   1 6 8 3 2\n",
            "Ground truth: 1 6 8 3 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wUzqsCuMDa0"
      },
      "source": [
        "Do these attentions make sense? Do you see how the attention mechanism solves the bottleneck problem in vanilla seq2seq?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhPxpk7_MDa0"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iRVvkBaMDa1"
      },
      "source": [
        "In RNN-based neural encoder-decoder models, we used recurrence to model the dependencies among words. For example, by running a unidirectional RNN from $y_{1}$ to $y_{t}$, we can consider the past history when predicting $y_{t+1}$. However, running an RNN over a sequence is a serial process: we need to wait for it to finish running from $y_1$ to $y_t$ before being able to compute the outputs at $y_{t+1}$. This serial process cannot be parallelized on GPUs along the sequence length dimension.\n",
        "\n",
        "The attention mechanism provides an alternative, and most importantly, parallelizable solution. [The transformer model](https://arxiv.org/abs/1706.03762) completely gets rid of recurrence and only uses attention to model the dependencies among words. For example, we can use attention to incorporate the representations from $y_1$ to $y_t$ when predicting $y_{t+1}$, simply by attending to their word embeddings. This is called _decoder self-attention_.\n",
        "\n",
        "Similarly, at the encoder side, for each word $x_i$, we let it attend to the embeddings of $x_1, \\ldots, x_S$, to model the context in which $x_i$ appears. This is called _encoder self-attention_. It is different from decoder self-attention in that here every word attends to all words, but at the decoder side, every word can only attend to the previous words.\n",
        "\n",
        "To incorporate source-side information at the decoder side, at each time step, we let the decoder attend to the top-layer encoder outputs, as we did in the RNN-based encoder-decoder model above. This is called _cross-attention_. Note that there's no initialization of decoder hidden state here, since we no longer use an RNN.\n",
        "\n",
        "The process we describe above is only a single layer of attention. In practice, transformers stack multiple layers of attention and feedforward layers, using the outputs from the layer below as the inputs to the layer above, as shown in the illustration below.\n",
        "\n",
        "<img src=\"https://github.com/nlp-course/data/raw/master/img/transformer.png\" alt=\"transformer illustration\" />\n",
        "\n",
        "In the above illustration, due to space limits, we ommited the details of encoder self-attention and decoder self-attention, and we describe it here, using encoder-self-attention at layer 0 as an example. First, we use three linear projections to project each hidden state $h_{0,i}$ to a query vector $q_{0,i}$, a key vector $k_{0,i}$, and a value vector $v_{0,i}$. Then at each position $i$, we use $q_i$ as the query, and $\\{(k_{0,j}, v_{0,j}): j \\in \\{1, \\ldots, S\\}\\}$ as keys/values to produce a context vector $c_{0,i}$. Note that the keys/values are the same for different positions, and the only difference is that a different query vector is used for each position.\n",
        "\n",
        "A clear difference between the transformer architecture and the RNN-based encoder decoder archiecture is that there are no horizontal arrows in the transformer model: transformers only use position-wise operations and attention operations. The dependencies among words are **only introduced by the attention operations**, while the other operations such as feedforwad, nonlinearity, and normalization are position-wise, that is, they do not depend on other positions, and can thus be performed in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "OwMrt6QRMDa1"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** In the above transformer model, if we shuffle the input words $x_1, \\ldots, x_4$, would we get a different distribution over $y$? Why or why not?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_transformer_shuffle\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGPe-GZ7MDa2"
      },
      "source": [
        "There will be a different distribution because different words will appear in different places, therefore the probabilties will change of a word being seen after another word. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnJFfurrMDa2"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "Since the transformer model itself doesn't have any sense of position/order, we encode the position of the word in the sentence, and add it to the word embedding as the input representation, as illustrated below.\n",
        "\n",
        "<img src=\"https://github.com/nlp-course/data/raw/master/img/transformer_pos.png\" alt=\"transformer w/ positional encoding illustration\" />\n",
        "\n",
        "> The illustrations above also omitted residual connections, which add the inputs to certain operations (such as attention and feedforward) to the outputs. More details can be found in the code below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "RdqVfRvmMDa3"
      },
      "source": [
        "### Causal Attention mask\n",
        "\n",
        "To efficiently train the transformer model, we want to batch the attention operations together such that they can be fully parallelized along the sequence length dimension. (The non-attention operations are position-wise so they are trivally parallelizable.) This is quite straightforward for encoder self-attention and decoder-encoder cross-attention given our batched implementation of the `attention` function. However, things are a bit trickier for the decoder: each word $y_t$ attends to $t-1$ previous words $y_1, \\ldots, y_{t-1}$, which means each word $y_t$ has a different set of key-value pairs. Is it possible to batch them together?\n",
        "\n",
        "The solution is to use attention masks. For every word $y_t$, we give it all key-value pairs at $y_1, \\ldots, y_T$, and we disallow attending to future words $y_{t}, y_{t+1},\\ldots, y_T$ through an attention mask. (Recall that the `attention` function takes a `mask` argument.) We usually call this attention mask a _causal attention mask_, as it prevents the leakage of future information. Since every $y_t$ has the same set of (key, value) pairs, we can batch them and compute the context vectors using a single call to the function `attention`.\n",
        "\n",
        "What should such a mask be? Implement the `causal_mask` function below to generate this mask.\n",
        "\n",
        "> Hint: you might find [`torch.triu`](https://pytorch.org/docs/stable/generated/torch.triu.html) useful.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: causal_attention_mask\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIgt9GISMDa4"
      },
      "source": [
        "#TODO - implement this function, which returns a causal attention mask\n",
        "def causal_mask(T):\n",
        "  \"\"\"\n",
        "  Generate a causal mask.\n",
        "  Arguments:\n",
        "      T: the length of target sequence\n",
        "  Returns:\n",
        "      mask: a T x T tensor, where `mask[i, j]` should be `True` \n",
        "      if y_i can attend to y_{j-1} (there's a \"-1\" since the first \n",
        "      token in decoder input is <bos>) and `False` if y_i cannot \n",
        "      attend to y_{j-1}\n",
        "  \"\"\"\n",
        "  full = torch.empty(T, T, dtype=torch.bool)\n",
        "  full.fill_(True)\n",
        "  mask = torch.triu(full).T\n",
        "\n",
        "  return mask.to(device)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "OB7UHFSkMDa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "outputId": "e86cfd7a-48ae-4a4f-96c2-34006a3d7f28"
      },
      "source": [
        "grader.check(\"causal_attention_mask\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ],
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8gqJxIYMDa5"
      },
      "source": [
        "We can visualize the attention mask and manually check if it's what we expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kpo_WpFMDa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "c9dad4a3-5cbd-4c98-c068-8b69071df5ed"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "T = 7\n",
        "mask = causal_mask(T)\n",
        "ax.imshow(mask.cpu())\n",
        "\n",
        "# Uncomment the line below if the plot does not show up\n",
        "# Make sure to comment that before submitting to gradescope\n",
        "# since there would be some autograder issues with `plt.show()`\n",
        "#plt.show()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f486d598860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fkEUux2dMDa6"
      },
      "source": [
        "Now we are ready to complete the implementation of the transformer model. The code is structured as a set of classes: `TransformerEncoderLayer`\\*, `TransformerEncoder`, `TransformDecoderLayer`\\*, `TransformDecoder`, `PositionalEmbedding`, and `TransformerEncoderDecoder`\\*. We've provided almost all the necessary code. In particular, we provide code for all position-wise operations. Your job is only to implement the parts involving attention and to figure out the correct attention masks, which involves only the three classes marked above with a star.\n",
        "\n",
        "> Hint: Completing this transformer implementation should require very little code, just a few lines.\n",
        "\n",
        "> Hint: The causal mask is a 2-D matrix, but we want to add a batch dimension, and expand it to be of the desired size. For this purpose, you can use [`torch.repeat`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.repeat).\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: transformer\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBk2uMlHMDa7"
      },
      "source": [
        "\n",
        "#TODO - implement `forward_encoder`, `forward_decoder`, and `forward`.\n",
        "# `TransformerEncoderDecoder` inherits most functions from `AttnEncoderDecoder`\n",
        "class TransformerEncoderDecoder(AttnEncoderDecoder):\n",
        "  def __init__(self, src_field, tgt_field, hidden_size=64, layers=3):\n",
        "    \"\"\"\n",
        "    Initializer. Creates network modules and loss function.\n",
        "    Arguments:\n",
        "        src_field: src field\n",
        "        tgt_field: tgt field\n",
        "        hidden_size: hidden layer size of both encoder and decoder\n",
        "        layers: number of layers of both encoder and decoder\n",
        "    \"\"\"\n",
        "    super(AttnEncoderDecoder, self).__init__()\n",
        "    self.src_field = src_field\n",
        "    self.tgt_field = tgt_field\n",
        "    \n",
        "    # Keep the vocabulary sizes available\n",
        "    self.V_src = len(src_field.vocab.itos)\n",
        "    self.V_tgt = len(tgt_field.vocab.itos)\n",
        "    \n",
        "    # Get special word ids\n",
        "    self.padding_id_src = src_field.vocab.stoi[src_field.pad_token]\n",
        "    self.padding_id_tgt = tgt_field.vocab.stoi[tgt_field.pad_token]\n",
        "    self.bos_id = tgt_field.vocab.stoi[tgt_field.init_token]\n",
        "    self.eos_id = tgt_field.vocab.stoi[tgt_field.eos_token]\n",
        "\n",
        "    # Keep hyper-parameters available\n",
        "    self.embedding_size = hidden_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.layers = layers\n",
        "\n",
        "    # Create essential modules\n",
        "    self.encoder = TransformerEncoder(self.V_src, hidden_size, layers)\n",
        "    self.decoder = TransformerDecoder(self.V_tgt, hidden_size, layers)\n",
        "\n",
        "    # Final projection layer\n",
        "    self.hidden2output = nn.Linear(hidden_size, self.V_tgt)\n",
        "   \n",
        "    # Create loss function\n",
        "    self.loss_function = nn.CrossEntropyLoss(reduction='sum', \n",
        "                                             ignore_index=self.padding_id_tgt)\n",
        "\n",
        "  def forward_encoder(self, src, src_lengths):\n",
        "    \"\"\"\n",
        "    Encodes source words `src`.\n",
        "    Arguments:\n",
        "        src: src batch of size (max_src_len, bsz)\n",
        "        src_lengths: src lengths (bsz)\n",
        "    Returns:\n",
        "        memory_bank: a tensor of size (src_len, bsz, hidden_size) \n",
        "    \"\"\"\n",
        "    # The reason we don't directly pass in src_mask as in `forward_decoder` is to\n",
        "    # enable us to reuse beam search implemented for RNN-based encoder-decoder\n",
        "    src_len = src.size(0)\n",
        "    #TODO - compute `src_mask`\n",
        "    src_mask = causal_mask(src_len)\n",
        "    memory_bank = self.encoder(src, src_mask)\n",
        "    return memory_bank, None\n",
        "\n",
        "  def forward_decoder(self, tgt_in, memory_bank, src_mask):\n",
        "    \"\"\"\n",
        "    Decodes based on memory bank, and ground truth target words.\n",
        "    Arguments:\n",
        "        tgt_in: a tensor of size (tgt_len, bsz)\n",
        "        memory_bank: a tensor of size (src_len, bsz, hidden_size), encoder outputs \n",
        "                     at every position\n",
        "    Returns:\n",
        "        Logits of size (tgt_len, bsz, V_tgt) (before the softmax operation)\n",
        "    \"\"\"\n",
        "    tgt_len = tgt_in.size(0)\n",
        "    bsz = tgt_in.size(1)\n",
        "    #TODO - compute `src_mask` and `tgt_mask`, note that the src_mask here has a different\n",
        "    #       shape as the `src_mask` passed in, as the attention function needs\n",
        "    #       a mask of the same size as the attention matrix\n",
        "    src_mask = torch.ones(tgt_len, tgt_len, dtype=torch.bool, device=device).unsqueeze(0).repeat(bsz,1,1)\n",
        "    tgt_mask = torch.ones(tgt_len, memory_bank.size()[0], dtype=torch.bool, device=device).unsqueeze(0).repeat(bsz,1,1)\n",
        "    \n",
        "    outputs = self.decoder(tgt_in, memory_bank, src_mask, tgt_mask)\n",
        "    logits = self.hidden2output(outputs)\n",
        "    return logits\n",
        "\n",
        "  def forward(self, src, src_lengths, tgt_in):\n",
        "    \"\"\"\n",
        "    Performs forward computation, returns logits.\n",
        "    Arguments:\n",
        "        src: src batch of size (max_src_len, bsz)\n",
        "        src_lengths: src lengths of size (bsz)\n",
        "        tgt_in:  a tensor of size (tgt_len, bsz)\n",
        "    \"\"\"\n",
        "    src_mask = src.ne(self.padding_id_src) # max_src_len, bsz\n",
        "    # TODO\n",
        "\n",
        "    memory_bank, _ = self.forward_encoder(src, src_lengths)\n",
        "    logits = self.forward_decoder(tgt_in, memory_bank, src_mask)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def forward_decoder_incrementally(self, prev_decoder_states, tgt_in_onestep, \n",
        "                                    memory_bank, src_mask, normalize=True):\n",
        "    \"\"\"\n",
        "    Forward the decoder at `decoder_state` for a single step with token `tgt_in_onestep`.\n",
        "    This function will be used in beam search. Note that the implementation here is\n",
        "    very inefficient, since we do not cache any decoder state, but instead we only\n",
        "    cache previously generated tokens in `prev_decoder_states`, and do a fresh\n",
        "    `forward_decoder`.\n",
        "    Arguments:\n",
        "        prev_decoder_states: previous tgt words. None for the first step.\n",
        "        tgt_in_onestep: a tensor of size (bsz), tokens at one step\n",
        "        memory_bank: a tensor of size (src_len, bsz, hidden_size), src hidden states \n",
        "                     at every position\n",
        "        src_mask: a tensor of size (src_len, bsz): a boolean tensor, `False` where\n",
        "                  src is padding.\n",
        "        normalize: use log_softmax to normalize or not. Beam search needs to normalize,\n",
        "                   while `forward_decoder` does not\n",
        "    Returns:\n",
        "        logits: Log probabilities for `tgt_in_token` of size (bsz, V_tgt)\n",
        "        decoder_states: we use tgt words up to now as states, a tensor of size (len, bsz)\n",
        "        None: to keep output format the same as AttnEncoderDecoder, such that we can\n",
        "              reuse beam search code\n",
        "        \n",
        "    \"\"\"\n",
        "    prev_tgt_in = prev_decoder_states # tgt_len, bsz\n",
        "    src_len = memory_bank.size(0)\n",
        "    bsz = memory_bank.size(1)\n",
        "    tgt_in_onestep = tgt_in_onestep.view(1, -1) # 1, bsz\n",
        "    if prev_tgt_in is not None:\n",
        "      tgt_in = torch.cat((prev_tgt_in, tgt_in_onestep), 0) # tgt_len+1, bsz\n",
        "    else:\n",
        "      tgt_in = tgt_in_onestep\n",
        "    tgt_len = tgt_in.size(1)\n",
        "    \n",
        "    logits = self.forward_decoder(tgt_in, memory_bank, src_mask)\n",
        "    logits = logits[-1]\n",
        "    if normalize:\n",
        "      logits = torch.log_softmax(logits, dim=-1)\n",
        "    decoder_states = tgt_in\n",
        "    return logits, decoder_states, None"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CGa6F1UMDa7"
      },
      "source": [
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "  r\"\"\"TransformerEncoder is an embedding layer and a stack of N encoder layers.\n",
        "  Arguments:\n",
        "      hidden_size: hidden size.\n",
        "      layers: the number of encoder layers.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, hidden_size, layers):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.embed = PositionalEmbedding(vocab_size, hidden_size)\n",
        "    encoder_layer = TransformerEncoderLayer(hidden_size)\n",
        "    self.layers = _get_clones(encoder_layer, layers)\n",
        "    self.norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "  def forward(self, src, src_mask):\n",
        "    r\"\"\"Pass the input through the word embedding layer, followed by\n",
        "    the encoder layers in turn.\n",
        "    Arguments:\n",
        "        src: src batch of size (max_src_len, bsz)\n",
        "        src_mask: the mask for the src sequence, (max_src_len, bsz)\n",
        "    Returns:\n",
        "        a tensor of size (max_src_len, bsz, hidden_size)\n",
        "    \"\"\"\n",
        "    output = self.embed(src)\n",
        "    for mod in self.layers:\n",
        "      output = mod(output, src_mask=src_mask)\n",
        "    output = self.norm(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "  r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
        "  Arguments:\n",
        "      hidden_size: hidden size.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, hidden_size):\n",
        "    super(TransformerEncoderLayer, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    fwd_hidden_size = hidden_size * 4\n",
        "    \n",
        "    # Create modules\n",
        "    self.linear1 = nn.Linear(hidden_size, fwd_hidden_size)\n",
        "    self.linear2 = nn.Linear(fwd_hidden_size, hidden_size)\n",
        "    self.norm1 = nn.LayerNorm(hidden_size)\n",
        "    self.norm2 = nn.LayerNorm(hidden_size)\n",
        "    self.activation = nn.ReLU()\n",
        "    # Attention related\n",
        "    self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
        "    self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
        "    self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
        "    self.context_proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "\n",
        "  def forward(self, src, src_mask):\n",
        "    r\"\"\"Pass the input through the encoder layer.\n",
        "    Arguments:\n",
        "        src: an input tensor of size (max_src_len, bsz, hidden_size).\n",
        "        src_mask: attention mask of size (max_src_len, bsz), it's `False`\n",
        "        where the corresponding source word is padding.\n",
        "    Returns:\n",
        "        a tensor of size (max_src_len, bsz, hidden_size).\n",
        "    \"\"\"\n",
        "    # Attend\n",
        "    q = self.q_proj(src) / math.sqrt(self.hidden_size) # a trick needed to make transformer work\n",
        "    k = self.k_proj(src)\n",
        "    v = self.v_proj(src)\n",
        "    #TODO - compute `context`\n",
        "\n",
        "    Q = torch.transpose(q, dim0=0, dim1=1)\n",
        "    K = torch.transpose(k, dim0=0, dim1=1)\n",
        "    K = torch.transpose(K, dim0=1, dim1=2)\n",
        "    QK = Q @ K\n",
        "    mask = src_mask == False\n",
        "    bsz = q.size()[1]\n",
        "    mask = mask.unsqueeze(0).repeat(bsz, 1, 1)\n",
        "    QK = QK.masked_fill_(mask, -float(\"Inf\"))\n",
        "    A = nn.Softmax(dim=-1)(QK)\n",
        "    V = torch.transpose(v, dim0=0, dim1=1)\n",
        "    context = A @ V\n",
        "    context = torch.transpose(context, dim0=0, dim1=1)\n",
        "\n",
        "    src2 = self.context_proj(context)\n",
        "    # Residual connection\n",
        "    src = src + src2\n",
        "    src = self.norm1(src)\n",
        "    # Feedforward for each position\n",
        "    src2 = self.linear2(self.activation(self.linear1(src)))\n",
        "    src = src + src2\n",
        "    src = self.norm2(src)\n",
        "    return src\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "  r\"\"\"TransformerDecoder is an embedding layer and a stack of N decoder layers.\n",
        "  Arguments:\n",
        "      hidden_size: hidden size.\n",
        "      layers: the number of sub-encoder-layers in the encoder.\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_size, hidden_size, layers):\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "    self.embed = PositionalEmbedding(vocab_size, hidden_size)\n",
        "    decoder_layer = TransformerDecoderLayer(hidden_size)\n",
        "    self.layers = _get_clones(decoder_layer, layers)\n",
        "    self.norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "  def forward(self, tgt_in, memory, src_mask, tgt_mask):\n",
        "    r\"\"\"Pass the inputs (and mask) through the word embedding layer, followed by\n",
        "    the decoder layer in turn.\n",
        "    Arguments:\n",
        "        tgt_in: tgt batch of size (max_tgt_len, bsz)\n",
        "        memory: the outputs of the encoder (max_src_len, bsz, hidden_size)\n",
        "        src_mask: attention mask of size (max_src_len, bsz), it's `False`\n",
        "                  where the corresponding source word is padding.\n",
        "    Returns:\n",
        "        a tensor of size (max_tgt_len, bsz, hidden_size)\n",
        "    \"\"\"\n",
        "    output = self.embed(tgt_in)\n",
        "    for mod in self.layers:\n",
        "      output = mod(output, memory, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "    output = self.norm(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "  r\"\"\"TransformerDecoderLayer is made up of self-attn, cross-attn, and \n",
        "  feedforward network.\n",
        "  Arguments:\n",
        "      hidden_size: hidden size.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, hidden_size):\n",
        "    super(TransformerDecoderLayer, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    fwd_hidden_size = hidden_size * 4\n",
        "    \n",
        "    # Create modules\n",
        "    self.linear1 = nn.Linear(hidden_size, fwd_hidden_size)\n",
        "    self.linear2 = nn.Linear(fwd_hidden_size, hidden_size)\n",
        "    \n",
        "    self.activation = nn.ReLU()\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(hidden_size)\n",
        "    self.norm2 = nn.LayerNorm(hidden_size)\n",
        "    self.norm3 = nn.LayerNorm(hidden_size)\n",
        "    \n",
        "    # Attention related\n",
        "    self.q_proj_self = nn.Linear(hidden_size, hidden_size)\n",
        "    self.k_proj_self = nn.Linear(hidden_size, hidden_size)\n",
        "    self.v_proj_self = nn.Linear(hidden_size, hidden_size)\n",
        "    self.context_proj_self = nn.Linear(hidden_size, hidden_size)\n",
        "    \n",
        "    self.q_proj_cross = nn.Linear(hidden_size, hidden_size)\n",
        "    self.k_proj_cross = nn.Linear(hidden_size, hidden_size)\n",
        "    self.v_proj_cross = nn.Linear(hidden_size, hidden_size)\n",
        "    self.context_proj_cross = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "  def forward(self, tgt, memory, src_mask, tgt_mask):\n",
        "    r\"\"\"Pass the inputs (and mask) through the decoder layer.\n",
        "    Arguments:\n",
        "        tgt: an input tensor of size (max_tgt_len, bsz, hidden_size).\n",
        "        memory: encoder outputs of size (max_src_len, bsz, hidden_size).\n",
        "        src_mask: attention mask of size (bsz, max_tgt_len, max_src_len), \n",
        "                  it's `False` where the cross-attention is disallowed.\n",
        "        tgt_mask: attention mask of size (bsz, max_tgt_len, max_tgt_len),\n",
        "                  it's `False` where the self-attention is disallowed.\n",
        "    Returns:\n",
        "        a tensor of size (max_tgt_len, bsz, hidden_size)\n",
        "    \"\"\"\n",
        "    # Self attention (decoder-side)\n",
        "    q = self.q_proj_self(tgt) / math.sqrt(self.hidden_size)\n",
        "    k = self.k_proj_self(tgt)\n",
        "    v = self.v_proj_self(tgt)\n",
        "    #TODO - compute `context`\n",
        "    Q = torch.transpose(q, dim0=0, dim1=1)\n",
        "    K = torch.transpose(k, dim0=0, dim1=1)\n",
        "    K = torch.transpose(K, dim0=1, dim1=2)\n",
        "    QK = Q @ K\n",
        "    mask = src_mask == False\n",
        "    QK = QK.masked_fill_(mask, -float(\"Inf\"))\n",
        "    A = nn.Softmax(dim=-1)(QK)\n",
        "    V = torch.transpose(v, dim0=0, dim1=1)\n",
        "    context = A @ V\n",
        "    context = torch.transpose(context, dim0=0, dim1=1)\n",
        "\n",
        "    tgt2 = self.context_proj_self(context)\n",
        "    tgt = tgt + tgt2\n",
        "    tgt = self.norm1(tgt)\n",
        "    # Cross attention (decoder attends to encoder)\n",
        "    q = self.q_proj_cross(tgt) / math.sqrt(self.hidden_size)\n",
        "    k = self.k_proj_cross(memory)\n",
        "    v = self.v_proj_cross(memory)\n",
        "    #TODO - compute `context`\n",
        "    Q = torch.transpose(q, dim0=0, dim1=1)\n",
        "    K = torch.transpose(k, dim0=0, dim1=1)\n",
        "    K = torch.transpose(K, dim0=1, dim1=2)\n",
        "    QK = Q @ K\n",
        "    mask = tgt_mask == False\n",
        "    QK = QK.masked_fill_(mask, -float(\"Inf\"))\n",
        "    A = nn.Softmax(dim=-1)(QK)\n",
        "    V = torch.transpose(v, dim0=0, dim1=1)\n",
        "    context = A @ V\n",
        "    context = torch.transpose(context, dim0=0, dim1=1)\n",
        "\n",
        "    tgt2 = self.context_proj_cross(context)\n",
        "    tgt = tgt + tgt2\n",
        "    tgt = self.norm2(tgt)\n",
        "    tgt2 = self.linear2(self.activation(self.linear1(tgt)))\n",
        "    tgt = tgt + tgt2\n",
        "    tgt = self.norm3(tgt)\n",
        "    return tgt\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "  \"\"\"\"Embeds a word both by its word id and by its position in the sentence.\"\"\"\n",
        "  def __init__(self, vocab_size, embedding_size, max_len=1024):\n",
        "    super(PositionalEmbedding, self).__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_size)\n",
        "    pe = torch.zeros(max_len, embedding_size)\n",
        "    position = torch.arange(0, max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, embedding_size, 2) *\n",
        "                         -(math.log(10000.0) / embedding_size))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(1) # max_len, 1, embedding_size\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, batch):\n",
        "    x = self.embed(batch) * math.sqrt(self.embedding_size) # type embedding\n",
        "    # Add positional encoding to type embedding\n",
        "    x = x + self.pe[:x.size(0)].detach()\n",
        "    return x\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "  \"\"\"Copies a module `N` times\"\"\"\n",
        "  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_rB1IFaMDa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba030faa-5f71-4374-a401-f9a3624cd6a7"
      },
      "source": [
        "EPOCHS = 2 # epochs, we highly recommend starting with a smaller number like 1\n",
        "LEARNING_RATE = 2e-3 # learning rate\n",
        "\n",
        "# Instantiate and train classifier\n",
        "model_transformer = TransformerEncoderDecoder(SRC, TGT,\n",
        "  hidden_size    = 64,\n",
        "  layers         = 3,\n",
        ").to(device)\n",
        "\n",
        "model_transformer.train_all(train_iter, val_iter, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
        "model_transformer.load_state_dict(model_transformer.best_model)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2032/2032 [01:09<00:00, 29.17it/s]\n",
            "  0%|          | 1/2032 [00:00<03:56,  8.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Training Perplexity: 1.3669 Validation Perplexity: 1.0790\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2032/2032 [01:09<00:00, 29.09it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Training Perplexity: 1.0458 Validation Perplexity: 1.0316\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XELmQ875MDa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe09e721-3e4f-4de0-b348-b478a026e34d"
      },
      "source": [
        "# Evaluate model performance, the expected value should be < 1.5\n",
        "print (f'Test perplexity: {model_transformer.evaluate_ppl(test_iter):.3f}')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test perplexity: 1.033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ao1FspHZMDa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "outputId": "2bdd8074-03de-4e7f-f908-aa5579603bf8"
      },
      "source": [
        "grader.check(\"transformer_ppl\")"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ],
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnbROfRdMDa-"
      },
      "source": [
        "Now that we have a trained model, we can decode from it using our previously implemented beam search function. If the code below throws any errors, you might need to modify your beam search code such that it generalizes here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "axmdPP1xMDa-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61cfe23f-ce07-4f4e-aba0-f2f57c99f851"
      },
      "source": [
        "grader.check(\"transformer_beam_search\")"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>0 of 1 tests passed</p>\n",
              "        \n",
              "        \n",
              "        <p> <strong>Tests failed: </strong>\n",
              "            <ul>\n",
              "            \n",
              "                <li> \n",
              "    <p><strong style='color: red;'>tests/transformer_beam_search.py</strong></p>\n",
              "    <p><strong>Test code:</strong><pre><div class=\"highlight\" style=\"background: #f8f8f8\"><pre style=\"line-height: 125%\"><span></span><span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>K <span style=\"color: #666666\">=</span> <span style=\"color: #666666\">5</span>;\n",
              "<span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>\n",
              "<span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>correct <span style=\"color: #666666\">=</span> <span style=\"color: #666666\">0</span>;\n",
              "<span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>total <span style=\"color: #666666\">=</span> <span style=\"color: #666666\">0</span>;\n",
              "<span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>\n",
              "<span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span><span style=\"color: #408080; font-style: italic\"># create beam searcher;</span>\n",
              "<span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>beam_searcher <span style=\"color: #666666\">=</span> BeamSearcher(model_transformer);\n",
              "<span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>\n",
              "<span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span><span style=\"color: #008000; font-weight: bold\">for</span> batch <span style=\"color: #AA22FF; font-weight: bold\">in</span> test_iter:\n",
              "<span style=\"color: #000080; font-weight: bold\">... </span>  <span style=\"color: #408080; font-style: italic\"># Input and output</span>\n",
              "<span style=\"color: #000080; font-weight: bold\">... </span>  src, src_lengths <span style=\"color: #666666\">=</span> batch<span style=\"color: #666666\">.</span>src\n",
              "<span style=\"color: #000080; font-weight: bold\">... </span>  <span style=\"color: #408080; font-style: italic\"># Predict</span>\n",
              "<span style=\"color: #000080; font-weight: bold\">... </span>  prediction, _ <span style=\"color: #666666\">=</span> beam_searcher<span style=\"color: #666666\">.</span>beam_search(src, src_lengths, K)\n",
              "<span style=\"color: #000080; font-weight: bold\">... </span>  <span style=\"color: #408080; font-style: italic\"># Convert to string</span>\n",
              "<span style=\"color: #000080; font-weight: bold\">... </span>  prediction <span style=\"color: #666666\">=</span> <span style=\"color: #BA2121\">&#39; &#39;</span><span style=\"color: #666666\">.</span>join([TGT<span style=\"color: #666666\">.</span>vocab<span style=\"color: #666666\">.</span>itos[token] <span style=\"color: #008000; font-weight: bold\">for</span> token <span style=\"color: #AA22FF; font-weight: bold\">in</span> prediction])\n",
              "<span style=\"color: #000080; font-weight: bold\">... </span>  prediction <span style=\"color: #666666\">=</span> prediction<span style=\"color: #666666\">.</span>lstrip(<span style=\"color: #BA2121\">&#39;&lt;bos&gt;&#39;</span>)<span style=\"color: #666666\">.</span>rstrip(<span style=\"color: #BA2121\">&#39;&lt;eos&gt;&#39;</span>)<span style=\"color: #666666\">.</span>strip()\n",
              "<span style=\"color: #000080; font-weight: bold\">... </span>  ground_truth <span style=\"color: #666666\">=</span> <span style=\"color: #BA2121\">&#39; &#39;</span><span style=\"color: #666666\">.</span>join([TGT<span style=\"color: #666666\">.</span>vocab<span style=\"color: #666666\">.</span>itos[token] <span style=\"color: #008000; font-weight: bold\">for</span> token <span style=\"color: #AA22FF; font-weight: bold\">in</span> batch<span style=\"color: #666666\">.</span>tgt<span style=\"color: #666666\">.</span>view(<span style=\"color: #666666\">-1</span>)])\n",
              "<span style=\"color: #000080; font-weight: bold\">... </span>  ground_truth <span style=\"color: #666666\">=</span> ground_truth<span style=\"color: #666666\">.</span>lstrip(<span style=\"color: #BA2121\">&#39;&lt;bos&gt;&#39;</span>)<span style=\"color: #666666\">.</span>rstrip(<span style=\"color: #BA2121\">&#39;&lt;eos&gt;&#39;</span>)<span style=\"color: #666666\">.</span>strip()\n",
              "<span style=\"color: #000080; font-weight: bold\">... </span>  <span style=\"color: #008000; font-weight: bold\">if</span> ground_truth <span style=\"color: #666666\">==</span> prediction:\n",
              "<span style=\"color: #000080; font-weight: bold\">... </span>    correct <span style=\"color: #666666\">+=</span> <span style=\"color: #666666\">1</span>\n",
              "<span style=\"color: #000080; font-weight: bold\">... </span>    <span style=\"color: #008000; font-weight: bold\">break</span>\n",
              "<span style=\"color: #000080; font-weight: bold\">... </span>  total <span style=\"color: #666666\">+=</span> <span style=\"color: #666666\">1</span>\n",
              "<span style=\"color: #000080; font-weight: bold\">... </span>  \n",
              "<span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>correct <span style=\"color: #666666\">&gt;</span> <span style=\"color: #666666\">0</span>\n",
              "<span style=\"color: #888888\">True</span>\n",
              "</pre></div>\n",
              "</pre></p>\n",
              "    <p><strong>Test result:</strong><pre>Trying:\n",
              "    K = 5;\n",
              "Expecting nothing\n",
              "ok\n",
              "Trying:\n",
              "    correct = 0;\n",
              "Expecting nothing\n",
              "ok\n",
              "Trying:\n",
              "    total = 0;\n",
              "Expecting nothing\n",
              "ok\n",
              "Trying:\n",
              "    beam_searcher = BeamSearcher(model_transformer);\n",
              "Expecting nothing\n",
              "ok\n",
              "Trying:\n",
              "    for batch in test_iter:\n",
              "      # Input and output\n",
              "      src, src_lengths = batch.src\n",
              "      # Predict\n",
              "      prediction, _ = beam_searcher.beam_search(src, src_lengths, K)\n",
              "      # Convert to string\n",
              "      prediction = ' '.join([TGT.vocab.itos[token] for token in prediction])\n",
              "      prediction = prediction.lstrip('<bos>').rstrip('<eos>').strip()\n",
              "      ground_truth = ' '.join([TGT.vocab.itos[token] for token in batch.tgt.view(-1)])\n",
              "      ground_truth = ground_truth.lstrip('<bos>').rstrip('<eos>').strip()\n",
              "      if ground_truth == prediction:\n",
              "        correct += 1\n",
              "        break\n",
              "      total += 1\n",
              "      \n",
              "Expecting nothing\n",
              "ok\n",
              "Trying:\n",
              "    correct > 0\n",
              "Expecting:\n",
              "    True\n",
              "**********************************************************************\n",
              "Line 24, in tests/transformer_beam_search.py 0\n",
              "Failed example:\n",
              "    correct > 0\n",
              "Expected:\n",
              "    True\n",
              "Got:\n",
              "    False\n",
              "</pre></p>\n",
              "     </li>\n",
              "            \n",
              "            </ul>\n",
              "        \n",
              "    \n",
              "    "
            ],
            "text/plain": [
              "\n",
              "    \n",
              "    0 of 1 tests passed\n",
              "    \n",
              "    \n",
              "    Tests failed:\n",
              "    \n",
              "           tests/transformer_beam_search.py\n",
              "\n",
              "Test result:\n",
              "Trying:\n",
              "    K = 5;\n",
              "Expecting nothing\n",
              "ok\n",
              "Trying:\n",
              "    correct = 0;\n",
              "Expecting nothing\n",
              "ok\n",
              "Trying:\n",
              "    total = 0;\n",
              "Expecting nothing\n",
              "ok\n",
              "Trying:\n",
              "    beam_searcher = BeamSearcher(model_transformer);\n",
              "Expecting nothing\n",
              "ok\n",
              "Trying:\n",
              "    for batch in test_iter:\n",
              "      # Input and output\n",
              "      src, src_lengths = batch.src\n",
              "      # Predict\n",
              "      prediction, _ = beam_searcher.beam_search(src, src_lengths, K)\n",
              "      # Convert to string\n",
              "      prediction = ' '.join([TGT.vocab.itos[token] for token in prediction])\n",
              "      prediction = prediction.lstrip('<bos>').rstrip('<eos>').strip()\n",
              "      ground_truth = ' '.join([TGT.vocab.itos[token] for token in batch.tgt.view(-1)])\n",
              "      ground_truth = ground_truth.lstrip('<bos>').rstrip('<eos>').strip()\n",
              "      if ground_truth == prediction:\n",
              "        correct += 1\n",
              "        break\n",
              "      total += 1\n",
              "      \n",
              "Expecting nothing\n",
              "ok\n",
              "Trying:\n",
              "    correct > 0\n",
              "Expecting:\n",
              "    True\n",
              "**********************************************************************\n",
              "Line 24, in tests/transformer_beam_search.py 0\n",
              "Failed example:\n",
              "    correct > 0\n",
              "Expected:\n",
              "    True\n",
              "Got:\n",
              "    False\n",
              "\n",
              "    \n",
              "    \n",
              "    \n",
              "    "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9mmy45zMDa-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b60103-472c-4393-e8c6-8c2a7ca07332"
      },
      "source": [
        "DEBUG = True # set to False to disable printing predictions\n",
        "K = 1 # beam size 1\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# create beam searcher\n",
        "beam_searcher = BeamSearcher(model_transformer)\n",
        "\n",
        "for batch in test_iter:\n",
        "  # Input and output\n",
        "  src, src_lengths = batch.src\n",
        "  # Predict\n",
        "  model.all_attns = []\n",
        "  prediction, _ = beam_searcher.beam_search(src, src_lengths, K)\n",
        "  # Convert to string\n",
        "  prediction = ' '.join([TGT.vocab.itos[token] for token in prediction])\n",
        "  prediction = prediction.lstrip('<bos>').rstrip('<eos>').strip()\n",
        "  ground_truth = ' '.join([TGT.vocab.itos[token] for token in batch.tgt.view(-1)])\n",
        "  ground_truth = ground_truth.lstrip('<bos>').rstrip('<eos>').strip()\n",
        "  if DEBUG:\n",
        "    src = ' '.join([SRC.vocab.itos[item] for item in src.view(-1)])\n",
        "    print (f'Source: {src}')\n",
        "    print (f'Prediction:   {prediction}')\n",
        "    print (f'Ground truth: {ground_truth}')\n",
        "  if ground_truth == prediction:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "\n",
        "print (f'Accuracy: {correct/total:.2f}')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source: sixteen thousand eight hundred and thirty two\n",
            "Prediction:   \n",
            "Ground truth: 1 6 8 3 2\n",
            "Source: sixty seven million six hundred and eighty five thousand two hundred and thirty\n",
            "Prediction:   \n",
            "Ground truth: 6 7 6 8 5 2 3 0\n",
            "Source: six thousand two hundred and twelve\n",
            "Prediction:   \n",
            "Ground truth: 6 2 1 2\n",
            "Source: seven hundred and ninety eight million three hundred and thirty one thousand eight hundred and eighteen\n",
            "Prediction:   \n",
            "Ground truth: 7 9 8 3 3 1 8 1 8\n",
            "Source: eighty eight million four hundred and thirteen thousand nine hundred and eighteen\n",
            "Prediction:   \n",
            "Ground truth: 8 8 4 1 3 9 1 8\n",
            "Source: three hundred and seventy four thousand two hundred and seventy\n",
            "Prediction:   \n",
            "Ground truth: 3 7 4 2 7 0\n",
            "Source: ninety eight million three hundred and seventy thousand five hundred and forty five\n",
            "Prediction:   \n",
            "Ground truth: 9 8 3 7 0 5 4 5\n",
            "Source: ninety seven thousand seven hundred and sixty two\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 7 7 6 2\n",
            "Source: four hundred and ten thousand two hundred and three\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 1 0 2 0 3\n",
            "Source: five hundred and ninety eight thousand three hundred and ninety seven\n",
            "Prediction:   \n",
            "Ground truth: 5 9 8 3 9 7\n",
            "Source: eighty nine thousand and forty three\n",
            "Prediction:   \n",
            "Ground truth: 8 9 0 4 3\n",
            "Source: eight hundred million five hundred and sixty two thousand seven hundred and twenty seven\n",
            "Prediction:   \n",
            "Ground truth: 8 0 0 5 6 2 7 2 7\n",
            "Source: nine hundred and forty seven million seven hundred and twenty five thousand one hundred and eighty five\n",
            "Prediction:   \n",
            "Ground truth: 9 4 7 7 2 5 1 8 5\n",
            "Source: four billion six hundred and fifty seven million seven hundred and sixty seven thousand one hundred and three\n",
            "Prediction:   4 4 4 4\n",
            "Ground truth: 4 6 5 7 7 6 7 1 0 3\n",
            "Source: thirty one million eight hundred and thirty thousand two hundred and fifty four\n",
            "Prediction:   \n",
            "Ground truth: 3 1 8 3 0 2 5 4\n",
            "Source: three billion seven hundred and eighty two million one hundred and sixty two thousand three hundred and ninety\n",
            "Prediction:   \n",
            "Ground truth: 3 7 8 2 1 6 2 3 9 0\n",
            "Source: thirty five million six hundred and twenty four thousand seven hundred\n",
            "Prediction:   \n",
            "Ground truth: 3 5 6 2 4 7 0 0\n",
            "Source: six billion three hundred and eleven million nine hundred and eighty seven thousand six hundred and seventy four\n",
            "Prediction:   \n",
            "Ground truth: 6 3 1 1 9 8 7 6 7 4\n",
            "Source: six hundred and fifty eight thousand six hundred and ninety eight\n",
            "Prediction:   \n",
            "Ground truth: 6 5 8 6 9 8\n",
            "Source: eight billion nine hundred and thirty seven million six hundred and thirty four thousand three hundred and seven\n",
            "Prediction:   \n",
            "Ground truth: 8 9 3 7 6 3 4 3 0 7\n",
            "Source: one hundred and three thousand six hundred and ninety four\n",
            "Prediction:   \n",
            "Ground truth: 1 0 3 6 9 4\n",
            "Source: seven hundred and fifty eight million seven hundred and fifty nine thousand one hundred and twenty seven\n",
            "Prediction:   \n",
            "Ground truth: 7 5 8 7 5 9 1 2 7\n",
            "Source: nine thousand four hundred and twelve\n",
            "Prediction:   9 2\n",
            "Ground truth: 9 4 1 2\n",
            "Source: two hundred and thirty one million six hundred and fifteen thousand nine hundred and twenty\n",
            "Prediction:   \n",
            "Ground truth: 2 3 1 6 1 5 9 2 0\n",
            "Source: seven million one hundred and forty five thousand six hundred and eighteen\n",
            "Prediction:   \n",
            "Ground truth: 7 1 4 5 6 1 8\n",
            "Source: fourteen thousand five hundred and sixty\n",
            "Prediction:   \n",
            "Ground truth: 1 4 5 6 0\n",
            "Source: four billion two hundred and thirteen million six hundred and twenty six thousand nine hundred and sixty nine\n",
            "Prediction:   \n",
            "Ground truth: 4 2 1 3 6 2 6 9 6 9\n",
            "Source: six million three hundred and sixty three thousand and fourteen\n",
            "Prediction:   \n",
            "Ground truth: 6 3 6 3 0 1 4\n",
            "Source: eight million sixty eight thousand four hundred and eighty one\n",
            "Prediction:   \n",
            "Ground truth: 8 0 6 8 4 8 1\n",
            "Source: nine billion one hundred and sixty seven million eighty eight thousand six hundred and seventy eight\n",
            "Prediction:   \n",
            "Ground truth: 9 1 6 7 0 8 8 6 7 8\n",
            "Source: four million nine hundred and thirty one thousand five hundred and fifty seven\n",
            "Prediction:   4 4 4 4\n",
            "Ground truth: 4 9 3 1 5 5 7\n",
            "Source: six hundred and forty seven million six hundred and fourteen thousand two hundred and seventy seven\n",
            "Prediction:   \n",
            "Ground truth: 6 4 7 6 1 4 2 7 7\n",
            "Source: three thousand four hundred and seventy seven\n",
            "Prediction:   \n",
            "Ground truth: 3 4 7 7\n",
            "Source: forty six thousand six hundred and five\n",
            "Prediction:   \n",
            "Ground truth: 4 6 6 0 5\n",
            "Source: eight thousand seven hundred and sixty eight\n",
            "Prediction:   \n",
            "Ground truth: 8 7 6 8\n",
            "Source: three million four hundred and sixteen thousand five hundred and sixty six\n",
            "Prediction:   \n",
            "Ground truth: 3 4 1 6 5 6 6\n",
            "Source: two hundred and forty seven thousand five hundred and twenty eight\n",
            "Prediction:   \n",
            "Ground truth: 2 4 7 5 2 8\n",
            "Source: three hundred and four thousand three hundred and sixty four\n",
            "Prediction:   \n",
            "Ground truth: 3 0 4 3 6 4\n",
            "Source: three hundred and seventy six million three hundred and ninety one thousand eight hundred and thirty two\n",
            "Prediction:   \n",
            "Ground truth: 3 7 6 3 9 1 8 3 2\n",
            "Source: four hundred and nineteen million five hundred and eighty eight thousand three hundred and twenty one\n",
            "Prediction:   4 4 4 4\n",
            "Ground truth: 4 1 9 5 8 8 3 2 1\n",
            "Source: three hundred and eighty five million eight hundred and thirty two thousand two hundred and seventy two\n",
            "Prediction:   \n",
            "Ground truth: 3 8 5 8 3 2 2 7 2\n",
            "Source: sixty six million two hundred and ninety five thousand and forty\n",
            "Prediction:   \n",
            "Ground truth: 6 6 2 9 5 0 4 0\n",
            "Source: three hundred and forty five million five hundred and forty two thousand five hundred and twenty two\n",
            "Prediction:   \n",
            "Ground truth: 3 4 5 5 4 2 5 2 2\n",
            "Source: one hundred and ten thousand six hundred and fifty five\n",
            "Prediction:   \n",
            "Ground truth: 1 1 0 6 5 5\n",
            "Source: five million two hundred and eighteen thousand six hundred and eleven\n",
            "Prediction:   \n",
            "Ground truth: 5 2 1 8 6 1 1\n",
            "Source: seven thousand eight hundred and fourteen\n",
            "Prediction:   \n",
            "Ground truth: 7 8 1 4\n",
            "Source: five million seven hundred and sixty two thousand eight hundred and eighty nine\n",
            "Prediction:   \n",
            "Ground truth: 5 7 6 2 8 8 9\n",
            "Source: two million sixty nine thousand three hundred and seventy three\n",
            "Prediction:   \n",
            "Ground truth: 2 0 6 9 3 7 3\n",
            "Source: three hundred and seven million nine hundred and twenty one thousand four hundred and fourteen\n",
            "Prediction:   \n",
            "Ground truth: 3 0 7 9 2 1 4 1 4\n",
            "Source: two billion nine hundred and eighty seven million one hundred and thirty thousand eight hundred and seventy six\n",
            "Prediction:   \n",
            "Ground truth: 2 9 8 7 1 3 0 8 7 6\n",
            "Source: eight million two hundred and ninety thousand two hundred and sixty nine\n",
            "Prediction:   \n",
            "Ground truth: 8 2 9 0 2 6 9\n",
            "Source: eight billion one million six hundred and forty four thousand nine hundred and twenty nine\n",
            "Prediction:   \n",
            "Ground truth: 8 0 0 1 6 4 4 9 2 9\n",
            "Source: one hundred and eighty thousand four hundred and forty one\n",
            "Prediction:   \n",
            "Ground truth: 1 8 0 4 4 1\n",
            "Source: five hundred and fifty three million six hundred and five thousand four hundred and sixty one\n",
            "Prediction:   \n",
            "Ground truth: 5 5 3 6 0 5 4 6 1\n",
            "Source: nine hundred and twenty seven million two hundred and seventy three thousand three hundred and thirty eight\n",
            "Prediction:   \n",
            "Ground truth: 9 2 7 2 7 3 3 3 8\n",
            "Source: four thousand three hundred and twenty one\n",
            "Prediction:   4 4\n",
            "Ground truth: 4 3 2 1\n",
            "Source: fifty two million five hundred and fifty six thousand six hundred and sixty six\n",
            "Prediction:   \n",
            "Ground truth: 5 2 5 5 6 6 6 6\n",
            "Source: four thousand three hundred and fifty six\n",
            "Prediction:   4 4\n",
            "Ground truth: 4 3 5 6\n",
            "Source: four million one hundred and forty three thousand five hundred and one\n",
            "Prediction:   4 4 4 4\n",
            "Ground truth: 4 1 4 3 5 0 1\n",
            "Source: eighty million three hundred and four thousand four hundred and sixty six\n",
            "Prediction:   \n",
            "Ground truth: 8 0 3 0 4 4 6 6\n",
            "Source: nine hundred and thirty thousand three hundred and thirty four\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 3 0 3 3 4\n",
            "Source: three thousand nine hundred and fifty four\n",
            "Prediction:   \n",
            "Ground truth: 3 9 5 4\n",
            "Source: forty three thousand three hundred and thirty six\n",
            "Prediction:   \n",
            "Ground truth: 4 3 3 3 6\n",
            "Source: seventy one thousand three hundred and eighteen\n",
            "Prediction:   \n",
            "Ground truth: 7 1 3 1 8\n",
            "Source: four hundred and six thousand four hundred and seven\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 0 6 4 0 7\n",
            "Source: two thousand three hundred and ninety three\n",
            "Prediction:   \n",
            "Ground truth: 2 3 9 3\n",
            "Source: eighteen million four hundred and eighty five thousand two hundred and ten\n",
            "Prediction:   \n",
            "Ground truth: 1 8 4 8 5 2 1 0\n",
            "Source: one billion six hundred and forty five million eight hundred and twenty nine thousand nine hundred and eleven\n",
            "Prediction:   \n",
            "Ground truth: 1 6 4 5 8 2 9 9 1 1\n",
            "Source: three thousand six hundred and seventeen\n",
            "Prediction:   \n",
            "Ground truth: 3 6 1 7\n",
            "Source: two thousand four hundred and sixty four\n",
            "Prediction:   \n",
            "Ground truth: 2 4 6 4\n",
            "Source: four thousand one hundred and ninety three\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 1 9 3\n",
            "Source: five billion eight hundred and seventy three million four hundred and twenty nine thousand nine hundred and fifty six\n",
            "Prediction:   \n",
            "Ground truth: 5 8 7 3 4 2 9 9 5 6\n",
            "Source: six hundred and ninety thousand one hundred and eighteen\n",
            "Prediction:   \n",
            "Ground truth: 6 9 0 1 1 8\n",
            "Source: ninety four thousand two hundred and twenty two\n",
            "Prediction:   \n",
            "Ground truth: 9 4 2 2 2\n",
            "Source: eight thousand nine hundred and twenty six\n",
            "Prediction:   \n",
            "Ground truth: 8 9 2 6\n",
            "Source: two billion eight hundred and ninety six million six hundred and eighty six thousand and thirty two\n",
            "Prediction:   \n",
            "Ground truth: 2 8 9 6 6 8 6 0 3 2\n",
            "Source: two million nine hundred and ten thousand nine hundred and eighty four\n",
            "Prediction:   \n",
            "Ground truth: 2 9 1 0 9 8 4\n",
            "Source: ninety two million seven hundred thousand three hundred and eighty\n",
            "Prediction:   \n",
            "Ground truth: 9 2 7 0 0 3 8 0\n",
            "Source: seventy four million six hundred and fifty thousand two hundred and forty seven\n",
            "Prediction:   \n",
            "Ground truth: 7 4 6 5 0 2 4 7\n",
            "Source: thirty seven million two hundred and sixty two thousand four hundred and fifty four\n",
            "Prediction:   \n",
            "Ground truth: 3 7 2 6 2 4 5 4\n",
            "Source: thirty nine thousand five hundred and thirty eight\n",
            "Prediction:   \n",
            "Ground truth: 3 9 5 3 8\n",
            "Source: four hundred and seventy nine thousand four hundred and forty six\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 7 9 4 4 6\n",
            "Source: three thousand five hundred and three\n",
            "Prediction:   \n",
            "Ground truth: 3 5 0 3\n",
            "Source: ninety two thousand seven hundred and thirty seven\n",
            "Prediction:   \n",
            "Ground truth: 9 2 7 3 7\n",
            "Source: one hundred and eight million three hundred and forty eight thousand six hundred and eighteen\n",
            "Prediction:   \n",
            "Ground truth: 1 0 8 3 4 8 6 1 8\n",
            "Source: eight thousand three hundred and thirteen\n",
            "Prediction:   \n",
            "Ground truth: 8 3 1 3\n",
            "Source: nine billion eight hundred and twenty seven million eight hundred and eighty four thousand nine hundred and ninety\n",
            "Prediction:   \n",
            "Ground truth: 9 8 2 7 8 8 4 9 9 0\n",
            "Source: four million eight hundred and twenty thousand one hundred and sixteen\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 8 2 0 1 1 6\n",
            "Source: seventy five thousand three hundred and fifty eight\n",
            "Prediction:   \n",
            "Ground truth: 7 5 3 5 8\n",
            "Source: two hundred and ninety one million forty four thousand two hundred and thirty\n",
            "Prediction:   \n",
            "Ground truth: 2 9 1 0 4 4 2 3 0\n",
            "Source: two thousand seven hundred and thirteen\n",
            "Prediction:   \n",
            "Ground truth: 2 7 1 3\n",
            "Source: one hundred and fifty million nine thousand five hundred and thirty one\n",
            "Prediction:   \n",
            "Ground truth: 1 5 0 0 0 9 5 3 1\n",
            "Source: nine billion two hundred and forty five million forty seven thousand one hundred and seventy six\n",
            "Prediction:   \n",
            "Ground truth: 9 2 4 5 0 4 7 1 7 6\n",
            "Source: one million three hundred and sixty three thousand three hundred and ninety eight\n",
            "Prediction:   \n",
            "Ground truth: 1 3 6 3 3 9 8\n",
            "Source: two billion twelve million one hundred and forty eight thousand five hundred and twenty eight\n",
            "Prediction:   \n",
            "Ground truth: 2 0 1 2 1 4 8 5 2 8\n",
            "Source: one hundred and forty eight thousand six hundred and sixty three\n",
            "Prediction:   \n",
            "Ground truth: 1 4 8 6 6 3\n",
            "Source: ninety two thousand nine hundred and eighty four\n",
            "Prediction:   \n",
            "Ground truth: 9 2 9 8 4\n",
            "Source: six billion eight hundred and thirty six million two hundred and twelve thousand five hundred and forty eight\n",
            "Prediction:   \n",
            "Ground truth: 6 8 3 6 2 1 2 5 4 8\n",
            "Source: fifty one million four hundred and thirty one thousand seven hundred and fifty\n",
            "Prediction:   \n",
            "Ground truth: 5 1 4 3 1 7 5 0\n",
            "Source: eight hundred and sixty one thousand nine hundred and fifty seven\n",
            "Prediction:   \n",
            "Ground truth: 8 6 1 9 5 7\n",
            "Source: six million two hundred and eighty thousand nine hundred and sixty eight\n",
            "Prediction:   \n",
            "Ground truth: 6 2 8 0 9 6 8\n",
            "Source: eight hundred and forty five million four hundred and forty two thousand seven hundred and sixty seven\n",
            "Prediction:   \n",
            "Ground truth: 8 4 5 4 4 2 7 6 7\n",
            "Source: eighty seven thousand three hundred and thirteen\n",
            "Prediction:   \n",
            "Ground truth: 8 7 3 1 3\n",
            "Source: seven billion four hundred and ninety two million eight hundred and twenty five thousand two hundred and sixty one\n",
            "Prediction:   \n",
            "Ground truth: 7 4 9 2 8 2 5 2 6 1\n",
            "Source: seven hundred and nine million four hundred and twenty seven thousand two hundred and eighty nine\n",
            "Prediction:   \n",
            "Ground truth: 7 0 9 4 2 7 2 8 9\n",
            "Source: eight million four hundred and seventy thousand two hundred and seventy five\n",
            "Prediction:   \n",
            "Ground truth: 8 4 7 0 2 7 5\n",
            "Source: two thousand five hundred and seventy\n",
            "Prediction:   \n",
            "Ground truth: 2 5 7 0\n",
            "Source: six hundred and eighteen thousand seven hundred and sixty six\n",
            "Prediction:   \n",
            "Ground truth: 6 1 8 7 6 6\n",
            "Source: three billion four hundred and sixty one million seventy one thousand one hundred and fourteen\n",
            "Prediction:   \n",
            "Ground truth: 3 4 6 1 0 7 1 1 1 4\n",
            "Source: seventy million seven hundred and thirty five thousand six hundred and forty seven\n",
            "Prediction:   \n",
            "Ground truth: 7 0 7 3 5 6 4 7\n",
            "Source: one billion six hundred and ninety seven million two hundred and eighty six thousand one hundred and eighty two\n",
            "Prediction:   \n",
            "Ground truth: 1 6 9 7 2 8 6 1 8 2\n",
            "Source: ten thousand nine hundred and seventy seven\n",
            "Prediction:   \n",
            "Ground truth: 1 0 9 7 7\n",
            "Source: fifty seven million nine hundred and forty five thousand nine hundred and fifty three\n",
            "Prediction:   \n",
            "Ground truth: 5 7 9 4 5 9 5 3\n",
            "Source: seven thousand nine hundred and sixty two\n",
            "Prediction:   \n",
            "Ground truth: 7 9 6 2\n",
            "Source: sixty thousand nine hundred and thirteen\n",
            "Prediction:   \n",
            "Ground truth: 6 0 9 1 3\n",
            "Source: one hundred and forty eight million six hundred and thirty five thousand two hundred and twenty seven\n",
            "Prediction:   \n",
            "Ground truth: 1 4 8 6 3 5 2 2 7\n",
            "Source: seven hundred and thirty eight thousand one hundred and seventy three\n",
            "Prediction:   \n",
            "Ground truth: 7 3 8 1 7 3\n",
            "Source: eight hundred and ninety eight million seven hundred and fifty thousand two hundred and fifty one\n",
            "Prediction:   \n",
            "Ground truth: 8 9 8 7 5 0 2 5 1\n",
            "Source: two hundred and forty nine million eight hundred and seventeen thousand three hundred and two\n",
            "Prediction:   \n",
            "Ground truth: 2 4 9 8 1 7 3 0 2\n",
            "Source: nineteen million three hundred and nineteen thousand and seventy four\n",
            "Prediction:   \n",
            "Ground truth: 1 9 3 1 9 0 7 4\n",
            "Source: eight billion two hundred and thirteen million five hundred and eighty three thousand three hundred and fifty six\n",
            "Prediction:   \n",
            "Ground truth: 8 2 1 3 5 8 3 3 5 6\n",
            "Source: fifty nine thousand eight hundred and seventy one\n",
            "Prediction:   \n",
            "Ground truth: 5 9 8 7 1\n",
            "Source: nine hundred and ninety two thousand three hundred and five\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 9 2 3 0 5\n",
            "Source: three billion eight hundred and eleven million seven hundred and eighty thousand and ninety\n",
            "Prediction:   \n",
            "Ground truth: 3 8 1 1 7 8 0 0 9 0\n",
            "Source: three hundred and fifty five million two hundred and thirty five thousand nine hundred and seventy four\n",
            "Prediction:   \n",
            "Ground truth: 3 5 5 2 3 5 9 7 4\n",
            "Source: nine hundred and seventy five million three hundred and two thousand nine hundred and thirty four\n",
            "Prediction:   \n",
            "Ground truth: 9 7 5 3 0 2 9 3 4\n",
            "Source: four thousand nine hundred and thirty five\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 9 3 5\n",
            "Source: eight hundred and twenty eight thousand three hundred and thirty eight\n",
            "Prediction:   \n",
            "Ground truth: 8 2 8 3 3 8\n",
            "Source: two thousand four hundred and thirty\n",
            "Prediction:   \n",
            "Ground truth: 2 4 3 0\n",
            "Source: nine hundred and ninety five thousand eight hundred and three\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 9 5 8 0 3\n",
            "Source: six hundred and seventy four thousand nine hundred and ninety three\n",
            "Prediction:   \n",
            "Ground truth: 6 7 4 9 9 3\n",
            "Source: seven hundred and thirty nine million three hundred and twenty one thousand four hundred and fifty three\n",
            "Prediction:   \n",
            "Ground truth: 7 3 9 3 2 1 4 5 3\n",
            "Source: nine billion eight hundred and eighty nine million six hundred and fifty seven thousand eight hundred and seventy four\n",
            "Prediction:   \n",
            "Ground truth: 9 8 8 9 6 5 7 8 7 4\n",
            "Source: six thousand seven hundred and forty five\n",
            "Prediction:   \n",
            "Ground truth: 6 7 4 5\n",
            "Source: ninety two thousand eight hundred and twelve\n",
            "Prediction:   \n",
            "Ground truth: 9 2 8 1 2\n",
            "Source: five million three hundred and seventy nine thousand two hundred and eighty six\n",
            "Prediction:   \n",
            "Ground truth: 5 3 7 9 2 8 6\n",
            "Source: seventy four million six hundred and five thousand nine hundred and thirty six\n",
            "Prediction:   \n",
            "Ground truth: 7 4 6 0 5 9 3 6\n",
            "Source: eight million ninety three thousand nine hundred and seventy two\n",
            "Prediction:   \n",
            "Ground truth: 8 0 9 3 9 7 2\n",
            "Source: four hundred and thirty four thousand three hundred and eighty five\n",
            "Prediction:   4 4 8 8\n",
            "Ground truth: 4 3 4 3 8 5\n",
            "Source: eight hundred and sixty four million one thousand four hundred and twenty five\n",
            "Prediction:   \n",
            "Ground truth: 8 6 4 0 0 1 4 2 5\n",
            "Source: nine million one hundred thousand one hundred and seventy\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 1 0 0 1 7 0\n",
            "Source: five hundred and seventy seven thousand seven hundred and sixty two\n",
            "Prediction:   \n",
            "Ground truth: 5 7 7 7 6 2\n",
            "Source: seven million two hundred and thirty nine thousand three hundred and thirty one\n",
            "Prediction:   \n",
            "Ground truth: 7 2 3 9 3 3 1\n",
            "Source: eight thousand four hundred and sixty eight\n",
            "Prediction:   \n",
            "Ground truth: 8 4 6 8\n",
            "Source: ninety six thousand eight hundred and seventeen\n",
            "Prediction:   \n",
            "Ground truth: 9 6 8 1 7\n",
            "Source: five hundred and nineteen thousand seven hundred and one\n",
            "Prediction:   \n",
            "Ground truth: 5 1 9 7 0 1\n",
            "Source: twenty nine million seven hundred and sixty two thousand two hundred and thirty nine\n",
            "Prediction:   \n",
            "Ground truth: 2 9 7 6 2 2 3 9\n",
            "Source: thirty six thousand one hundred and sixty two\n",
            "Prediction:   \n",
            "Ground truth: 3 6 1 6 2\n",
            "Source: five hundred and sixty four million two hundred and ninety nine thousand two hundred and twenty\n",
            "Prediction:   \n",
            "Ground truth: 5 6 4 2 9 9 2 2 0\n",
            "Source: one hundred and sixty eight million thirty five thousand eight hundred and twenty two\n",
            "Prediction:   \n",
            "Ground truth: 1 6 8 0 3 5 8 2 2\n",
            "Source: three hundred and fifty two million ten thousand seven hundred and fifty two\n",
            "Prediction:   \n",
            "Ground truth: 3 5 2 0 1 0 7 5 2\n",
            "Source: eight hundred and ninety five thousand six hundred and three\n",
            "Prediction:   \n",
            "Ground truth: 8 9 5 6 0 3\n",
            "Source: five hundred and forty five million four hundred and seventy nine thousand eight hundred and eighty eight\n",
            "Prediction:   \n",
            "Ground truth: 5 4 5 4 7 9 8 8 8\n",
            "Source: one million two hundred and four thousand and twenty three\n",
            "Prediction:   \n",
            "Ground truth: 1 2 0 4 0 2 3\n",
            "Source: seventy three thousand nine hundred and eight\n",
            "Prediction:   \n",
            "Ground truth: 7 3 9 0 8\n",
            "Source: four million three hundred and fifty thousand five hundred and eighty four\n",
            "Prediction:   4 4 4 4 4 4 4\n",
            "Ground truth: 4 3 5 0 5 8 4\n",
            "Source: seven billion nine hundred and forty one million eight hundred and twenty three thousand nine hundred and thirty five\n",
            "Prediction:   \n",
            "Ground truth: 7 9 4 1 8 2 3 9 3 5\n",
            "Source: nine billion three hundred and sixty five million seven hundred and fifty four thousand two hundred and seventy seven\n",
            "Prediction:   \n",
            "Ground truth: 9 3 6 5 7 5 4 2 7 7\n",
            "Source: eight hundred and twenty four million eight hundred and forty nine thousand eight hundred and six\n",
            "Prediction:   \n",
            "Ground truth: 8 2 4 8 4 9 8 0 6\n",
            "Source: nine hundred and seven thousand and twenty one\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 0 7 0 2 1\n",
            "Source: five hundred and six thousand nine hundred and forty four\n",
            "Prediction:   \n",
            "Ground truth: 5 0 6 9 4 4\n",
            "Source: eight thousand six hundred and eighty three\n",
            "Prediction:   \n",
            "Ground truth: 8 6 8 3\n",
            "Source: seven million one hundred and ninety three thousand five hundred and eighty\n",
            "Prediction:   \n",
            "Ground truth: 7 1 9 3 5 8 0\n",
            "Source: three million seven thousand nine hundred and fifty four\n",
            "Prediction:   \n",
            "Ground truth: 3 0 0 7 9 5 4\n",
            "Source: ninety seven thousand four hundred and thirty three\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 7 4 3 3\n",
            "Source: sixty five million seven hundred and sixty five thousand eight hundred and seventeen\n",
            "Prediction:   \n",
            "Ground truth: 6 5 7 6 5 8 1 7\n",
            "Source: forty three million eight hundred and twenty eight thousand one hundred and twenty\n",
            "Prediction:   \n",
            "Ground truth: 4 3 8 2 8 1 2 0\n",
            "Source: nine billion four hundred and fifty million two hundred and four thousand one hundred and sixty eight\n",
            "Prediction:   \n",
            "Ground truth: 9 4 5 0 2 0 4 1 6 8\n",
            "Source: ninety two thousand three hundred and ninety eight\n",
            "Prediction:   \n",
            "Ground truth: 9 2 3 9 8\n",
            "Source: two hundred and forty three million three hundred and forty three thousand nine hundred and twenty nine\n",
            "Prediction:   \n",
            "Ground truth: 2 4 3 3 4 3 9 2 9\n",
            "Source: seven billion three hundred and seventeen million three hundred and thirty one thousand nine hundred and forty five\n",
            "Prediction:   \n",
            "Ground truth: 7 3 1 7 3 3 1 9 4 5\n",
            "Source: seven billion nine hundred and eighty seven million one hundred and fifty seven thousand eight hundred and fifty\n",
            "Prediction:   \n",
            "Ground truth: 7 9 8 7 1 5 7 8 5 0\n",
            "Source: seventy three million eight hundred and sixty thousand and thirty five\n",
            "Prediction:   \n",
            "Ground truth: 7 3 8 6 0 0 3 5\n",
            "Source: five hundred and twenty two thousand eight hundred and twenty four\n",
            "Prediction:   \n",
            "Ground truth: 5 2 2 8 2 4\n",
            "Source: seven billion five hundred and fifty three million five hundred and twenty seven thousand two hundred and seventy one\n",
            "Prediction:   \n",
            "Ground truth: 7 5 5 3 5 2 7 2 7 1\n",
            "Source: one hundred and forty five thousand seven hundred and ninety three\n",
            "Prediction:   \n",
            "Ground truth: 1 4 5 7 9 3\n",
            "Source: six million four hundred and nineteen thousand one hundred and seventy six\n",
            "Prediction:   \n",
            "Ground truth: 6 4 1 9 1 7 6\n",
            "Source: eight hundred and seventy eight thousand two hundred and eleven\n",
            "Prediction:   \n",
            "Ground truth: 8 7 8 2 1 1\n",
            "Source: sixty one million three hundred and sixty seven thousand six hundred and thirty six\n",
            "Prediction:   \n",
            "Ground truth: 6 1 3 6 7 6 3 6\n",
            "Source: six billion nine hundred and forty six million seven hundred and eighty two thousand one hundred and sixty two\n",
            "Prediction:   \n",
            "Ground truth: 6 9 4 6 7 8 2 1 6 2\n",
            "Source: five million two hundred and thirty nine thousand four hundred and three\n",
            "Prediction:   \n",
            "Ground truth: 5 2 3 9 4 0 3\n",
            "Source: seventy nine million six hundred and sixty four thousand three hundred and fifty eight\n",
            "Prediction:   \n",
            "Ground truth: 7 9 6 6 4 3 5 8\n",
            "Source: six thousand three hundred and ninety eight\n",
            "Prediction:   \n",
            "Ground truth: 6 3 9 8\n",
            "Source: five thousand and twenty eight\n",
            "Prediction:   \n",
            "Ground truth: 5 0 2 8\n",
            "Source: three billion one hundred and eighty million nine hundred and eighty one thousand three hundred and seventy\n",
            "Prediction:   \n",
            "Ground truth: 3 1 8 0 9 8 1 3 7 0\n",
            "Source: sixteen million eight hundred and twenty three thousand eight hundred and seventy eight\n",
            "Prediction:   \n",
            "Ground truth: 1 6 8 2 3 8 7 8\n",
            "Source: eighty seven thousand four hundred and twenty six\n",
            "Prediction:   \n",
            "Ground truth: 8 7 4 2 6\n",
            "Source: six hundred and forty four thousand six hundred and fifty eight\n",
            "Prediction:   \n",
            "Ground truth: 6 4 4 6 5 8\n",
            "Source: eighty five million one hundred and ninety thousand two hundred and forty seven\n",
            "Prediction:   \n",
            "Ground truth: 8 5 1 9 0 2 4 7\n",
            "Source: thirty eight million two hundred and thirty four thousand four hundred and seventy two\n",
            "Prediction:   \n",
            "Ground truth: 3 8 2 3 4 4 7 2\n",
            "Source: nine hundred and forty four thousand and eighty\n",
            "Prediction:   9 9\n",
            "Ground truth: 9 4 4 0 8 0\n",
            "Source: four million four hundred and forty two thousand six hundred and thirty three\n",
            "Prediction:   4 4 4 4\n",
            "Ground truth: 4 4 4 2 6 3 3\n",
            "Source: six hundred and seventy two thousand one hundred and sixty two\n",
            "Prediction:   \n",
            "Ground truth: 6 7 2 1 6 2\n",
            "Source: five hundred and eighty three million six hundred and fifteen thousand and seventy nine\n",
            "Prediction:   \n",
            "Ground truth: 5 8 3 6 1 5 0 7 9\n",
            "Source: two hundred and ninety eight million nine hundred and thirteen thousand eight hundred and forty one\n",
            "Prediction:   \n",
            "Ground truth: 2 9 8 9 1 3 8 4 1\n",
            "Source: three million one hundred and thirty thousand nine hundred and three\n",
            "Prediction:   \n",
            "Ground truth: 3 1 3 0 9 0 3\n",
            "Source: seven million eight hundred and eight thousand five hundred and thirty one\n",
            "Prediction:   \n",
            "Ground truth: 7 8 0 8 5 3 1\n",
            "Source: seventy seven million seven hundred and eight thousand nine hundred and fifty eight\n",
            "Prediction:   \n",
            "Ground truth: 7 7 7 0 8 9 5 8\n",
            "Source: sixty two million seventy thousand eight hundred and ten\n",
            "Prediction:   \n",
            "Ground truth: 6 2 0 7 0 8 1 0\n",
            "Source: seven billion five hundred and twelve million seven hundred and forty eight thousand two hundred and thirty eight\n",
            "Prediction:   \n",
            "Ground truth: 7 5 1 2 7 4 8 2 3 8\n",
            "Source: two hundred and sixty three thousand seven hundred and ninety three\n",
            "Prediction:   \n",
            "Ground truth: 2 6 3 7 9 3\n",
            "Source: seven billion two hundred and seventy one million twenty one thousand eight hundred and twenty one\n",
            "Prediction:   \n",
            "Ground truth: 7 2 7 1 0 2 1 8 2 1\n",
            "Source: eight million four hundred and eighty five thousand five hundred and seventy six\n",
            "Prediction:   \n",
            "Ground truth: 8 4 8 5 5 7 6\n",
            "Source: three million nine hundred and twenty one thousand six hundred and thirty three\n",
            "Prediction:   \n",
            "Ground truth: 3 9 2 1 6 3 3\n",
            "Source: ninety nine million four hundred and eighty five thousand seven hundred and ninety seven\n",
            "Prediction:   \n",
            "Ground truth: 9 9 4 8 5 7 9 7\n",
            "Source: five million six hundred and twenty nine thousand six hundred and sixty five\n",
            "Prediction:   \n",
            "Ground truth: 5 6 2 9 6 6 5\n",
            "Source: two thousand six hundred and fifty three\n",
            "Prediction:   \n",
            "Ground truth: 2 6 5 3\n",
            "Source: three hundred and thirty thousand seven hundred and sixty four\n",
            "Prediction:   \n",
            "Ground truth: 3 3 0 7 6 4\n",
            "Source: seven billion one hundred and ninety one million nine hundred and forty seven thousand one hundred and forty two\n",
            "Prediction:   \n",
            "Ground truth: 7 1 9 1 9 4 7 1 4 2\n",
            "Source: seven hundred and sixty two thousand six hundred and eighty eight\n",
            "Prediction:   \n",
            "Ground truth: 7 6 2 6 8 8\n",
            "Source: thirty seven thousand five hundred and sixty nine\n",
            "Prediction:   \n",
            "Ground truth: 3 7 5 6 9\n",
            "Source: nine hundred and sixty one million five hundred and forty seven thousand seven hundred and sixty one\n",
            "Prediction:   \n",
            "Ground truth: 9 6 1 5 4 7 7 6 1\n",
            "Source: thirteen thousand six hundred and seventy six\n",
            "Prediction:   \n",
            "Ground truth: 1 3 6 7 6\n",
            "Source: fifty five million nine hundred and sixty nine thousand eight hundred and twenty one\n",
            "Prediction:   \n",
            "Ground truth: 5 5 9 6 9 8 2 1\n",
            "Source: two hundred and twenty nine million four hundred and thirty four thousand and seven\n",
            "Prediction:   \n",
            "Ground truth: 2 2 9 4 3 4 0 0 7\n",
            "Source: eighty six million one hundred and eighty thousand five hundred and ninety seven\n",
            "Prediction:   \n",
            "Ground truth: 8 6 1 8 0 5 9 7\n",
            "Source: seventy million forty nine thousand five hundred and thirty\n",
            "Prediction:   \n",
            "Ground truth: 7 0 0 4 9 5 3 0\n",
            "Source: two billion five hundred and ninety four million five hundred and eighty thousand and sixty\n",
            "Prediction:   \n",
            "Ground truth: 2 5 9 4 5 8 0 0 6 0\n",
            "Source: three billion five hundred and thirty nine million three hundred and fifty eight thousand six hundred and forty one\n",
            "Prediction:   \n",
            "Ground truth: 3 5 3 9 3 5 8 6 4 1\n",
            "Source: twelve million eight hundred and sixty six thousand five hundred and seventy one\n",
            "Prediction:   \n",
            "Ground truth: 1 2 8 6 6 5 7 1\n",
            "Source: forty three thousand eight hundred and twenty seven\n",
            "Prediction:   \n",
            "Ground truth: 4 3 8 2 7\n",
            "Source: six million forty four thousand seven hundred and twenty\n",
            "Prediction:   \n",
            "Ground truth: 6 0 4 4 7 2 0\n",
            "Source: five thousand six hundred and thirty three\n",
            "Prediction:   \n",
            "Ground truth: 5 6 3 3\n",
            "Source: nine thousand and forty six\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 0 4 6\n",
            "Source: fifty nine million three hundred and eighty thousand three hundred and eighty two\n",
            "Prediction:   \n",
            "Ground truth: 5 9 3 8 0 3 8 2\n",
            "Source: one billion eleven million three hundred and twenty five thousand three hundred and sixty three\n",
            "Prediction:   \n",
            "Ground truth: 1 0 1 1 3 2 5 3 6 3\n",
            "Source: four thousand nine hundred and seventy nine\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 9 7 9\n",
            "Source: seventy one million one hundred and sixty two thousand seven hundred and fifty nine\n",
            "Prediction:   \n",
            "Ground truth: 7 1 1 6 2 7 5 9\n",
            "Source: two thousand six hundred and twenty one\n",
            "Prediction:   \n",
            "Ground truth: 2 6 2 1\n",
            "Source: five million seven hundred and forty thousand three hundred and fifty three\n",
            "Prediction:   \n",
            "Ground truth: 5 7 4 0 3 5 3\n",
            "Source: five hundred and twenty two thousand six hundred\n",
            "Prediction:   \n",
            "Ground truth: 5 2 2 6 0 0\n",
            "Source: nine billion four hundred and thirty nine million forty three thousand eight hundred and eighty eight\n",
            "Prediction:   9 9 9 9\n",
            "Ground truth: 9 4 3 9 0 4 3 8 8 8\n",
            "Source: thirty seven million five hundred and twenty nine thousand four hundred and sixty\n",
            "Prediction:   \n",
            "Ground truth: 3 7 5 2 9 4 6 0\n",
            "Source: seventy eight million seven thousand two hundred and sixty five\n",
            "Prediction:   \n",
            "Ground truth: 7 8 0 0 7 2 6 5\n",
            "Source: sixty one million two hundred and four thousand three hundred and thirty five\n",
            "Prediction:   \n",
            "Ground truth: 6 1 2 0 4 3 3 5\n",
            "Source: seven million three hundred and forty nine thousand six hundred and ninety six\n",
            "Prediction:   \n",
            "Ground truth: 7 3 4 9 6 9 6\n",
            "Source: three billion two hundred and seventy six million three hundred and sixty one thousand six hundred and twenty two\n",
            "Prediction:   \n",
            "Ground truth: 3 2 7 6 3 6 1 6 2 2\n",
            "Source: six hundred and forty seven million one hundred and eighty eight thousand and twenty nine\n",
            "Prediction:   \n",
            "Ground truth: 6 4 7 1 8 8 0 2 9\n",
            "Source: four billion forty five million four hundred and eleven thousand three hundred\n",
            "Prediction:   4 4 4 4\n",
            "Ground truth: 4 0 4 5 4 1 1 3 0 0\n",
            "Source: eight thousand one hundred and forty two\n",
            "Prediction:   \n",
            "Ground truth: 8 1 4 2\n",
            "Source: five billion six hundred and nine million four hundred and thirteen thousand nine hundred and ninety two\n",
            "Prediction:   \n",
            "Ground truth: 5 6 0 9 4 1 3 9 9 2\n",
            "Source: one hundred and fifteen million seven hundred and twenty eight thousand nine hundred and sixteen\n",
            "Prediction:   \n",
            "Ground truth: 1 1 5 7 2 8 9 1 6\n",
            "Source: two hundred and twenty one thousand six hundred and forty five\n",
            "Prediction:   \n",
            "Ground truth: 2 2 1 6 4 5\n",
            "Source: seven thousand two hundred and four\n",
            "Prediction:   \n",
            "Ground truth: 7 2 0 4\n",
            "Source: six thousand nine hundred and thirty nine\n",
            "Prediction:   \n",
            "Ground truth: 6 9 3 9\n",
            "Source: three billion four hundred and eighty six million nine hundred and eighteen thousand seven hundred and eleven\n",
            "Prediction:   \n",
            "Ground truth: 3 4 8 6 9 1 8 7 1 1\n",
            "Source: seven billion one hundred and eighty eight million one hundred and twelve thousand and seventy eight\n",
            "Prediction:   \n",
            "Ground truth: 7 1 8 8 1 1 2 0 7 8\n",
            "Source: six million three hundred and eighty nine thousand and sixty two\n",
            "Prediction:   \n",
            "Ground truth: 6 3 8 9 0 6 2\n",
            "Source: two hundred and six million five hundred and forty seven thousand seven hundred and eighty six\n",
            "Prediction:   \n",
            "Ground truth: 2 0 6 5 4 7 7 8 6\n",
            "Source: four million two hundred and eighty four thousand nine hundred and forty eight\n",
            "Prediction:   4 4 4 4\n",
            "Ground truth: 4 2 8 4 9 4 8\n",
            "Source: eight thousand eight hundred and fifty two\n",
            "Prediction:   \n",
            "Ground truth: 8 8 5 2\n",
            "Source: ninety four thousand and thirty six\n",
            "Prediction:   9 9 6\n",
            "Ground truth: 9 4 0 3 6\n",
            "Source: twenty one thousand one hundred and twenty six\n",
            "Prediction:   \n",
            "Ground truth: 2 1 1 2 6\n",
            "Source: twelve thousand one hundred\n",
            "Prediction:   \n",
            "Ground truth: 1 2 1 0 0\n",
            "Source: five million three hundred and thirty five thousand one hundred and thirty eight\n",
            "Prediction:   \n",
            "Ground truth: 5 3 3 5 1 3 8\n",
            "Source: four million three hundred and eighty eight thousand three hundred and nine\n",
            "Prediction:   4 4 4 4 4 4\n",
            "Ground truth: 4 3 8 8 3 0 9\n",
            "Source: one thousand and eighty eight\n",
            "Prediction:   \n",
            "Ground truth: 1 0 8 8\n",
            "Source: ninety one thousand seven hundred and thirty three\n",
            "Prediction:   9\n",
            "Ground truth: 9 1 7 3 3\n",
            "Source: two million four hundred and thirty six thousand five hundred and sixty nine\n",
            "Prediction:   \n",
            "Ground truth: 2 4 3 6 5 6 9\n",
            "Source: eight million five hundred and sixty three thousand one hundred and seventy one\n",
            "Prediction:   \n",
            "Ground truth: 8 5 6 3 1 7 1\n",
            "Source: three hundred and eighteen thousand and eighty\n",
            "Prediction:   \n",
            "Ground truth: 3 1 8 0 8 0\n",
            "Source: three million nine hundred and sixty three thousand one hundred and thirty two\n",
            "Prediction:   \n",
            "Ground truth: 3 9 6 3 1 3 2\n",
            "Source: six million six hundred and seventy thousand six hundred and sixty\n",
            "Prediction:   \n",
            "Ground truth: 6 6 7 0 6 6 0\n",
            "Source: twenty eight million one hundred and thirty two thousand three hundred and ninety two\n",
            "Prediction:   \n",
            "Ground truth: 2 8 1 3 2 3 9 2\n",
            "Source: fifteen million nine hundred and eighty seven thousand four hundred and fifty nine\n",
            "Prediction:   \n",
            "Ground truth: 1 5 9 8 7 4 5 9\n",
            "Source: five hundred and forty four thousand five hundred\n",
            "Prediction:   \n",
            "Ground truth: 5 4 4 5 0 0\n",
            "Source: nine thousand three hundred and seventy six\n",
            "Prediction:   9 9 6\n",
            "Ground truth: 9 3 7 6\n",
            "Source: seventy six thousand nine hundred and twenty eight\n",
            "Prediction:   \n",
            "Ground truth: 7 6 9 2 8\n",
            "Source: eight hundred and thirty three thousand and eighteen\n",
            "Prediction:   \n",
            "Ground truth: 8 3 3 0 1 8\n",
            "Source: six billion four hundred and forty four million eight hundred and thirty two thousand and eleven\n",
            "Prediction:   \n",
            "Ground truth: 6 4 4 4 8 3 2 0 1 1\n",
            "Source: twenty one thousand three hundred and thirty three\n",
            "Prediction:   \n",
            "Ground truth: 2 1 3 3 3\n",
            "Source: thirty thousand three hundred and eighty three\n",
            "Prediction:   \n",
            "Ground truth: 3 0 3 8 3\n",
            "Source: fifty million four hundred and ninety nine thousand and ninety four\n",
            "Prediction:   \n",
            "Ground truth: 5 0 4 9 9 0 9 4\n",
            "Source: twenty one thousand and twenty two\n",
            "Prediction:   \n",
            "Ground truth: 2 1 0 2 2\n",
            "Source: eight thousand and ninety two\n",
            "Prediction:   \n",
            "Ground truth: 8 0 9 2\n",
            "Source: six thousand nine hundred and twenty nine\n",
            "Prediction:   \n",
            "Ground truth: 6 9 2 9\n",
            "Source: forty five million five hundred and thirty nine thousand five hundred and thirty\n",
            "Prediction:   \n",
            "Ground truth: 4 5 5 3 9 5 3 0\n",
            "Source: eighty million eight hundred and sixty nine thousand and ninety two\n",
            "Prediction:   \n",
            "Ground truth: 8 0 8 6 9 0 9 2\n",
            "Source: eighty seven million one hundred and seventy three thousand two hundred and ninety two\n",
            "Prediction:   \n",
            "Ground truth: 8 7 1 7 3 2 9 2\n",
            "Source: fifty four thousand five hundred and fifty three\n",
            "Prediction:   \n",
            "Ground truth: 5 4 5 5 3\n",
            "Source: six thousand six hundred and eighty\n",
            "Prediction:   \n",
            "Ground truth: 6 6 8 0\n",
            "Source: forty five thousand seven hundred and sixty four\n",
            "Prediction:   \n",
            "Ground truth: 4 5 7 6 4\n",
            "Source: six hundred and eighty six million two hundred and three thousand four hundred and two\n",
            "Prediction:   \n",
            "Ground truth: 6 8 6 2 0 3 4 0 2\n",
            "Source: three hundred and fifty five million eight hundred and seventy nine thousand one hundred and twenty five\n",
            "Prediction:   \n",
            "Ground truth: 3 5 5 8 7 9 1 2 5\n",
            "Source: one billion eight hundred and four million five hundred and ninety nine thousand three hundred and sixty seven\n",
            "Prediction:   \n",
            "Ground truth: 1 8 0 4 5 9 9 3 6 7\n",
            "Source: six billion two hundred and fifty four million forty one thousand two hundred and seventy one\n",
            "Prediction:   \n",
            "Ground truth: 6 2 5 4 0 4 1 2 7 1\n",
            "Source: nine hundred and fifty three million six hundred and thirty six thousand eight hundred and eighty five\n",
            "Prediction:   \n",
            "Ground truth: 9 5 3 6 3 6 8 8 5\n",
            "Source: six hundred and eighty six thousand six hundred and seventy two\n",
            "Prediction:   \n",
            "Ground truth: 6 8 6 6 7 2\n",
            "Source: ninety eight million three hundred and seventy six thousand five hundred and thirty five\n",
            "Prediction:   \n",
            "Ground truth: 9 8 3 7 6 5 3 5\n",
            "Source: three thousand five hundred and ninety six\n",
            "Prediction:   \n",
            "Ground truth: 3 5 9 6\n",
            "Source: nine billion six hundred million one hundred and ninety two thousand four hundred and forty eight\n",
            "Prediction:   \n",
            "Ground truth: 9 6 0 0 1 9 2 4 4 8\n",
            "Source: six billion four hundred and nine million five hundred and sixty six thousand four hundred and eighty\n",
            "Prediction:   \n",
            "Ground truth: 6 4 0 9 5 6 6 4 8 0\n",
            "Source: one billion five hundred and sixty million nine hundred and twenty four thousand three hundred and twenty four\n",
            "Prediction:   \n",
            "Ground truth: 1 5 6 0 9 2 4 3 2 4\n",
            "Source: four thousand two hundred and forty\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 2 4 0\n",
            "Source: thirty seven thousand eight hundred and ninety eight\n",
            "Prediction:   \n",
            "Ground truth: 3 7 8 9 8\n",
            "Source: five billion two hundred and thirty six million seventy eight thousand one hundred and fifty five\n",
            "Prediction:   \n",
            "Ground truth: 5 2 3 6 0 7 8 1 5 5\n",
            "Source: ninety nine million two hundred and eighty one thousand four hundred and twenty three\n",
            "Prediction:   \n",
            "Ground truth: 9 9 2 8 1 4 2 3\n",
            "Source: eight hundred and forty six million two hundred and thirty three thousand six hundred and twenty eight\n",
            "Prediction:   \n",
            "Ground truth: 8 4 6 2 3 3 6 2 8\n",
            "Source: nine million six hundred and twenty two thousand three hundred and fourteen\n",
            "Prediction:   \n",
            "Ground truth: 9 6 2 2 3 1 4\n",
            "Source: four thousand one hundred and sixty three\n",
            "Prediction:   4 4\n",
            "Ground truth: 4 1 6 3\n",
            "Source: one hundred and twenty nine thousand six hundred and forty seven\n",
            "Prediction:   \n",
            "Ground truth: 1 2 9 6 4 7\n",
            "Source: twenty eight thousand nine hundred and ninety eight\n",
            "Prediction:   \n",
            "Ground truth: 2 8 9 9 8\n",
            "Source: nine thousand one hundred and ninety\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 1 9 0\n",
            "Source: nine thousand four hundred and eighty three\n",
            "Prediction:   9 9\n",
            "Ground truth: 9 4 8 3\n",
            "Source: three billion one hundred and sixty five million two hundred and fifty thousand four hundred and eighty eight\n",
            "Prediction:   \n",
            "Ground truth: 3 1 6 5 2 5 0 4 8 8\n",
            "Source: seven thousand three hundred and ninety four\n",
            "Prediction:   \n",
            "Ground truth: 7 3 9 4\n",
            "Source: fifty nine million seven hundred and six thousand two hundred and eighty five\n",
            "Prediction:   \n",
            "Ground truth: 5 9 7 0 6 2 8 5\n",
            "Source: two hundred and eighty nine million seven hundred and thirteen thousand and sixty eight\n",
            "Prediction:   \n",
            "Ground truth: 2 8 9 7 1 3 0 6 8\n",
            "Source: sixty one thousand seven hundred and forty seven\n",
            "Prediction:   \n",
            "Ground truth: 6 1 7 4 7\n",
            "Source: three hundred and sixty thousand four hundred\n",
            "Prediction:   \n",
            "Ground truth: 3 6 0 4 0 0\n",
            "Source: fifty seven thousand nine hundred and thirty seven\n",
            "Prediction:   \n",
            "Ground truth: 5 7 9 3 7\n",
            "Source: forty nine million nine hundred and ten thousand seven hundred and seventy eight\n",
            "Prediction:   \n",
            "Ground truth: 4 9 9 1 0 7 7 8\n",
            "Source: twelve million one hundred and sixty three thousand three hundred and seventy seven\n",
            "Prediction:   \n",
            "Ground truth: 1 2 1 6 3 3 7 7\n",
            "Source: six thousand six hundred and thirty three\n",
            "Prediction:   \n",
            "Ground truth: 6 6 3 3\n",
            "Source: six hundred and twenty nine thousand three hundred and sixteen\n",
            "Prediction:   \n",
            "Ground truth: 6 2 9 3 1 6\n",
            "Source: five thousand seven hundred and seventy three\n",
            "Prediction:   \n",
            "Ground truth: 5 7 7 3\n",
            "Source: eighty five million six hundred and three thousand five hundred and eighty two\n",
            "Prediction:   \n",
            "Ground truth: 8 5 6 0 3 5 8 2\n",
            "Source: eight million one hundred and twenty three thousand six hundred and forty one\n",
            "Prediction:   \n",
            "Ground truth: 8 1 2 3 6 4 1\n",
            "Source: seven hundred and fifty one million nine hundred and thirty nine thousand eight hundred and forty two\n",
            "Prediction:   \n",
            "Ground truth: 7 5 1 9 3 9 8 4 2\n",
            "Source: ninety two thousand and twenty nine\n",
            "Prediction:   9 9 9 9 9\n",
            "Ground truth: 9 2 0 2 9\n",
            "Source: fifty five thousand three hundred and twenty six\n",
            "Prediction:   \n",
            "Ground truth: 5 5 3 2 6\n",
            "Source: seven billion one hundred and thirty six million two hundred and seventy five thousand eight hundred and eighty seven\n",
            "Prediction:   \n",
            "Ground truth: 7 1 3 6 2 7 5 8 8 7\n",
            "Source: six hundred and eighteen million eight hundred and eighty one thousand seven hundred and eighty nine\n",
            "Prediction:   \n",
            "Ground truth: 6 1 8 8 8 1 7 8 9\n",
            "Source: sixty seven thousand four hundred and fifty seven\n",
            "Prediction:   \n",
            "Ground truth: 6 7 4 5 7\n",
            "Source: seven hundred and ninety one thousand eight hundred and twenty three\n",
            "Prediction:   \n",
            "Ground truth: 7 9 1 8 2 3\n",
            "Source: five hundred and seventy four million three hundred and seventy three thousand eight hundred and eighty eight\n",
            "Prediction:   \n",
            "Ground truth: 5 7 4 3 7 3 8 8 8\n",
            "Source: eight thousand seven hundred and seven\n",
            "Prediction:   \n",
            "Ground truth: 8 7 0 7\n",
            "Source: eighty six million five hundred and sixty nine thousand two hundred and eighty six\n",
            "Prediction:   \n",
            "Ground truth: 8 6 5 6 9 2 8 6\n",
            "Source: forty eight thousand two hundred and eighty four\n",
            "Prediction:   \n",
            "Ground truth: 4 8 2 8 4\n",
            "Source: eighty two thousand two hundred and sixty four\n",
            "Prediction:   \n",
            "Ground truth: 8 2 2 6 4\n",
            "Source: three thousand four hundred and forty one\n",
            "Prediction:   \n",
            "Ground truth: 3 4 4 1\n",
            "Source: five hundred and sixty two thousand two hundred and forty\n",
            "Prediction:   \n",
            "Ground truth: 5 6 2 2 4 0\n",
            "Source: two thousand and ninety three\n",
            "Prediction:   \n",
            "Ground truth: 2 0 9 3\n",
            "Source: seventy million six thousand four hundred and forty seven\n",
            "Prediction:   \n",
            "Ground truth: 7 0 0 0 6 4 4 7\n",
            "Source: seventy nine thousand and twenty two\n",
            "Prediction:   \n",
            "Ground truth: 7 9 0 2 2\n",
            "Source: eight hundred and sixty eight thousand three hundred and twenty one\n",
            "Prediction:   \n",
            "Ground truth: 8 6 8 3 2 1\n",
            "Source: seven hundred and thirty three million six hundred and eight thousand five hundred and two\n",
            "Prediction:   \n",
            "Ground truth: 7 3 3 6 0 8 5 0 2\n",
            "Source: four hundred and fifty three thousand six hundred and forty six\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 5 3 6 4 6\n",
            "Source: eighty three million five hundred and five thousand nine hundred and eighty three\n",
            "Prediction:   \n",
            "Ground truth: 8 3 5 0 5 9 8 3\n",
            "Source: three hundred and seventy five million nine hundred and forty four thousand two hundred and thirty\n",
            "Prediction:   \n",
            "Ground truth: 3 7 5 9 4 4 2 3 0\n",
            "Source: twenty three million three hundred and twenty thousand seven hundred and forty five\n",
            "Prediction:   \n",
            "Ground truth: 2 3 3 2 0 7 4 5\n",
            "Source: six thousand three hundred and fifty one\n",
            "Prediction:   \n",
            "Ground truth: 6 3 5 1\n",
            "Source: nine hundred and thirty thousand three hundred and ninety seven\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 3 0 3 9 7\n",
            "Source: six hundred and fifty two thousand three hundred and forty\n",
            "Prediction:   \n",
            "Ground truth: 6 5 2 3 4 0\n",
            "Source: six million two hundred and thirty five thousand eight hundred and forty four\n",
            "Prediction:   \n",
            "Ground truth: 6 2 3 5 8 4 4\n",
            "Source: five billion nine hundred and eighty five million two hundred and five thousand four hundred and eighty seven\n",
            "Prediction:   \n",
            "Ground truth: 5 9 8 5 2 0 5 4 8 7\n",
            "Source: seven billion three hundred and fifty seven million five hundred and one thousand eight hundred and twenty\n",
            "Prediction:   \n",
            "Ground truth: 7 3 5 7 5 0 1 8 2 0\n",
            "Source: three million one hundred and ten thousand eight hundred and seventy\n",
            "Prediction:   \n",
            "Ground truth: 3 1 1 0 8 7 0\n",
            "Source: four million three hundred and eighty five thousand one hundred and fifty one\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 3 8 5 1 5 1\n",
            "Source: eight hundred and two thousand two hundred and nine\n",
            "Prediction:   \n",
            "Ground truth: 8 0 2 2 0 9\n",
            "Source: one million four hundred and thirty one thousand eight hundred and sixty three\n",
            "Prediction:   \n",
            "Ground truth: 1 4 3 1 8 6 3\n",
            "Source: forty four million two hundred and two thousand five hundred and sixty eight\n",
            "Prediction:   \n",
            "Ground truth: 4 4 2 0 2 5 6 8\n",
            "Source: eight million nine hundred and twenty one thousand seven hundred and thirty one\n",
            "Prediction:   \n",
            "Ground truth: 8 9 2 1 7 3 1\n",
            "Source: forty five million four hundred and twenty six thousand one hundred and fifty\n",
            "Prediction:   \n",
            "Ground truth: 4 5 4 2 6 1 5 0\n",
            "Source: forty two million five hundred and sixty nine thousand five hundred and eighty two\n",
            "Prediction:   \n",
            "Ground truth: 4 2 5 6 9 5 8 2\n",
            "Source: five hundred and sixty eight million eighty three thousand four hundred and thirty nine\n",
            "Prediction:   \n",
            "Ground truth: 5 6 8 0 8 3 4 3 9\n",
            "Source: sixteen thousand six hundred and thirteen\n",
            "Prediction:   \n",
            "Ground truth: 1 6 6 1 3\n",
            "Source: eighty million seventy thousand six hundred and thirty one\n",
            "Prediction:   \n",
            "Ground truth: 8 0 0 7 0 6 3 1\n",
            "Source: forty seven million four hundred and ninety nine thousand two hundred and forty\n",
            "Prediction:   \n",
            "Ground truth: 4 7 4 9 9 2 4 0\n",
            "Source: four billion eight hundred and seventy one million five hundred and twenty three thousand and forty five\n",
            "Prediction:   4 4 4 4 4 4 4 4 4\n",
            "Ground truth: 4 8 7 1 5 2 3 0 4 5\n",
            "Source: eight million six hundred and sixty four thousand seven hundred and thirty eight\n",
            "Prediction:   \n",
            "Ground truth: 8 6 6 4 7 3 8\n",
            "Source: one million six hundred and eighty nine thousand seven hundred and seventy seven\n",
            "Prediction:   \n",
            "Ground truth: 1 6 8 9 7 7 7\n",
            "Source: two hundred and fifty five million nine hundred and forty six thousand six hundred and seventy four\n",
            "Prediction:   \n",
            "Ground truth: 2 5 5 9 4 6 6 7 4\n",
            "Source: nine hundred and eleven thousand one hundred and twenty nine\n",
            "Prediction:   9\n",
            "Ground truth: 9 1 1 1 2 9\n",
            "Source: seventy nine thousand six hundred and thirty\n",
            "Prediction:   \n",
            "Ground truth: 7 9 6 3 0\n",
            "Source: seventy three thousand four hundred and seventy two\n",
            "Prediction:   \n",
            "Ground truth: 7 3 4 7 2\n",
            "Source: six million eight hundred and seventy five thousand one hundred and sixty six\n",
            "Prediction:   \n",
            "Ground truth: 6 8 7 5 1 6 6\n",
            "Source: six billion two hundred and twelve million twenty nine thousand nine hundred and forty three\n",
            "Prediction:   \n",
            "Ground truth: 6 2 1 2 0 2 9 9 4 3\n",
            "Source: fifty five million four hundred and eighty three thousand one hundred and three\n",
            "Prediction:   \n",
            "Ground truth: 5 5 4 8 3 1 0 3\n",
            "Source: thirty five million nine hundred and seventy four thousand six hundred and three\n",
            "Prediction:   \n",
            "Ground truth: 3 5 9 7 4 6 0 3\n",
            "Source: nine hundred and twenty eight thousand five hundred and fifty four\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 2 8 5 5 4\n",
            "Source: twenty four thousand three hundred and ninety nine\n",
            "Prediction:   \n",
            "Ground truth: 2 4 3 9 9\n",
            "Source: seven hundred and fifty six thousand eight hundred and sixty eight\n",
            "Prediction:   \n",
            "Ground truth: 7 5 6 8 6 8\n",
            "Source: seven billion seven hundred and four million nine hundred and forty nine thousand and eighty three\n",
            "Prediction:   \n",
            "Ground truth: 7 7 0 4 9 4 9 0 8 3\n",
            "Source: nine million eight hundred and ninety three thousand nine hundred and forty\n",
            "Prediction:   \n",
            "Ground truth: 9 8 9 3 9 4 0\n",
            "Source: seven billion seven hundred and thirty nine million two hundred and five thousand and three\n",
            "Prediction:   \n",
            "Ground truth: 7 7 3 9 2 0 5 0 0 3\n",
            "Source: nine hundred and forty thousand eight hundred and ninety three\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 4 0 8 9 3\n",
            "Source: one hundred and twenty eight thousand three hundred and thirty one\n",
            "Prediction:   \n",
            "Ground truth: 1 2 8 3 3 1\n",
            "Source: thirty three million thirty six thousand four hundred and ninety two\n",
            "Prediction:   \n",
            "Ground truth: 3 3 0 3 6 4 9 2\n",
            "Source: seven hundred and sixty one thousand four hundred and twenty nine\n",
            "Prediction:   \n",
            "Ground truth: 7 6 1 4 2 9\n",
            "Source: seventy four million two hundred and twenty seven thousand and fifty one\n",
            "Prediction:   \n",
            "Ground truth: 7 4 2 2 7 0 5 1\n",
            "Source: seven hundred thousand eight hundred and seven\n",
            "Prediction:   \n",
            "Ground truth: 7 0 0 8 0 7\n",
            "Source: twenty one thousand five hundred and ninety\n",
            "Prediction:   \n",
            "Ground truth: 2 1 5 9 0\n",
            "Source: seventy four million forty seven thousand five hundred and five\n",
            "Prediction:   \n",
            "Ground truth: 7 4 0 4 7 5 0 5\n",
            "Source: seven hundred and forty two thousand five hundred and seventy seven\n",
            "Prediction:   \n",
            "Ground truth: 7 4 2 5 7 7\n",
            "Source: eight hundred and twenty six million five hundred and ninety three thousand two hundred and seventy\n",
            "Prediction:   \n",
            "Ground truth: 8 2 6 5 9 3 2 7 0\n",
            "Source: five thousand nine hundred and ninety four\n",
            "Prediction:   \n",
            "Ground truth: 5 9 9 4\n",
            "Source: twenty seven thousand six hundred and nine\n",
            "Prediction:   \n",
            "Ground truth: 2 7 6 0 9\n",
            "Source: one million two hundred and twenty thousand eight hundred and twenty six\n",
            "Prediction:   \n",
            "Ground truth: 1 2 2 0 8 2 6\n",
            "Source: two thousand nine hundred and ninety\n",
            "Prediction:   \n",
            "Ground truth: 2 9 9 0\n",
            "Source: three hundred million two hundred and seventy four thousand nine hundred and twenty\n",
            "Prediction:   \n",
            "Ground truth: 3 0 0 2 7 4 9 2 0\n",
            "Source: one million seven hundred and sixty thousand nine hundred and nineteen\n",
            "Prediction:   \n",
            "Ground truth: 1 7 6 0 9 1 9\n",
            "Source: two hundred and sixty two thousand nine hundred and forty eight\n",
            "Prediction:   \n",
            "Ground truth: 2 6 2 9 4 8\n",
            "Source: four thousand six hundred and seventy three\n",
            "Prediction:   4 4\n",
            "Ground truth: 4 6 7 3\n",
            "Source: six hundred and fifty one million eight hundred and fifty thousand nine hundred and eighty one\n",
            "Prediction:   \n",
            "Ground truth: 6 5 1 8 5 0 9 8 1\n",
            "Source: ten thousand two hundred and ninety five\n",
            "Prediction:   \n",
            "Ground truth: 1 0 2 9 5\n",
            "Source: five million seven hundred and eighteen thousand three hundred and seventy two\n",
            "Prediction:   \n",
            "Ground truth: 5 7 1 8 3 7 2\n",
            "Source: forty one million eight hundred and twenty eight thousand five hundred and eighty three\n",
            "Prediction:   \n",
            "Ground truth: 4 1 8 2 8 5 8 3\n",
            "Source: nine hundred and ninety five thousand six hundred and sixty six\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 9 5 6 6 6\n",
            "Source: two billion seven hundred and seventy million nine hundred and twenty six thousand six hundred\n",
            "Prediction:   \n",
            "Ground truth: 2 7 7 0 9 2 6 6 0 0\n",
            "Source: eight hundred and twenty million two hundred and twelve thousand and ten\n",
            "Prediction:   \n",
            "Ground truth: 8 2 0 2 1 2 0 1 0\n",
            "Source: three million four hundred and eighty six thousand two hundred and sixty eight\n",
            "Prediction:   \n",
            "Ground truth: 3 4 8 6 2 6 8\n",
            "Source: five thousand four hundred and one\n",
            "Prediction:   \n",
            "Ground truth: 5 4 0 1\n",
            "Source: sixty three million five hundred and fifteen thousand one hundred and twenty five\n",
            "Prediction:   \n",
            "Ground truth: 6 3 5 1 5 1 2 5\n",
            "Source: one billion five hundred and ninety two million eight hundred and two thousand seven hundred and sixty one\n",
            "Prediction:   \n",
            "Ground truth: 1 5 9 2 8 0 2 7 6 1\n",
            "Source: six hundred and fifty two million eight hundred and forty eight thousand three hundred and twenty four\n",
            "Prediction:   \n",
            "Ground truth: 6 5 2 8 4 8 3 2 4\n",
            "Source: fifty million three hundred and fifty seven thousand two hundred and seventy two\n",
            "Prediction:   \n",
            "Ground truth: 5 0 3 5 7 2 7 2\n",
            "Source: one hundred and six million four hundred and seventy eight thousand three hundred and seventy seven\n",
            "Prediction:   \n",
            "Ground truth: 1 0 6 4 7 8 3 7 7\n",
            "Source: nine hundred and fifty three thousand two hundred\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 5 3 2 0 0\n",
            "Source: six million five hundred and seventy thousand seven hundred and eighty five\n",
            "Prediction:   \n",
            "Ground truth: 6 5 7 0 7 8 5\n",
            "Source: seven thousand eight hundred and one\n",
            "Prediction:   \n",
            "Ground truth: 7 8 0 1\n",
            "Source: eight million five hundred and fifty five thousand nine hundred and ninety three\n",
            "Prediction:   \n",
            "Ground truth: 8 5 5 5 9 9 3\n",
            "Source: sixty nine million fifty three thousand three hundred and five\n",
            "Prediction:   \n",
            "Ground truth: 6 9 0 5 3 3 0 5\n",
            "Source: seven hundred and eighty six thousand five hundred and eleven\n",
            "Prediction:   \n",
            "Ground truth: 7 8 6 5 1 1\n",
            "Source: four billion eight hundred and sixty eight million seven hundred and twenty one thousand four hundred and sixty six\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 8 6 8 7 2 1 4 6 6\n",
            "Source: eight thousand three hundred and forty seven\n",
            "Prediction:   \n",
            "Ground truth: 8 3 4 7\n",
            "Source: one thousand two hundred and nine\n",
            "Prediction:   \n",
            "Ground truth: 1 2 0 9\n",
            "Source: one billion four hundred and thirty seven million five hundred and seventy eight thousand one hundred and seventy eight\n",
            "Prediction:   \n",
            "Ground truth: 1 4 3 7 5 7 8 1 7 8\n",
            "Source: eighty six thousand eight hundred and forty four\n",
            "Prediction:   \n",
            "Ground truth: 8 6 8 4 4\n",
            "Source: six hundred and sixty five million nine hundred and thirty eight thousand nine hundred and forty four\n",
            "Prediction:   \n",
            "Ground truth: 6 6 5 9 3 8 9 4 4\n",
            "Source: one hundred and seven thousand six hundred and sixty seven\n",
            "Prediction:   \n",
            "Ground truth: 1 0 7 6 6 7\n",
            "Source: six thousand one hundred and twenty eight\n",
            "Prediction:   \n",
            "Ground truth: 6 1 2 8\n",
            "Source: ninety four thousand five hundred and seventy six\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 4 5 7 6\n",
            "Source: five thousand four hundred and twenty seven\n",
            "Prediction:   \n",
            "Ground truth: 5 4 2 7\n",
            "Source: ninety two thousand eight hundred and thirty nine\n",
            "Prediction:   \n",
            "Ground truth: 9 2 8 3 9\n",
            "Source: four hundred and thirty seven million four hundred and seventy one thousand one hundred and forty seven\n",
            "Prediction:   4 4 4 4\n",
            "Ground truth: 4 3 7 4 7 1 1 4 7\n",
            "Source: two hundred and sixty nine thousand and forty\n",
            "Prediction:   \n",
            "Ground truth: 2 6 9 0 4 0\n",
            "Source: two million eighty six thousand nine hundred and twenty six\n",
            "Prediction:   \n",
            "Ground truth: 2 0 8 6 9 2 6\n",
            "Source: five hundred and seventeen million two hundred and fifty eight thousand and eighty one\n",
            "Prediction:   \n",
            "Ground truth: 5 1 7 2 5 8 0 8 1\n",
            "Source: nine hundred and seventeen million one hundred and sixty seven thousand and forty nine\n",
            "Prediction:   \n",
            "Ground truth: 9 1 7 1 6 7 0 4 9\n",
            "Source: six billion one hundred and forty two million two hundred and fifty three thousand two hundred and five\n",
            "Prediction:   \n",
            "Ground truth: 6 1 4 2 2 5 3 2 0 5\n",
            "Source: nine million one hundred and six thousand seven hundred and thirty one\n",
            "Prediction:   \n",
            "Ground truth: 9 1 0 6 7 3 1\n",
            "Source: twenty five million one hundred and forty seven thousand two hundred and sixty three\n",
            "Prediction:   \n",
            "Ground truth: 2 5 1 4 7 2 6 3\n",
            "Source: sixty seven thousand and fourteen\n",
            "Prediction:   \n",
            "Ground truth: 6 7 0 1 4\n",
            "Source: eighty one thousand eight hundred and fourteen\n",
            "Prediction:   \n",
            "Ground truth: 8 1 8 1 4\n",
            "Source: seventy nine million seven hundred and thirty three thousand three hundred and eighty five\n",
            "Prediction:   \n",
            "Ground truth: 7 9 7 3 3 3 8 5\n",
            "Source: two hundred and thirteen million nine hundred and sixteen thousand six hundred and sixty eight\n",
            "Prediction:   \n",
            "Ground truth: 2 1 3 9 1 6 6 6 8\n",
            "Source: thirty eight thousand three hundred and ten\n",
            "Prediction:   \n",
            "Ground truth: 3 8 3 1 0\n",
            "Source: two hundred and seventy four million eighteen thousand five hundred and fifteen\n",
            "Prediction:   \n",
            "Ground truth: 2 7 4 0 1 8 5 1 5\n",
            "Source: five hundred and fifty three thousand two hundred and thirty nine\n",
            "Prediction:   \n",
            "Ground truth: 5 5 3 2 3 9\n",
            "Source: one billion eight hundred and fifty eight million four hundred and twenty five thousand five hundred and eighteen\n",
            "Prediction:   \n",
            "Ground truth: 1 8 5 8 4 2 5 5 1 8\n",
            "Source: four billion eight hundred and sixty six million one hundred and seventy five thousand one hundred and twenty six\n",
            "Prediction:   4 4 4 4\n",
            "Ground truth: 4 8 6 6 1 7 5 1 2 6\n",
            "Source: eight billion seven hundred and fifty million fifty four thousand five hundred\n",
            "Prediction:   \n",
            "Ground truth: 8 7 5 0 0 5 4 5 0 0\n",
            "Source: three hundred and thirty one thousand nine hundred and eighty eight\n",
            "Prediction:   \n",
            "Ground truth: 3 3 1 9 8 8\n",
            "Source: six hundred and seventy two million three hundred and twenty four thousand seven hundred and fourteen\n",
            "Prediction:   \n",
            "Ground truth: 6 7 2 3 2 4 7 1 4\n",
            "Source: three thousand five hundred and eleven\n",
            "Prediction:   \n",
            "Ground truth: 3 5 1 1\n",
            "Source: five hundred and seven million five hundred and eighty five thousand five hundred and sixty one\n",
            "Prediction:   \n",
            "Ground truth: 5 0 7 5 8 5 5 6 1\n",
            "Source: one hundred and eighty nine million eight hundred and thirty three thousand nine hundred and seventy seven\n",
            "Prediction:   \n",
            "Ground truth: 1 8 9 8 3 3 9 7 7\n",
            "Source: seven million five hundred and fifty five thousand eight hundred and fifty four\n",
            "Prediction:   \n",
            "Ground truth: 7 5 5 5 8 5 4\n",
            "Source: five billion nine hundred and forty five million one hundred and twenty one thousand two hundred and twenty three\n",
            "Prediction:   \n",
            "Ground truth: 5 9 4 5 1 2 1 2 2 3\n",
            "Source: four thousand and seventy one\n",
            "Prediction:   4 7 1\n",
            "Ground truth: 4 0 7 1\n",
            "Source: two hundred and eleven million six hundred and sixty two thousand five hundred and eighty six\n",
            "Prediction:   \n",
            "Ground truth: 2 1 1 6 6 2 5 8 6\n",
            "Source: seven hundred and seven million nine hundred and sixty six thousand eight hundred and thirteen\n",
            "Prediction:   \n",
            "Ground truth: 7 0 7 9 6 6 8 1 3\n",
            "Source: eighty nine thousand two hundred and seventeen\n",
            "Prediction:   \n",
            "Ground truth: 8 9 2 1 7\n",
            "Source: six hundred and thirteen thousand two hundred and sixty\n",
            "Prediction:   \n",
            "Ground truth: 6 1 3 2 6 0\n",
            "Source: thirty two million two hundred and thirty thousand four hundred and twenty seven\n",
            "Prediction:   \n",
            "Ground truth: 3 2 2 3 0 4 2 7\n",
            "Source: three billion eight hundred and sixty four million six hundred and twenty nine thousand and ninety\n",
            "Prediction:   \n",
            "Ground truth: 3 8 6 4 6 2 9 0 9 0\n",
            "Source: nine thousand four hundred and eighty\n",
            "Prediction:   9 8 8\n",
            "Ground truth: 9 4 8 0\n",
            "Source: nine billion sixty four million seven hundred and four thousand nine hundred and one\n",
            "Prediction:   \n",
            "Ground truth: 9 0 6 4 7 0 4 9 0 1\n",
            "Source: nine million five hundred and sixty one thousand and forty\n",
            "Prediction:   \n",
            "Ground truth: 9 5 6 1 0 4 0\n",
            "Source: one thousand seven hundred and twenty seven\n",
            "Prediction:   \n",
            "Ground truth: 1 7 2 7\n",
            "Source: forty one million four hundred and twenty eight thousand four hundred and thirty\n",
            "Prediction:   \n",
            "Ground truth: 4 1 4 2 8 4 3 0\n",
            "Source: seventy seven thousand and eighteen\n",
            "Prediction:   \n",
            "Ground truth: 7 7 0 1 8\n",
            "Source: seven hundred and twenty five thousand four hundred and fifty two\n",
            "Prediction:   \n",
            "Ground truth: 7 2 5 4 5 2\n",
            "Source: seven billion nine hundred and eighty nine million eight hundred and fifty six thousand one hundred and five\n",
            "Prediction:   \n",
            "Ground truth: 7 9 8 9 8 5 6 1 0 5\n",
            "Source: ninety one million four hundred and twenty eight thousand four hundred and ninety one\n",
            "Prediction:   \n",
            "Ground truth: 9 1 4 2 8 4 9 1\n",
            "Source: forty nine million six hundred and ninety eight thousand eight hundred and seven\n",
            "Prediction:   \n",
            "Ground truth: 4 9 6 9 8 8 0 7\n",
            "Source: eighty six million three hundred and seventy one thousand three hundred and twenty seven\n",
            "Prediction:   \n",
            "Ground truth: 8 6 3 7 1 3 2 7\n",
            "Source: twenty eight million five hundred and thirty seven thousand seven hundred and three\n",
            "Prediction:   \n",
            "Ground truth: 2 8 5 3 7 7 0 3\n",
            "Source: fifty thousand four hundred and fifty six\n",
            "Prediction:   \n",
            "Ground truth: 5 0 4 5 6\n",
            "Source: four hundred and seventeen thousand three hundred and thirty six\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 1 7 3 3 6\n",
            "Source: two thousand two hundred and seventy three\n",
            "Prediction:   \n",
            "Ground truth: 2 2 7 3\n",
            "Source: seven hundred and seventeen thousand seven hundred and fifteen\n",
            "Prediction:   \n",
            "Ground truth: 7 1 7 7 1 5\n",
            "Source: thirty five thousand four hundred and fifty five\n",
            "Prediction:   \n",
            "Ground truth: 3 5 4 5 5\n",
            "Source: two billion one hundred and twenty eight million five hundred and sixty seven thousand three hundred and seventy nine\n",
            "Prediction:   \n",
            "Ground truth: 2 1 2 8 5 6 7 3 7 9\n",
            "Source: one million four hundred and twenty three thousand one hundred and seventy two\n",
            "Prediction:   \n",
            "Ground truth: 1 4 2 3 1 7 2\n",
            "Source: four billion nine hundred and twenty five million four hundred and fifty nine thousand three hundred and twenty six\n",
            "Prediction:   4 4 4 4 4\n",
            "Ground truth: 4 9 2 5 4 5 9 3 2 6\n",
            "Source: eighty million ninety two thousand and forty seven\n",
            "Prediction:   \n",
            "Ground truth: 8 0 0 9 2 0 4 7\n",
            "Source: five hundred and ninety nine million eight hundred and forty six thousand three hundred and thirty eight\n",
            "Prediction:   \n",
            "Ground truth: 5 9 9 8 4 6 3 3 8\n",
            "Source: two hundred and twenty six million two hundred and ninety seven thousand five hundred and twenty five\n",
            "Prediction:   \n",
            "Ground truth: 2 2 6 2 9 7 5 2 5\n",
            "Source: four hundred and ninety one thousand eight hundred and thirty eight\n",
            "Prediction:   4 1 1 8\n",
            "Ground truth: 4 9 1 8 3 8\n",
            "Source: two thousand seven hundred and thirty one\n",
            "Prediction:   \n",
            "Ground truth: 2 7 3 1\n",
            "Source: one hundred and eighty four million eight hundred and one thousand seven hundred and fifty one\n",
            "Prediction:   \n",
            "Ground truth: 1 8 4 8 0 1 7 5 1\n",
            "Source: one billion five hundred and thirteen million two hundred and eighty six thousand four hundred and sixty eight\n",
            "Prediction:   \n",
            "Ground truth: 1 5 1 3 2 8 6 4 6 8\n",
            "Source: six hundred and eighty million two hundred and forty two thousand eight hundred and forty nine\n",
            "Prediction:   \n",
            "Ground truth: 6 8 0 2 4 2 8 4 9\n",
            "Source: nine thousand four hundred and fifteen\n",
            "Prediction:   9 5 5\n",
            "Ground truth: 9 4 1 5\n",
            "Source: one million two hundred and forty two thousand eight hundred and ninety six\n",
            "Prediction:   \n",
            "Ground truth: 1 2 4 2 8 9 6\n",
            "Source: forty eight million three hundred and ninety three thousand and thirty two\n",
            "Prediction:   \n",
            "Ground truth: 4 8 3 9 3 0 3 2\n",
            "Source: sixty thousand eight hundred and fifty\n",
            "Prediction:   \n",
            "Ground truth: 6 0 8 5 0\n",
            "Source: ninety nine thousand six hundred and forty eight\n",
            "Prediction:   \n",
            "Ground truth: 9 9 6 4 8\n",
            "Source: six billion seventy six million two hundred and eighteen thousand and ninety\n",
            "Prediction:   \n",
            "Ground truth: 6 0 7 6 2 1 8 0 9 0\n",
            "Source: one billion nine hundred and twenty four million seven hundred and eighteen thousand four hundred and eighty three\n",
            "Prediction:   \n",
            "Ground truth: 1 9 2 4 7 1 8 4 8 3\n",
            "Source: nineteen thousand two hundred and sixteen\n",
            "Prediction:   \n",
            "Ground truth: 1 9 2 1 6\n",
            "Source: three hundred and eighty one million nine hundred and seventy nine thousand five hundred and ninety two\n",
            "Prediction:   \n",
            "Ground truth: 3 8 1 9 7 9 5 9 2\n",
            "Source: three hundred and eighty two thousand six hundred and seventy six\n",
            "Prediction:   \n",
            "Ground truth: 3 8 2 6 7 6\n",
            "Source: two million six hundred and ninety six thousand five hundred and sixty\n",
            "Prediction:   \n",
            "Ground truth: 2 6 9 6 5 6 0\n",
            "Source: six hundred and fifty three million seven hundred and twenty six thousand seven hundred and eighty seven\n",
            "Prediction:   \n",
            "Ground truth: 6 5 3 7 2 6 7 8 7\n",
            "Source: four million seven hundred and fourteen thousand four hundred and twenty one\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 7 1 4 4 2 1\n",
            "Source: forty six million one hundred and thirty nine thousand eight hundred and seventy two\n",
            "Prediction:   \n",
            "Ground truth: 4 6 1 3 9 8 7 2\n",
            "Source: eighty four thousand six hundred and thirty four\n",
            "Prediction:   \n",
            "Ground truth: 8 4 6 3 4\n",
            "Source: one million four hundred and ninety two thousand five hundred and thirty two\n",
            "Prediction:   \n",
            "Ground truth: 1 4 9 2 5 3 2\n",
            "Source: two hundred and seventy seven million six hundred and sixteen thousand six hundred and two\n",
            "Prediction:   \n",
            "Ground truth: 2 7 7 6 1 6 6 0 2\n",
            "Source: two million one hundred and fifty one thousand eight hundred and fifty nine\n",
            "Prediction:   \n",
            "Ground truth: 2 1 5 1 8 5 9\n",
            "Source: two million one hundred and seventeen thousand two hundred and twenty five\n",
            "Prediction:   \n",
            "Ground truth: 2 1 1 7 2 2 5\n",
            "Source: seven thousand eight hundred and sixty three\n",
            "Prediction:   \n",
            "Ground truth: 7 8 6 3\n",
            "Source: seventy seven thousand and seventy three\n",
            "Prediction:   \n",
            "Ground truth: 7 7 0 7 3\n",
            "Source: one hundred and seventy nine thousand two hundred and thirty nine\n",
            "Prediction:   \n",
            "Ground truth: 1 7 9 2 3 9\n",
            "Source: fifty two thousand five hundred and seventy three\n",
            "Prediction:   \n",
            "Ground truth: 5 2 5 7 3\n",
            "Source: six billion three hundred and fifty nine million one hundred and forty one thousand three hundred and fifty two\n",
            "Prediction:   \n",
            "Ground truth: 6 3 5 9 1 4 1 3 5 2\n",
            "Source: five hundred and fifty five million six hundred and thirty four thousand four hundred and thirty\n",
            "Prediction:   \n",
            "Ground truth: 5 5 5 6 3 4 4 3 0\n",
            "Source: thirty nine million six hundred and fifty four thousand and eighty seven\n",
            "Prediction:   \n",
            "Ground truth: 3 9 6 5 4 0 8 7\n",
            "Source: fifty two thousand six hundred and thirty nine\n",
            "Prediction:   \n",
            "Ground truth: 5 2 6 3 9\n",
            "Source: fifty five million three hundred and thirty four thousand nine hundred and eighty one\n",
            "Prediction:   \n",
            "Ground truth: 5 5 3 3 4 9 8 1\n",
            "Source: eight million nine hundred and ninety three thousand one hundred and seventy one\n",
            "Prediction:   \n",
            "Ground truth: 8 9 9 3 1 7 1\n",
            "Source: six hundred and eighty five million one hundred and eighty nine thousand six hundred and forty four\n",
            "Prediction:   \n",
            "Ground truth: 6 8 5 1 8 9 6 4 4\n",
            "Source: nineteen million three hundred and sixty seven thousand nine hundred and thirty\n",
            "Prediction:   \n",
            "Ground truth: 1 9 3 6 7 9 3 0\n",
            "Source: two billion one hundred and thirty four million eight hundred and nineteen thousand and seventy six\n",
            "Prediction:   \n",
            "Ground truth: 2 1 3 4 8 1 9 0 7 6\n",
            "Source: seven hundred and fifteen thousand and sixty four\n",
            "Prediction:   \n",
            "Ground truth: 7 1 5 0 6 4\n",
            "Source: fifty four thousand seven hundred and eighty\n",
            "Prediction:   \n",
            "Ground truth: 5 4 7 8 0\n",
            "Source: nine hundred and forty thousand six hundred and eighty five\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 4 0 6 8 5\n",
            "Source: eighteen million seven hundred and forty seven thousand and twenty six\n",
            "Prediction:   \n",
            "Ground truth: 1 8 7 4 7 0 2 6\n",
            "Source: three hundred and ninety thousand seven hundred and eighty six\n",
            "Prediction:   \n",
            "Ground truth: 3 9 0 7 8 6\n",
            "Source: eight hundred and forty two thousand three hundred and twenty three\n",
            "Prediction:   \n",
            "Ground truth: 8 4 2 3 2 3\n",
            "Source: nine thousand one hundred and seventy nine\n",
            "Prediction:   9\n",
            "Ground truth: 9 1 7 9\n",
            "Source: fourteen thousand nine hundred and forty seven\n",
            "Prediction:   \n",
            "Ground truth: 1 4 9 4 7\n",
            "Source: three billion sixty seven million four hundred and ten thousand seven hundred and fifteen\n",
            "Prediction:   \n",
            "Ground truth: 3 0 6 7 4 1 0 7 1 5\n",
            "Source: seven million ninety six thousand eight hundred and eighty three\n",
            "Prediction:   \n",
            "Ground truth: 7 0 9 6 8 8 3\n",
            "Source: nine million eight hundred and forty one thousand eight hundred and twenty five\n",
            "Prediction:   \n",
            "Ground truth: 9 8 4 1 8 2 5\n",
            "Source: thirty two thousand two hundred and eighty one\n",
            "Prediction:   \n",
            "Ground truth: 3 2 2 8 1\n",
            "Source: seventeen million seven hundred and sixty seven thousand and sixty five\n",
            "Prediction:   \n",
            "Ground truth: 1 7 7 6 7 0 6 5\n",
            "Source: seventy nine thousand one hundred and fifty eight\n",
            "Prediction:   \n",
            "Ground truth: 7 9 1 5 8\n",
            "Source: six thousand four hundred and forty two\n",
            "Prediction:   \n",
            "Ground truth: 6 4 4 2\n",
            "Source: five million seven hundred and fifty two thousand and seventy eight\n",
            "Prediction:   \n",
            "Ground truth: 5 7 5 2 0 7 8\n",
            "Source: four million nine hundred and fifty four thousand six hundred and thirty\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 9 5 4 6 3 0\n",
            "Source: eight thousand four hundred and eighty three\n",
            "Prediction:   \n",
            "Ground truth: 8 4 8 3\n",
            "Source: eight hundred and eleven thousand and seventy four\n",
            "Prediction:   \n",
            "Ground truth: 8 1 1 0 7 4\n",
            "Source: one thousand one hundred and thirty nine\n",
            "Prediction:   \n",
            "Ground truth: 1 1 3 9\n",
            "Source: five thousand five hundred and ninety nine\n",
            "Prediction:   \n",
            "Ground truth: 5 5 9 9\n",
            "Source: two thousand seven hundred and eighty six\n",
            "Prediction:   \n",
            "Ground truth: 2 7 8 6\n",
            "Source: four hundred million seven hundred and fifty five thousand six hundred and twenty\n",
            "Prediction:   4 4 4 4\n",
            "Ground truth: 4 0 0 7 5 5 6 2 0\n",
            "Source: forty three million five hundred and forty thousand eight hundred and fifty four\n",
            "Prediction:   \n",
            "Ground truth: 4 3 5 4 0 8 5 4\n",
            "Source: six billion one hundred and eighty two million one hundred and fifty six thousand eight hundred and eighty four\n",
            "Prediction:   \n",
            "Ground truth: 6 1 8 2 1 5 6 8 8 4\n",
            "Source: eight million eight hundred and ninety three thousand seven hundred and seventy nine\n",
            "Prediction:   \n",
            "Ground truth: 8 8 9 3 7 7 9\n",
            "Source: ninety one million nine hundred and sixty seven thousand one hundred and eighty one\n",
            "Prediction:   \n",
            "Ground truth: 9 1 9 6 7 1 8 1\n",
            "Source: four hundred and fifty three million three hundred and thirty thousand nine hundred and seventy three\n",
            "Prediction:   4 4 4 4\n",
            "Ground truth: 4 5 3 3 3 0 9 7 3\n",
            "Source: twenty three thousand nine hundred and eighty three\n",
            "Prediction:   \n",
            "Ground truth: 2 3 9 8 3\n",
            "Source: eight thousand three hundred and eighty eight\n",
            "Prediction:   \n",
            "Ground truth: 8 3 8 8\n",
            "Source: nine hundred and eighty three thousand seven hundred and five\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 8 3 7 0 5\n",
            "Source: six billion one hundred and six million seven hundred and ninety one thousand four hundred and thirty one\n",
            "Prediction:   \n",
            "Ground truth: 6 1 0 6 7 9 1 4 3 1\n",
            "Source: two billion forty six million nine hundred and twelve thousand one hundred and fourteen\n",
            "Prediction:   \n",
            "Ground truth: 2 0 4 6 9 1 2 1 1 4\n",
            "Source: seventy eight million eight hundred and nineteen thousand four hundred and fifty seven\n",
            "Prediction:   \n",
            "Ground truth: 7 8 8 1 9 4 5 7\n",
            "Source: forty four thousand five hundred and eighty four\n",
            "Prediction:   \n",
            "Ground truth: 4 4 5 8 4\n",
            "Source: nine hundred and seventy six thousand two hundred and ninety seven\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 7 6 2 9 7\n",
            "Source: five million seventy two thousand eight hundred and fifty\n",
            "Prediction:   \n",
            "Ground truth: 5 0 7 2 8 5 0\n",
            "Source: six thousand one hundred and six\n",
            "Prediction:   \n",
            "Ground truth: 6 1 0 6\n",
            "Source: one hundred and ninety one million one hundred and fifty four thousand five hundred and forty two\n",
            "Prediction:   \n",
            "Ground truth: 1 9 1 1 5 4 5 4 2\n",
            "Source: eight billion eight hundred and ninety three million seven hundred and thirty eight thousand seven hundred and seventy four\n",
            "Prediction:   \n",
            "Ground truth: 8 8 9 3 7 3 8 7 7 4\n",
            "Source: five billion nine hundred and twenty one million one hundred and thirty four thousand one hundred and fifty eight\n",
            "Prediction:   \n",
            "Ground truth: 5 9 2 1 1 3 4 1 5 8\n",
            "Source: seventy six thousand seven hundred and eight\n",
            "Prediction:   \n",
            "Ground truth: 7 6 7 0 8\n",
            "Source: six billion two hundred and eighty eight million nine hundred and thirty three thousand four hundred and sixteen\n",
            "Prediction:   \n",
            "Ground truth: 6 2 8 8 9 3 3 4 1 6\n",
            "Source: nine billion seven hundred and fourteen million six hundred and fifty five thousand two hundred and thirty one\n",
            "Prediction:   \n",
            "Ground truth: 9 7 1 4 6 5 5 2 3 1\n",
            "Source: three billion four hundred and seventeen million nine hundred and thirty thousand four hundred and twelve\n",
            "Prediction:   \n",
            "Ground truth: 3 4 1 7 9 3 0 4 1 2\n",
            "Source: sixty three thousand four hundred and nineteen\n",
            "Prediction:   \n",
            "Ground truth: 6 3 4 1 9\n",
            "Source: nine billion eight hundred and two million three hundred and thirty seven thousand seven hundred and eighty two\n",
            "Prediction:   \n",
            "Ground truth: 9 8 0 2 3 3 7 7 8 2\n",
            "Source: two hundred and sixty seven thousand two hundred and eighteen\n",
            "Prediction:   \n",
            "Ground truth: 2 6 7 2 1 8\n",
            "Source: one billion seven hundred and fifty nine million three hundred and seventy three thousand three hundred and one\n",
            "Prediction:   \n",
            "Ground truth: 1 7 5 9 3 7 3 3 0 1\n",
            "Source: five hundred and twelve million two hundred and forty two thousand and thirty four\n",
            "Prediction:   \n",
            "Ground truth: 5 1 2 2 4 2 0 3 4\n",
            "Source: five thousand two hundred and ninety\n",
            "Prediction:   \n",
            "Ground truth: 5 2 9 0\n",
            "Source: nine hundred and twenty four million nine hundred and seventy seven thousand six hundred and eighty three\n",
            "Prediction:   \n",
            "Ground truth: 9 2 4 9 7 7 6 8 3\n",
            "Source: five hundred and fourteen million one hundred and ten thousand seven hundred and sixty one\n",
            "Prediction:   \n",
            "Ground truth: 5 1 4 1 1 0 7 6 1\n",
            "Source: eight billion four hundred and fourteen million two hundred and thirty thousand four hundred and ninety two\n",
            "Prediction:   \n",
            "Ground truth: 8 4 1 4 2 3 0 4 9 2\n",
            "Source: eight billion five hundred and ninety seven million seven hundred and sixty nine thousand one hundred and twenty\n",
            "Prediction:   \n",
            "Ground truth: 8 5 9 7 7 6 9 1 2 0\n",
            "Source: ninety four thousand six hundred and seventy seven\n",
            "Prediction:   \n",
            "Ground truth: 9 4 6 7 7\n",
            "Source: three hundred and ninety three thousand three hundred and seventy four\n",
            "Prediction:   \n",
            "Ground truth: 3 9 3 3 7 4\n",
            "Source: eighty eight thousand and nine\n",
            "Prediction:   \n",
            "Ground truth: 8 8 0 0 9\n",
            "Source: ninety nine million three hundred and ninety five thousand three hundred and eighty eight\n",
            "Prediction:   \n",
            "Ground truth: 9 9 3 9 5 3 8 8\n",
            "Source: thirty nine million seven hundred and fifty five thousand two hundred and twenty five\n",
            "Prediction:   \n",
            "Ground truth: 3 9 7 5 5 2 2 5\n",
            "Source: eighteen million two hundred and eighty two thousand two hundred and nine\n",
            "Prediction:   \n",
            "Ground truth: 1 8 2 8 2 2 0 9\n",
            "Source: seventy three million four hundred and fifty three thousand nine hundred and twenty two\n",
            "Prediction:   \n",
            "Ground truth: 7 3 4 5 3 9 2 2\n",
            "Source: four billion eight hundred and sixty two million six hundred and eighty six thousand two hundred and sixty eight\n",
            "Prediction:   \n",
            "Ground truth: 4 8 6 2 6 8 6 2 6 8\n",
            "Source: eight million six hundred and forty four thousand four hundred and two\n",
            "Prediction:   \n",
            "Ground truth: 8 6 4 4 4 0 2\n",
            "Source: seventy one million two hundred and forty nine thousand and two\n",
            "Prediction:   \n",
            "Ground truth: 7 1 2 4 9 0 0 2\n",
            "Source: ten thousand five hundred\n",
            "Prediction:   \n",
            "Ground truth: 1 0 5 0 0\n",
            "Source: sixty nine million three hundred and five thousand nine hundred and fifty nine\n",
            "Prediction:   \n",
            "Ground truth: 6 9 3 0 5 9 5 9\n",
            "Source: three billion eight hundred and twenty five million three hundred and fifty three thousand six hundred and ninety seven\n",
            "Prediction:   \n",
            "Ground truth: 3 8 2 5 3 5 3 6 9 7\n",
            "Source: four million six hundred and fifty two thousand four hundred and sixty three\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 6 5 2 4 6 3\n",
            "Source: three thousand three hundred and thirty three\n",
            "Prediction:   \n",
            "Ground truth: 3 3 3 3\n",
            "Source: nine thousand eight hundred and fifty two\n",
            "Prediction:   9 5 2\n",
            "Ground truth: 9 8 5 2\n",
            "Source: seven hundred and sixty five million fifty nine thousand six hundred and thirty three\n",
            "Prediction:   \n",
            "Ground truth: 7 6 5 0 5 9 6 3 3\n",
            "Source: seventy two thousand two hundred and ninety two\n",
            "Prediction:   \n",
            "Ground truth: 7 2 2 9 2\n",
            "Source: one hundred and thirty three million seven hundred and fifty seven thousand eight hundred and eighty six\n",
            "Prediction:   \n",
            "Ground truth: 1 3 3 7 5 7 8 8 6\n",
            "Source: nine hundred and sixty eight million three hundred and ninety five thousand seven hundred and thirty three\n",
            "Prediction:   \n",
            "Ground truth: 9 6 8 3 9 5 7 3 3\n",
            "Source: eight thousand seven hundred and twenty five\n",
            "Prediction:   \n",
            "Ground truth: 8 7 2 5\n",
            "Source: eight billion one hundred and seventy one million seven hundred and sixty seven thousand and seventy one\n",
            "Prediction:   \n",
            "Ground truth: 8 1 7 1 7 6 7 0 7 1\n",
            "Source: two billion seven hundred and thirty five million nine hundred and sixty thousand five hundred and eighty one\n",
            "Prediction:   \n",
            "Ground truth: 2 7 3 5 9 6 0 5 8 1\n",
            "Source: fourteen thousand eight hundred and thirty four\n",
            "Prediction:   \n",
            "Ground truth: 1 4 8 3 4\n",
            "Source: eight million four hundred and eighty two thousand two hundred and forty\n",
            "Prediction:   \n",
            "Ground truth: 8 4 8 2 2 4 0\n",
            "Source: nine million nine hundred and twenty thousand one hundred and seventy five\n",
            "Prediction:   \n",
            "Ground truth: 9 9 2 0 1 7 5\n",
            "Source: nine hundred and forty four thousand two hundred and ninety six\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 4 4 2 9 6\n",
            "Source: seventy thousand eight hundred and ninety six\n",
            "Prediction:   \n",
            "Ground truth: 7 0 8 9 6\n",
            "Source: two billion five hundred and forty nine million eight hundred and twenty four thousand one hundred and seventy eight\n",
            "Prediction:   \n",
            "Ground truth: 2 5 4 9 8 2 4 1 7 8\n",
            "Source: eight million nine hundred and ninety two thousand four hundred and seventy six\n",
            "Prediction:   \n",
            "Ground truth: 8 9 9 2 4 7 6\n",
            "Source: forty six million five hundred and ninety six thousand one hundred and thirty five\n",
            "Prediction:   \n",
            "Ground truth: 4 6 5 9 6 1 3 5\n",
            "Source: eight hundred and fifty eight thousand four hundred and twenty\n",
            "Prediction:   \n",
            "Ground truth: 8 5 8 4 2 0\n",
            "Source: fifty six thousand six hundred and eighty seven\n",
            "Prediction:   \n",
            "Ground truth: 5 6 6 8 7\n",
            "Source: five thousand eight hundred and thirty\n",
            "Prediction:   \n",
            "Ground truth: 5 8 3 0\n",
            "Source: four million four hundred and thirty one thousand six hundred and sixty\n",
            "Prediction:   4 4 4 4\n",
            "Ground truth: 4 4 3 1 6 6 0\n",
            "Source: seven million four hundred and thirty seven thousand eight hundred and thirty nine\n",
            "Prediction:   \n",
            "Ground truth: 7 4 3 7 8 3 9\n",
            "Source: twelve thousand six hundred and sixty eight\n",
            "Prediction:   \n",
            "Ground truth: 1 2 6 6 8\n",
            "Source: seventy two thousand four hundred and sixty nine\n",
            "Prediction:   \n",
            "Ground truth: 7 2 4 6 9\n",
            "Source: one hundred and ninety nine million five hundred and sixty four thousand six hundred and eleven\n",
            "Prediction:   \n",
            "Ground truth: 1 9 9 5 6 4 6 1 1\n",
            "Source: two million three hundred and fifty three thousand six hundred and twenty three\n",
            "Prediction:   \n",
            "Ground truth: 2 3 5 3 6 2 3\n",
            "Source: four thousand six hundred and five\n",
            "Prediction:   4 4\n",
            "Ground truth: 4 6 0 5\n",
            "Source: five billion five hundred and twenty eight million one hundred and seventy four thousand one hundred and sixty two\n",
            "Prediction:   \n",
            "Ground truth: 5 5 2 8 1 7 4 1 6 2\n",
            "Source: one billion sixteen million one hundred and nine thousand one hundred and seventy five\n",
            "Prediction:   \n",
            "Ground truth: 1 0 1 6 1 0 9 1 7 5\n",
            "Source: one hundred and ninety nine thousand eight hundred and nineteen\n",
            "Prediction:   \n",
            "Ground truth: 1 9 9 8 1 9\n",
            "Source: eight hundred and forty seven thousand six hundred and fifty two\n",
            "Prediction:   \n",
            "Ground truth: 8 4 7 6 5 2\n",
            "Source: sixty two thousand two hundred and ninety five\n",
            "Prediction:   \n",
            "Ground truth: 6 2 2 9 5\n",
            "Source: six hundred and sixty four thousand four hundred and eighty four\n",
            "Prediction:   \n",
            "Ground truth: 6 6 4 4 8 4\n",
            "Source: one hundred and twenty one million four hundred and forty four thousand five hundred and twenty three\n",
            "Prediction:   \n",
            "Ground truth: 1 2 1 4 4 4 5 2 3\n",
            "Source: five hundred and forty one million nine hundred and one thousand six hundred and forty five\n",
            "Prediction:   \n",
            "Ground truth: 5 4 1 9 0 1 6 4 5\n",
            "Source: nine million three hundred and eighty seven thousand one hundred and sixty four\n",
            "Prediction:   \n",
            "Ground truth: 9 3 8 7 1 6 4\n",
            "Source: fifty seven thousand eight hundred and ninety one\n",
            "Prediction:   \n",
            "Ground truth: 5 7 8 9 1\n",
            "Source: seven hundred and ninety six million seven hundred and sixty six thousand and forty eight\n",
            "Prediction:   \n",
            "Ground truth: 7 9 6 7 6 6 0 4 8\n",
            "Source: five million five hundred and forty two thousand two hundred and fifty five\n",
            "Prediction:   \n",
            "Ground truth: 5 5 4 2 2 5 5\n",
            "Source: eight thousand three hundred and eighty four\n",
            "Prediction:   \n",
            "Ground truth: 8 3 8 4\n",
            "Source: nine thousand five hundred and eighty three\n",
            "Prediction:   9 9\n",
            "Ground truth: 9 5 8 3\n",
            "Source: ninety nine thousand four hundred and eighty eight\n",
            "Prediction:   \n",
            "Ground truth: 9 9 4 8 8\n",
            "Source: five hundred and fifty three million six hundred and thirty four thousand eight hundred and thirty one\n",
            "Prediction:   \n",
            "Ground truth: 5 5 3 6 3 4 8 3 1\n",
            "Source: five hundred and fifty four million one hundred and thirty thousand seven hundred and sixty four\n",
            "Prediction:   \n",
            "Ground truth: 5 5 4 1 3 0 7 6 4\n",
            "Source: nine million sixty six thousand five hundred and seventeen\n",
            "Prediction:   \n",
            "Ground truth: 9 0 6 6 5 1 7\n",
            "Source: three thousand and forty seven\n",
            "Prediction:   \n",
            "Ground truth: 3 0 4 7\n",
            "Source: fifty five thousand four hundred and seven\n",
            "Prediction:   \n",
            "Ground truth: 5 5 4 0 7\n",
            "Source: seven billion one hundred and fifty million seven hundred and fifty thousand nine hundred and one\n",
            "Prediction:   \n",
            "Ground truth: 7 1 5 0 7 5 0 9 0 1\n",
            "Source: one thousand and eighty four\n",
            "Prediction:   \n",
            "Ground truth: 1 0 8 4\n",
            "Source: eight hundred and eighteen thousand three hundred and thirty five\n",
            "Prediction:   \n",
            "Ground truth: 8 1 8 3 3 5\n",
            "Source: two thousand nine hundred and three\n",
            "Prediction:   \n",
            "Ground truth: 2 9 0 3\n",
            "Source: two thousand seven hundred and seventy three\n",
            "Prediction:   \n",
            "Ground truth: 2 7 7 3\n",
            "Source: seven thousand seven hundred and sixty five\n",
            "Prediction:   \n",
            "Ground truth: 7 7 6 5\n",
            "Source: one billion ninety one million five hundred and forty five thousand nine hundred and twenty four\n",
            "Prediction:   \n",
            "Ground truth: 1 0 9 1 5 4 5 9 2 4\n",
            "Source: seven billion seven hundred and twenty eight million one hundred and fifty nine thousand two hundred and seven\n",
            "Prediction:   \n",
            "Ground truth: 7 7 2 8 1 5 9 2 0 7\n",
            "Source: nine hundred and sixty thousand three hundred and two\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 6 0 3 0 2\n",
            "Source: six hundred and forty two thousand five hundred and eleven\n",
            "Prediction:   \n",
            "Ground truth: 6 4 2 5 1 1\n",
            "Source: five hundred and ninety two thousand seven hundred and seventy four\n",
            "Prediction:   \n",
            "Ground truth: 5 9 2 7 7 4\n",
            "Source: two hundred and forty million seven hundred and fifty two thousand nine hundred and eighty nine\n",
            "Prediction:   \n",
            "Ground truth: 2 4 0 7 5 2 9 8 9\n",
            "Source: eight hundred and ninety two thousand eight hundred and fifty five\n",
            "Prediction:   \n",
            "Ground truth: 8 9 2 8 5 5\n",
            "Source: eighty six million six hundred and forty seven thousand seven hundred and sixty seven\n",
            "Prediction:   \n",
            "Ground truth: 8 6 6 4 7 7 6 7\n",
            "Source: ninety two thousand one hundred and ninety\n",
            "Prediction:   \n",
            "Ground truth: 9 2 1 9 0\n",
            "Source: two million two hundred and fifty six\n",
            "Prediction:   \n",
            "Ground truth: 2 0 0 0 2 5 6\n",
            "Source: nine hundred and one thousand two hundred and four\n",
            "Prediction:   9 9 9\n",
            "Ground truth: 9 0 1 2 0 4\n",
            "Source: eight hundred and fourteen million seven hundred and ninety one thousand and eighty two\n",
            "Prediction:   \n",
            "Ground truth: 8 1 4 7 9 1 0 8 2\n",
            "Source: seven million seven hundred and fifty eight thousand six hundred and forty\n",
            "Prediction:   \n",
            "Ground truth: 7 7 5 8 6 4 0\n",
            "Source: four thousand five hundred and ninety one\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 5 9 1\n",
            "Source: forty thousand four hundred and one\n",
            "Prediction:   \n",
            "Ground truth: 4 0 4 0 1\n",
            "Source: forty six million two hundred and sixty five thousand two hundred and thirty three\n",
            "Prediction:   \n",
            "Ground truth: 4 6 2 6 5 2 3 3\n",
            "Source: five million nine hundred and seventy three thousand eight hundred and ten\n",
            "Prediction:   \n",
            "Ground truth: 5 9 7 3 8 1 0\n",
            "Source: six million one hundred and sixty thousand seven hundred and eighty three\n",
            "Prediction:   \n",
            "Ground truth: 6 1 6 0 7 8 3\n",
            "Source: forty eight million four hundred and eighty four thousand two hundred and thirty\n",
            "Prediction:   \n",
            "Ground truth: 4 8 4 8 4 2 3 0\n",
            "Source: seven hundred and fifty seven million five hundred and eighty five thousand six hundred and fifty six\n",
            "Prediction:   \n",
            "Ground truth: 7 5 7 5 8 5 6 5 6\n",
            "Source: fifty nine thousand and seventy three\n",
            "Prediction:   \n",
            "Ground truth: 5 9 0 7 3\n",
            "Source: six thousand three hundred and nineteen\n",
            "Prediction:   \n",
            "Ground truth: 6 3 1 9\n",
            "Source: sixty six thousand five hundred and eighty\n",
            "Prediction:   \n",
            "Ground truth: 6 6 5 8 0\n",
            "Source: nine hundred and fifty million eighty five thousand one hundred and thirty two\n",
            "Prediction:   \n",
            "Ground truth: 9 5 0 0 8 5 1 3 2\n",
            "Source: six thousand eight hundred and twenty\n",
            "Prediction:   \n",
            "Ground truth: 6 8 2 0\n",
            "Source: four hundred and seventy five million five hundred and seventy six thousand and seventy nine\n",
            "Prediction:   4 4 4 4\n",
            "Ground truth: 4 7 5 5 7 6 0 7 9\n",
            "Source: forty seven thousand two hundred and seventy nine\n",
            "Prediction:   \n",
            "Ground truth: 4 7 2 7 9\n",
            "Source: five thousand nine hundred and thirty one\n",
            "Prediction:   \n",
            "Ground truth: 5 9 3 1\n",
            "Source: three hundred and six million six hundred and fifty one thousand eight hundred and eight\n",
            "Prediction:   \n",
            "Ground truth: 3 0 6 6 5 1 8 0 8\n",
            "Source: three hundred and eighty one thousand four hundred and ninety two\n",
            "Prediction:   \n",
            "Ground truth: 3 8 1 4 9 2\n",
            "Source: seven billion six hundred and sixty five million three hundred and ninety eight thousand five hundred and seven\n",
            "Prediction:   \n",
            "Ground truth: 7 6 6 5 3 9 8 5 0 7\n",
            "Source: two hundred and seventy two thousand one hundred and ninety five\n",
            "Prediction:   \n",
            "Ground truth: 2 7 2 1 9 5\n",
            "Source: twenty seven million three hundred and forty five thousand seven hundred and seventy five\n",
            "Prediction:   \n",
            "Ground truth: 2 7 3 4 5 7 7 5\n",
            "Source: six thousand four hundred and thirty seven\n",
            "Prediction:   \n",
            "Ground truth: 6 4 3 7\n",
            "Source: eighty three thousand eight hundred and seventy six\n",
            "Prediction:   \n",
            "Ground truth: 8 3 8 7 6\n",
            "Source: eighteen thousand and sixty five\n",
            "Prediction:   \n",
            "Ground truth: 1 8 0 6 5\n",
            "Source: four thousand six hundred and twenty nine\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 6 2 9\n",
            "Source: one hundred and twenty six thousand four hundred and seventy two\n",
            "Prediction:   \n",
            "Ground truth: 1 2 6 4 7 2\n",
            "Source: one hundred and eighty one thousand two hundred and fifty eight\n",
            "Prediction:   \n",
            "Ground truth: 1 8 1 2 5 8\n",
            "Source: eighty five thousand eight hundred and sixty three\n",
            "Prediction:   \n",
            "Ground truth: 8 5 8 6 3\n",
            "Source: four hundred and forty seven million four hundred and fifty eight thousand seven hundred and fifty eight\n",
            "Prediction:   \n",
            "Ground truth: 4 4 7 4 5 8 7 5 8\n",
            "Source: six million nine hundred and ninety four thousand five hundred and eighty eight\n",
            "Prediction:   \n",
            "Ground truth: 6 9 9 4 5 8 8\n",
            "Source: three thousand and sixty eight\n",
            "Prediction:   \n",
            "Ground truth: 3 0 6 8\n",
            "Source: three million three hundred and fifty thousand six hundred and twenty five\n",
            "Prediction:   \n",
            "Ground truth: 3 3 5 0 6 2 5\n",
            "Source: nine thousand two hundred and ten\n",
            "Prediction:   9 1\n",
            "Ground truth: 9 2 1 0\n",
            "Source: eight million nine hundred and three thousand five hundred and forty seven\n",
            "Prediction:   \n",
            "Ground truth: 8 9 0 3 5 4 7\n",
            "Source: one million eight hundred and seven thousand six hundred and eleven\n",
            "Prediction:   \n",
            "Ground truth: 1 8 0 7 6 1 1\n",
            "Source: eight million nine hundred and twenty five thousand five hundred and seventy one\n",
            "Prediction:   \n",
            "Ground truth: 8 9 2 5 5 7 1\n",
            "Source: nine hundred and seventy million eight hundred and sixty nine thousand two hundred and thirty four\n",
            "Prediction:   \n",
            "Ground truth: 9 7 0 8 6 9 2 3 4\n",
            "Source: four million nine hundred and seven thousand eight hundred and sixty two\n",
            "Prediction:   4 4 4\n",
            "Ground truth: 4 9 0 7 8 6 2\n",
            "Source: ninety million nine hundred and ninety nine thousand eight hundred and seventy\n",
            "Prediction:   \n",
            "Ground truth: 9 0 9 9 9 8 7 0\n",
            "Source: two million eight hundred and seventy eight thousand nine hundred and fifteen\n",
            "Prediction:   \n",
            "Ground truth: 2 8 7 8 9 1 5\n",
            "Source: eighty nine million eighty seven thousand six hundred and ninety four\n",
            "Prediction:   \n",
            "Ground truth: 8 9 0 8 7 6 9 4\n",
            "Source: six billion five hundred and eighteen million seven hundred and forty thousand six hundred and twenty seven\n",
            "Prediction:   \n",
            "Ground truth: 6 5 1 8 7 4 0 6 2 7\n",
            "Source: sixty two thousand two hundred and sixty six\n",
            "Prediction:   \n",
            "Ground truth: 6 2 2 6 6\n",
            "Source: one thousand two hundred and ninety six\n",
            "Prediction:   \n",
            "Ground truth: 1 2 9 6\n",
            "Source: four thousand seven hundred and thirty four\n",
            "Prediction:   4 4 4 4\n",
            "Ground truth: 4 7 3 4\n",
            "Source: three million nine hundred and fifty two thousand nine hundred and eighty seven\n",
            "Prediction:   \n",
            "Ground truth: 3 9 5 2 9 8 7\n",
            "Source: one hundred and seventy four thousand and thirty seven\n",
            "Prediction:   \n",
            "Ground truth: 1 7 4 0 3 7\n",
            "Source: two hundred and eight million eight hundred and ninety one thousand one hundred and seven\n",
            "Prediction:   \n",
            "Ground truth: 2 0 8 8 9 1 1 0 7\n",
            "Source: three thousand nine hundred and twelve\n",
            "Prediction:   \n",
            "Ground truth: 3 9 1 2\n",
            "Accuracy: 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wLXRx6dyMDa_"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "---\n",
        "**Question:** Can we interpret the attentions from a multi-layer transformer like what we did before? If so, what would you expect the attentions to be? If not, explain why. \n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_attn_transformer\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbimO3MmMDa_"
      },
      "source": [
        "We can interpret it like each rnn step. We expect it to be as the RNN matrices. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfQTq2EOMDa_"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "You might have noticed that the transformer model underperforms the RNN-based encoder-decoder on this particular task. This might be due to several reasons: \n",
        "\n",
        "* Transformers tend to be data hungry, sometimes requiring billions of words to train.\n",
        "* The transformer formulation presented in this lab is not in its full form: for instance, instead of only doing attention once at each position for each layer, researchers usually use multiple attention operations in the hope of capturing different aspects of \"relevance\", which is called \"multi-headed attention\". For example, one attention head might be focusing on pronoun resolution, while the other might be looking for similar contexts before.\n",
        "* Transformers are usually sensitive to hyper-parameters and require heavy tuning. For example, while we used a fixed learning rate, researchers usually use a customized learning rate scheduler which first warms up the learning rate, and then gradually decreases it. If you are interested, more details can be found in [the original paper](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "We also recommend the excellent pedagogic blog posts: [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) and [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention).\n",
        "\n",
        "In real-world applications, many state-of-the-art NLP approaches are based on transformers, such as [BERT](https://arxiv.org/abs/1810.04805) and [GPT-3](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "nd6dYgW-MDbA"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "---\n",
        "\n",
        "## Lab debrief – for consensus submission only\n",
        "\n",
        "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n",
        "\n",
        "* Was the lab too long or too short?\n",
        "* Were the readings appropriate for the lab? \n",
        "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n",
        "* Are there additions or changes you think would make the lab better?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_debrief\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqPWNTnxMDbA"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61iAoHc-3C6O"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# End of Lab 4-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "z7XRwKDUMDbA"
      },
      "source": [
        "---\n",
        "\n",
        "To double-check your work, the cell below will rerun all of the autograder tests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "tSnTY8rJMDbB"
      },
      "source": [
        "grader.check_all()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}