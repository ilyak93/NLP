{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"title":"CS187 Lab 1-1: Types, tokens, and representing text","colab":{"name":"lab1-1.ipynb","provenance":[],"collapsed_sections":["nl8zn76A3gYy"]}},"cells":[{"cell_type":"code","metadata":{"deletable":false,"editable":false,"jupyter":{"outputs_hidden":true,"source_hidden":true},"id":"n0EoheOB3gCk","executionInfo":{"status":"ok","timestamp":1604611281000,"user_tz":-120,"elapsed":3419,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"1115c32c-e8b4-4632-a0f5-c3daf2603049","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Please do not change this cell because some hidden tests might depend on it.\n","import os\n","\n","# Otter grader does not handle ! commands well, so we define and use our\n","# own function to execute shell commands.\n","def shell(commands, warn=True):\n","    \"\"\"Executes the string `commands` as a sequence of shell commands.\n","     \n","       Prints the result to stdout and returns the exit status. \n","       Provides a printed warning on non-zero exit status unless `warn` \n","       flag is unset.\n","    \"\"\"\n","    file = os.popen(commands)\n","    print (file.read().rstrip('\\n'))\n","    exit_status = file.close()\n","    if warn and exit_status != None:\n","        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n","    return exit_status\n","\n","shell(\"\"\"\n","ls requirements.txt >/dev/null 2>&1\n","if [ ! $? = 0 ]; then\n"," rm -rf .tmp\n"," git clone https://github.com/cs236299-2020/lab1-1.git .tmp\n"," mv .tmp/tests ./\n"," mv .tmp/requirements.txt ./\n"," rm -rf .tmp\n","fi\n","pip install -q -r requirements.txt\n","\"\"\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"vGlDSo3h3gCu","executionInfo":{"status":"ok","timestamp":1604611285591,"user_tz":-120,"elapsed":964,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["# Initialize Otter\n","import otter\n","grader = otter.Notebook()"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Rjf_L383gDc"},"source":["# Course 236299\n","## Lab 1-1 – Types, tokens, and representing text"]},{"cell_type":"code","metadata":{"id":"ySaEzC4i3gD4"},"source":["import math\n","import numpy as np\n","import re\n","import sys\n","import torchtext"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9W8fdOcU3gEM"},"source":["Where we're headed: Nearest neighbor text classification works by classifying a novel text with the same class as that of the text that is closest according to some distance metric. These metrics are calculated based on representations of the texts. In this lab, we'll introduce some different representations and you'll use nearest neighbor classification to predict the speaker of sentences selected from a children's book.\n","    \n","The objectives of this lab are to:\n","\n","* Clarify terminology around words and texts,\n","* Manipulate different representations of words and texts,\n","* Apply these representations to calculate text similarity, and\n","* Classify documents by a simple nearest neighbor model.\n","   \n","In this and later labs, we have you carry out several exercises in notebook cells. The cells you are to do are marked '`#TODO`'. They will typically have a `...` where your code or answer should go. Where specified, you needn't write code to calculate the answer, but instead, simply work out the answer yourself and enter it."]},{"cell_type":"markdown","metadata":{"id":"lsGLwkpp3gEO"},"source":["New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n","\n","* `len`\n","* `math.pi`\n","* `numpy.dot`\n","* `numpy.linalg.norm`\n","* `numpy.maximum`\n","* `numpy.minimum`\n","* `numpy.sum`\n","* `numpy.where`\n","* `numpy.zeros`\n","* `numpy.zeros_like`\n","* `re.match`\n","* `str.join`\n","* `str.lower`\n","* `torchtext.data.get_tokenizer`"]},{"cell_type":"markdown","metadata":{"id":"oYaYOkV13gEQ"},"source":["## Counting words\n","\n","<img src=\"https://github.com/nlp-236299/data/blob/master/Seuss/seuss%20-%201966%20-%20green%20eggs%20and%20ham.gif?raw=true\" width=150 align=right />\n","\n","Here are five sentences from Dr. Seuss's [_Green Eggs and Ham_](https://en.wikipedia.org/wiki/Green_Eggs_and_Ham):\n","\n","    Would you like them here or there?\n","    I would not like them here or there.\n","    I would not like them anywhere.\n","    I do not like green eggs and ham.\n","    I do not like them, Sam-I-am.\n","\n","We'll make this text available in the variable `text`."]},{"cell_type":"code","metadata":{"id":"nAU-DqTD3gES","executionInfo":{"status":"ok","timestamp":1604604872702,"user_tz":-120,"elapsed":858,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["text = \"\"\"\n","    Would you like them here or there?\n","    I would not like them here or there.\n","    I would not like them anywhere.\n","    I do not like green eggs and ham.\n","    I do not like them, Sam-I-am.\n","    \"\"\""],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"SzE3fL3g3gEw"},"source":["A Python string like this is, of course, a sequence of characters. But we think of this text as a sequence of sentences each composed of a sequence of words. How many words are there in this text? That is a fraught question, for several reasons, including\n","\n","* The type-token distinction\n","* Tokenization issues\n","* Normalization\n","\n","## Types versus tokens\n","\n","In determining the number of words in `text`, are we talking about word _types_ or word _tokens_. (For instance, there are five _tokens_ of the word _type_ 'like'.)\n","\n","How many word tokens are there in total in this text?\n","Write code that counts them, or count them manually.\n","Assign the number to the variable `token_count` in the next cell.\n","<!--\n","BEGIN QUESTION\n","name: token_count\n","-->\n","> If you have the otter-grader module properly installed (`pip install otter-grader`), you can test your solution using this unit test by running the `grader.check`(\"token_count\") two cells below. There's also a cell at the end of the notebook to run all of the unit tests."]},{"cell_type":"code","metadata":{"id":"7ISOxFmZ3gEy","executionInfo":{"status":"error","timestamp":1604611303625,"user_tz":-120,"elapsed":1261,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"560dd429-8e9b-4a29-b71e-f338cd61ab7c","colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["#TODO - define `token_count` to be the number of tokens in `text`\n","words = re.findall(r'[^\\s!,.?\":;]+', text)\n","token_count = len(words)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-a4d759630c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TODO - define `token_count` to be the number of tokens in `text`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\s!,.?\":;]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtoken_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 're' is not defined"]}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"I5dhx2z83gFH","executionInfo":{"status":"ok","timestamp":1604604876212,"user_tz":-120,"elapsed":667,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"aa528d9e-450c-4f98-af1f-25cd02b49316","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"token_count\")"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"9jk6_1Be3gFl"},"source":["How many word types are there? (Again, you can just count manually.)\n","<!--\n","BEGIN QUESTION\n","name: type_count\n","-->"]},{"cell_type":"code","metadata":{"id":"velT4Mgv3gFo","executionInfo":{"status":"ok","timestamp":1604604902665,"user_tz":-120,"elapsed":878,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["#TODO - define `type_count` to be the number of types in `text`\n","type_count = len(set(words))"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"QFxd3QCI3gGZ","executionInfo":{"status":"ok","timestamp":1604604906067,"user_tz":-120,"elapsed":1192,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"fc5775fd-7e26-46d9-a814-563f74f61e08","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"type_count\")"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"M1Ay7xYD3gGi"},"source":["<!-- BEGIN QUESTION -->\n","\n","The set of types of a language is referred to as its _vocabulary_. Are there more types or tokens as you calculated above? Could it be otherwise?\n","<!--\n","BEGIN QUESTION\n","name: type_vs_token_count\n","manual: true\n","-->"]},{"cell_type":"markdown","metadata":{"id":"s6D8K5hC3gGk"},"source":["From a quantitative point of view, comparing the number of tokens and the number of types, the number of tokens is greater or equal (but not lesser) then types, because types is the unique number of tokens (some tokens can appear more then once, then they are omitted in the types count). Those counts of tokens and types also depend on 'the issue of tokenization of text', i.e tokenize separately punctuations, capital/non-capital words and e.t.c."]},{"cell_type":"markdown","metadata":{"id":"RSFKA24V3gGo"},"source":["<!-- END QUESTION -->\n","\n","\n","\n","## Tokenization "]},{"cell_type":"markdown","metadata":{"id":"Aje-MzYs3gGr"},"source":["Did you count 'there?' as one token or two? This raises the issue of _tokenization_ of text, how to decide where the token boundaries occur. For instance, here's a simple way to split a string – to _tokenize_ it – in Python by splitting at whitespace."]},{"cell_type":"code","metadata":{"id":"4gzex6vH3gGs"},"source":["def whitespace_tokenize(str):\n","    return str.split()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"N2sbzm9o3gG3"},"source":["Try it out on the `text` defined above.\n","<!--\n","BEGIN QUESTION\n","name: tokens_whitespace\n","-->"]},{"cell_type":"code","metadata":{"id":"BG3czRSP3gG-"},"source":["#TODO - define `tokens` to be the tokens as defined by the `whitespace_tokenize` function\n","tokens = whitespace_tokenize(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"k9gUpdeG3gHH","executionInfo":{"status":"ok","timestamp":1604492366326,"user_tz":-120,"elapsed":726,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"15e7dd21-4bb2-43f8-c851-c497c9c3a44a","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"tokens_whitespace\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"U9-Z6bii3gHp"},"source":["Using this tokenization method, count the number of tokens in the text, this time using Python to do the work.\n","<!--\n","BEGIN QUESTION\n","name: token_count_whitespace\n","-->"]},{"cell_type":"code","metadata":{"id":"T_feIb8Z3gHq"},"source":["#TODO - place your token count here\n","token_count_2 = len(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"WxgWgyUP3gH0","executionInfo":{"status":"ok","timestamp":1604518458905,"user_tz":-120,"elapsed":747,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"0c9341dc-4e1e-4e1f-a538-6ca695e8725c","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"token_count_whitespace\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":163}]},{"cell_type":"markdown","metadata":{"id":"RjLJOpDS3gH9"},"source":["Arguably, we _should_ split off punctuation as separate tokens, but even there, some care must be taken. We don't want to split 'don't' into three tokens or 'Sam-I-Am' into five. (There's a good argument to be made however that the string 'don't' should be construed as two tokens, namely, 'do' and 'n't', but that's beyond the scope of today's discussion.)\n","\n","Here, we provide an alternative tokenizer that splits tokens at whitespace and splits off punctuation at the beginning and end of non-whitespace regions as separate tokens as well. It makes use of [the Python `re` module](https://docs.python.org/3/library/re.html) for regular expressions to specify the splitting process. Look over the code and make sure you understand what's going on."]},{"cell_type":"code","metadata":{"id":"DOO1c4qu3gH_"},"source":["def punc_tokenize(str):\n","    return [tok for tok in re.split('(\\W*?)\\s+(\\W*)', str) if tok != '']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"QrRAqe9-3gIF"},"source":["Now how many tokens are there in the text if tokenized in this way?\n","<!--\n","BEGIN QUESTION\n","name: token_count_punc\n","-->"]},{"cell_type":"code","metadata":{"id":"-M2Sqi8h3gIo","executionInfo":{"status":"ok","timestamp":1604492403327,"user_tz":-120,"elapsed":742,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"8030de3c-38ff-4ead-8dc5-10e474bc4457","colab":{"base_uri":"https://localhost:8080/"}},"source":["#TODO\n","token_count_3 = len(punc_tokenize(text))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["41\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"hXuTjdy73gIx","outputId":"605d2653-8e0b-469c-e62c-13ec2180e193"},"source":["grader.check(\"token_count_punc\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"wnRl73bI3gI8"},"source":["## Normalization\n","\n","This tokenization method counts 'Would' and 'would' (capitalized and uncapitalized) as separate types. Is that a good idea? This raises the issue of text _normalization_.\n","\n","Define a function `normalize_token` that normalizes tokens by making them lowercase if at most the first character is uppercase. (Hints [here](https://docs.python.org/3/library/stdtypes.html#str.lower) and [here](https://docs.python.org/3/library/re.html#re.match). These are also listed in the hint cell at the top of the lab, so we'll mostly stop providing these hints from here on.)\n","<!--\n","BEGIN QUESTION\n","name: normalize_token\n","-->"]},{"cell_type":"code","metadata":{"id":"gwJAcyDe3gI9","executionInfo":{"status":"ok","timestamp":1604610041752,"user_tz":-120,"elapsed":1369,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["#TODO - implement normalize_token, which returns the normalized word for a single word `str`\n","def normalize_token(str):\n","    import copy\n","    word = copy.copy(str)\n","    if re.match(r'\\b[A-Z].*?\\b', str):\n","        return word.lower()\n","    return word"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"Hwxq618Q3gJe","executionInfo":{"status":"error","timestamp":1604610041759,"user_tz":-120,"elapsed":1350,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"60d57a7e-853f-4d38-b006-e13a79bea45d","colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["grader.check(\"normalize_token\")"],"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-97b6a9b506c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"normalize_token\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'grader' is not defined"]}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"pxUkF6TE3gKN"},"source":["Now define `norm_tokens_punc` to be the sequence of normalized tokens as tokenized by `punc_tokenize`\n","<!--\n","BEGIN QUESTION\n","name: norm_tokens_punc\n","-->"]},{"cell_type":"code","metadata":{"id":"U2qJAyHf3gKT","executionInfo":{"status":"aborted","timestamp":1604610041754,"user_tz":-120,"elapsed":1339,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["#TODO\n","before = punc_tokenize(text)\n","norm_tokens_punc = [normalize_token(word) for word in punc_tokenize(text)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"LrSiKL0m3gK6","executionInfo":{"status":"aborted","timestamp":1604610041757,"user_tz":-120,"elapsed":1328,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["grader.check(\"norm_tokens_punc\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"nWMIteEO3gLD"},"source":["How many types are there when tokenized and normalized in this way?\n","<!--\n","BEGIN QUESTION\n","name: type_count_norm_punc\n","-->"]},{"cell_type":"code","metadata":{"id":"1dp1zic03gLH","executionInfo":{"status":"aborted","timestamp":1604610041758,"user_tz":-120,"elapsed":1321,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["#TODO\n","type_count_norm_punc = len(set(norm_tokens_punc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"4rHq31k-3gLa","executionInfo":{"status":"aborted","timestamp":1604610041759,"user_tz":-120,"elapsed":1309,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["grader.check(\"type_count_norm_punc\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M2Q044SD3gLu"},"source":["## Using prebuilt tokenizers\n","\n","Tokenization and normalization are so commonly needed that many packages provide pre-built tokenizers of various sorts. We'll use one from `torchtext`, a package that we'll make a fair amount of use of in the course."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"7h6Uv63T3gLw"},"source":["Define two tokenizers, versions of `whitespace_tokenize` and a normalized version of `punc_tokenize` above, using [the `torchtext.data.get_tokenizer` function](https://pytorch.org/text/data.html#get-tokenizer). (Use respectively the `None` and `\"basic_english\"` tokenizers that they provide.)\n","<!--\n","BEGIN QUESTION\n","name: tt_whitespace_tokenize_and_tt_normpunc_tokenize\n","-->"]},{"cell_type":"code","metadata":{"id":"83SfV6_g3gLz"},"source":["#TODO\n","def tt_whitespace_tokenize(str):\n","    toknizer = torchtext.data.get_tokenizer(None)\n","    return toknizer(str)\n","    \n","def tt_normpunc_tokenize(str):\n","    toknizer = torchtext.data.get_tokenizer(\"basic_english\")\n","    return toknizer(str)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"FOGi-1D03gL-","executionInfo":{"status":"ok","timestamp":1604496720525,"user_tz":-120,"elapsed":630,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"3774b14d-de9f-44ef-c261-b37ee8ca0d2e","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"tt_whitespace_tokenize_and_tt_normpunc_tokenize\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"LWFCihq73gMF"},"source":["Now we should be able to print out the last few tokens of the sample text, tokenized using these functions."]},{"cell_type":"code","metadata":{"id":"d9zCo9Ju3gMG"},"source":["print(tt_whitespace_tokenize(text)[-10:])\n","print(tt_normpunc_tokenize(text)[-10:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yi_7mhda3gML"},"source":["> _Meta-comment:_ Because it's important that you get practice both with implementing the ideas in the course from first principles and also with using prebuilt software that provides similar functionality, we'll often have you engage in this seemingly redundant process of first implementing a small example and then applying a prebuilt method to do much the same thing. The effort may be duplicative, but it is not wasted."]},{"cell_type":"markdown","metadata":{"id":"jXG-uS7P3gMN"},"source":["## Representing words\n","\n","In this section, we'll explore some simple representations for tokens, as a step on the way to representing texts – sentences or documents:\n","\n","### String encoding\n","We've already seen string encoding above, representing a token of a word type by a string specific to that type: a token 'green' represented by an instance of the Python string `'green'`, for instance, or 'Sam-I-am' represented by `'Sam-I-am'`. So let's move on.\n","\n","### 1-hot encoding\n","Given a vocabulary for a language, we can associate each type with an integer, say by its index in a vector. Notice that we've already imported the `numpy` module under the name `np`; we'll use a `numpy` array for the vocabulary vector. For the Seuss text, we can use this vocabulary:"]},{"cell_type":"code","metadata":{"id":"h3JuOIV13gMO","executionInfo":{"status":"ok","timestamp":1604496750612,"user_tz":-120,"elapsed":654,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"ec75fc54-757c-47c8-c15b-993220228724","colab":{"base_uri":"https://localhost:8080/"}},"source":["vocabulary = np.array(sorted(set(norm_tokens_punc)))\n","vocabulary"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([',', '.', '?', 'and', 'anywhere', 'do', 'eggs', 'green', 'ham',\n","       'here', 'i', 'like', 'not', 'or', 'sam-i-am', 'them', 'there',\n","       'would', 'you'], dtype='<U8')"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"markdown","metadata":{"id":"cZwgYeYx3gNf"},"source":["#### A digression on `numpy` arrays\n","\n","An important property of `numpy` arrays is that many operations on them work componentwise, that is, separately for each component of the array, rather than on the array all at once. This is called [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html), and it is common in libraries such as `numpy`, `torch`, and `tensorflow`. Compare the following two operations, first on lists, then on `numpy` arrays."]},{"cell_type":"code","metadata":{"id":"Twszk9YP3gNk","executionInfo":{"status":"ok","timestamp":1604496759128,"user_tz":-120,"elapsed":1153,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"79d4f4b8-92d5-49f7-ef2c-a28cf358020d","colab":{"base_uri":"https://localhost:8080/"}},"source":["['once', 'twice'] == 'once'"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"ihddckAZ3gNq","executionInfo":{"status":"ok","timestamp":1604496762341,"user_tz":-120,"elapsed":783,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"c625b264-1a80-4725-a36e-0c93636d3ffc","colab":{"base_uri":"https://localhost:8080/"}},"source":["np.array(['once', 'twice']) == 'once'"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ True, False])"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"markdown","metadata":{"id":"1QHZ3Oej3gN6"},"source":["This behavior of `numpy` arrays (and PyTorch tensors, as we will see) is quite powerful, allowing for simply specifying complex operations and for efficient, even parallelizable, computation of them. (If you want a deeper understanding of how `numpy`'s N-dimensional arrays work, see [the array documentation](https://numpy.org/doc/stable/reference/arrays.html).)\n","\n","But back to the 1-hot representation."]},{"cell_type":"markdown","metadata":{"id":"out_Oz5r3gN7"},"source":["In the _1-hot representation_ of words, a token is then represented by a bit vector (again given as a `numpy` array), with a 1 at the index of the token's type. For instance, the 1-hot representation of the comma token ',' would be"]},{"cell_type":"code","metadata":{"id":"vvJwUttP3gOC"},"source":["np.array([1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mCqPCHSs3gOP"},"source":["Conversion back and forth between these various representations is useful. Define functions `str_to_onehot` and `onehot_to_str` that convert between the string and one-hot representations using a vocabulary array to define the  conversion. \n","\n","Ideally, in your implementation, you'll want to take advantage of the componentwise nature of many `numpy` operations discussed above."]},{"cell_type":"code","metadata":{"id":"3QumLy0C3gOR"},"source":["#TODO\n","def str_to_onehot(wordtype, vocab):\n","    \"\"\"Returns the 1-hot representation of `wordtype`, a string, using \n","    `vocabulary`, a string vector, to define the conversion.\n","    The returned value should be a np.array with data type int.\n","    \"\"\"\n","    return np.array(vocab == wordtype, dtype=int)\n","\n","def onehot_to_str(onehot, vocab):\n","    \"\"\"Returns the string representation of `onehot`, a one-hot \n","    representation of a word type, using `vocabulary`, a string \n","    vector, to define the conversion.\n","    \"\"\"\n","    return vocab[list(onehot).index(1)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"wT6eOZ0R3gOc"},"source":["Now use `str_to_onehot` to define the variable `anywhere_1hot` to be the 1-hot representation for a token of the type 'anywhere'. \n","<!--\n","BEGIN QUESTION\n","name: anywhere_1hot\n","-->"]},{"cell_type":"code","metadata":{"id":"wbPmM8OD3gOe","executionInfo":{"status":"ok","timestamp":1604498947268,"user_tz":-120,"elapsed":1138,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"abbc84ad-0973-43f9-c5ee-501102c77073","colab":{"base_uri":"https://localhost:8080/"}},"source":["#TODO\n","anywhere_1hot = str_to_onehot('anywhere', vocabulary)\n","word = onehot_to_str(anywhere_1hot, vocabulary)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","anywhere\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"CFy4Gftt3gOj","executionInfo":{"status":"ok","timestamp":1604498818601,"user_tz":-120,"elapsed":1037,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"8a769bff-99e1-4574-f931-ff60d4ef3ffa","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"anywhere_1hot\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":94}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"xLbJmcq43gOp"},"source":["You can verify that the conversion worked correctly by inverting it using `onehot_to_str`, which we've done in the following unit test.\n","<!--\n","BEGIN QUESTION\n","name: anywhere_1hot_reverse\n","-->"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"vJxdja7I3gOq","executionInfo":{"status":"ok","timestamp":1604498890225,"user_tz":-120,"elapsed":654,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"adb083bc-5ad9-42cc-bab1-bac690f1f158","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"anywhere_1hot_reverse\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":101}]},{"cell_type":"markdown","metadata":{"id":"mSct2zKi3gOx"},"source":["## Representing texts\n","\n","### The set-of-words representation\n","\n","We can represent a whole text (a sequence of words) by manipulating the vector representations of the words within the text. For instance, we can take the **elementwise maximum** of the vectors. We refer to this as the _set-of-words_ representation.\n","\n","Here we've defined a function `set_of_words` that returns the set of words representation for a token sequence."]},{"cell_type":"code","metadata":{"id":"0jdImTjO3gOy"},"source":["def set_of_words(tokens, vocabulary):\n","    \"\"\"Returns the set-of-words representation as a numpy array for the \n","    sequence of `tokens` using the `vocabulary` to specify the conversion.\n","    \"\"\"\n","    onehots = np.array([str_to_onehot(token, vocabulary) for token in tokens])\n","    return np.amax(onehots, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WDRPv0zv3gO5"},"source":["This representation for a text is a vector that has a `1` for each word type that occurs in the text. The vector represents the subset of vocabulary words that appear in the text; hence the term 'set of words'.\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"sF7zUXe-3gO6"},"source":["What is the set-of-words representation for the example text 'I would not, would not, here or there.'?\n","<!--\n","BEGIN QUESTION\n","name: example_sow\n","-->"]},{"cell_type":"code","metadata":{"id":"k1-haa9P3gO8"},"source":["#TODO - define the variable to be the set of words representation for the example text\n","# Use tt_normpunc_tokenize tokenizer\n","example_text = 'I would not, would not, here or there.'\n","tokens = tt_normpunc_tokenize(example_text)\n","example_sow = set_of_words(tokens, vocabulary)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"skA5pN0o3gQA","executionInfo":{"status":"ok","timestamp":1604501532289,"user_tz":-120,"elapsed":723,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"db383480-d486-4749-f571-a3e2c9b91b90","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"example_sow\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":112}]},{"cell_type":"markdown","metadata":{"id":"eVKtB1QQ3gQl"},"source":["### The bag of words representation\n","\n","If instead, we take the componentwise _addition_ of the vectors instead of the maximum, the text representation provides the _frequency_ of each word type in the text. We refer to this representation as the _[bag](https://en.wikipedia.org/wiki/Multiset) of words_ representation. \n","\n","Define a function `bag_of_words`, analogous to `set_of_words` above, that returns the bag-of-words representation for a token sequence."]},{"cell_type":"code","metadata":{"id":"0N7Pkxn73gQm"},"source":["#TODO\n","def bag_of_words(tokens, vocabulary):\n","    onehots = np.array([str_to_onehot(token, vocabulary) for token in tokens])\n","    return np.sum(onehots, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"HTRVxkjy3gQw"},"source":["What is the bag of words representation for the example text 'I would not, would not, here or there.'?\n","<!--\n","BEGIN QUESTION\n","name: example_bow\n","-->"]},{"cell_type":"code","metadata":{"id":"ZVKDCexo3gQx","executionInfo":{"status":"ok","timestamp":1604501719502,"user_tz":-120,"elapsed":617,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"7ae94001-b78f-4107-a5a7-d13810cac184","colab":{"base_uri":"https://localhost:8080/"}},"source":["#TODO - define the variable to be the bag of words representation for the example text\n","# Use tt_normpunc_tokenize tokenizer\n","tokens = tt_normpunc_tokenize(example_text)\n","example_bow = bag_of_words(tokens,vocabulary)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2 1 0 0 0 0 0 0 0 1 1 0 2 1 0 0 1 2 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"097q_56w3gRM","executionInfo":{"status":"ok","timestamp":1604501761969,"user_tz":-120,"elapsed":1206,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"5b345473-f5b0-4fa0-ac2d-d552f356b9e4","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"example_bow\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"markdown","metadata":{"id":"Ou0NEQtD3gRY"},"source":["## Document similarity metrics\n","\n","Consider the following text classification problem: Each sentence in _Green Eggs and Ham_ is spoken by one of two characters, Sam-I-Am and Guy-Am-I. We want to be able to classify new sentences as (most likely) being uttered by one of the two.\n","\n","A simple method for text classification is the _nearest neighbor_ method. We select the class for the new sentence that is the same as the class of the \"nearest\" (most similar) sentence for which we already know the class. (You'll experiment much more with this text classification method in the next lab.)\n","\n","To perform nearest neighbor classification, we need a method for measuring the (metaphorical) distance between two texts based on their representations. We'll explore a few methods here:\n","\n","* Hamming distance\n","* Jaccard distance\n","* Euclidean distance\n","* cosine distance\n","\n","You'll implement code for all of these distance metrics. Try to implement the functions using `numpy` vector functions only, without explicit iteration over the elements in the vector.\n","\n","We'll take a look at the distances among the following sentences:\n","\n","1. Would you like them here or there?\n","2. I would not like them here or there.\n","3. Do you like green eggs and ham?\n","4. I do not like them Sam-I-Am.\n","\n","We'll start with the set of words representations of these sentences:"]},{"cell_type":"code","metadata":{"id":"QTreTP9r3gRZ"},"source":["examples = \"\"\"Would you like them here or there?\n","              I would not like them here or there.\n","              Do you like green eggs and ham?\n","              I do not like them Sam-I-Am.\"\"\" \\\n","           .split(\"\\n\")\n","sows = [set_of_words(tt_normpunc_tokenize(sentence), vocabulary) \n","            for sentence in examples]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"SixMMI3q3gRd"},"source":["### Hamming distance\n","\n","The Hamming distance between two vectors is the number of positions at which they differ. Define a function `hamming_distance` that computes the Hamming distance between two vectors.\n","<!--\n","BEGIN QUESTION\n","name: hamming_distance\n","-->"]},{"cell_type":"code","metadata":{"id":"wFjJpBjy3gRf"},"source":["#TODO - implement hamming_distance. The returned value should be a integer.\n","def hamming_distance(v1, v2):\n","    v1, v2= np.array(v1), np.array(v2)\n","    return sum(v1 != v2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"myGVNZf43gR0","executionInfo":{"status":"ok","timestamp":1604503155466,"user_tz":-120,"elapsed":738,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"aa1b5003-5701-403c-8e45-aaf51b669678","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"hamming_distance\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":132}]},{"cell_type":"markdown","metadata":{"id":"LX_enQ5u3gR5"},"source":["Now we can generate the Hamming distances among all of the sample sentences in a little table. Do the values make sense?"]},{"cell_type":"code","metadata":{"id":"Bq9o3dek3gR8","executionInfo":{"status":"ok","timestamp":1604503148658,"user_tz":-120,"elapsed":835,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"165bf50a-2128-439d-eaf4-68689761d634","colab":{"base_uri":"https://localhost:8080/"}},"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{hamming_distance(sows[i], sows[j]):4} \", end='')\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   0    5   10   11 \n","   5    0   15    6 \n","  10   15    0   11 \n","  11    6   11    0 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"zyYYBwpW3gSW"},"source":["### Jaccard distance\n","\n","The Jaccard distance between two sets (and remember that these bit strings basically represent sets) is one minus the number of elements in their intersection divided by the number of elements in their union.\n","\n","$$ D_{jaccard}(v_1, v_2) = 1 - \\frac{| v_1 \\cap v_2 |}{| v_1 \\cup v_2 |} $$\n","\n","Define a function `jaccard_distance`.\n","<!--\n","BEGIN QUESTION\n","name: jaccard_distance\n","-->"]},{"cell_type":"code","metadata":{"id":"1sL35SHW3gSa"},"source":["#TODO\n","def jaccard_distance(v1, v2):\n","    v1, v2 = np.array(v1), np.array(v2)\n","    return 1 - np.sum(np.bitwise_and(v1,v2)) / np.sum(np.bitwise_or(v1,v2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"cmbTUhEN3gSg","executionInfo":{"status":"ok","timestamp":1604508089206,"user_tz":-120,"elapsed":604,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"3bd67d88-310d-4e4a-854d-8900ae6ef274","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"jaccard_distance\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":149}]},{"cell_type":"markdown","metadata":{"id":"7lGvxefc3gTB"},"source":["Again, here's a table of the Jaccard distances among the sample sentences."]},{"cell_type":"code","metadata":{"id":"oLSzriYM3gTI","executionInfo":{"status":"ok","timestamp":1604508092445,"user_tz":-120,"elapsed":603,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"67f21ef5-b3f0-4479-c5f6-fdee0ebd8d93","colab":{"base_uri":"https://localhost:8080/"}},"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{jaccard_distance(sows[i], sows[j]):5.3f} \", end='')\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.000 0.455 0.769 0.846 \n","0.455 0.000 0.938 0.545 \n","0.769 0.938 0.000 0.846 \n","0.846 0.545 0.846 0.000 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"QxsOuRiP3gTc"},"source":["## Euclidean distance\n","\n","The Euclidean distance between two vectors is the norm of the vector between them, that is,\n","\n","$$ D_{euclidean}(\\mathbf{x}, \\mathbf{y}) = |\\mathbf{x} - \\mathbf{y}| $$\n","\n","where $|\\mathbf{z}|$, the norm of a vector $\\mathbf{z}$, is calculated as\n","\n","$$ |\\mathbf{z}| = \\sqrt{\\sum_{i=1}^N \\mathbf{z}_i^2} $$\n","\n","Fortunately, `numpy` provides the function `np.linalg.norm` to compute the norm, and the vector between two vectors is computed by componentwise subtraction.\n","\n","Define a function `euclidean_distance` to compute the Euclidean distance between two vectors.\n","<!--\n","BEGIN QUESTION\n","name: euclidean_distance\n","-->"]},{"cell_type":"code","metadata":{"id":"lp33Q5Q_3gVZ"},"source":["#TODO\n","def euclidean_distance(v1, v2):\n","    v1, v2 = np.array(v1), np.array(v2)\n","    return np.linalg.norm(v1-v2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"flKIJGT03gV7","executionInfo":{"status":"ok","timestamp":1604508262637,"user_tz":-120,"elapsed":1144,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"9153fdd6-2089-446a-c1e7-efe2f81275d5","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"euclidean_distance\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":152}]},{"cell_type":"markdown","metadata":{"id":"bZ2LsHQa3gWC"},"source":["Again, here's a table of the Euclidean distances among the sample sentences."]},{"cell_type":"code","metadata":{"id":"18-7ilG93gWC","executionInfo":{"status":"ok","timestamp":1604508265853,"user_tz":-120,"elapsed":643,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"1455ac70-cd45-420f-bb5a-0fd189d1a3ee","colab":{"base_uri":"https://localhost:8080/"}},"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{euclidean_distance(sows[i], sows[j]):5.3f} \", end='')\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.000 2.236 3.162 3.317 \n","2.236 0.000 3.873 2.449 \n","3.162 3.873 0.000 3.317 \n","3.317 2.449 3.317 0.000 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qfhMCvwJ3gWH"},"source":["## Cosine distance\n","\n","The _cosine similarity_ of two vectors of length $N$ is the cosine of the angle that they form, which is computed as the dot product of the two vectors divided by their norms.\n","\n","$$ cos(\\mathbf{x}, \\mathbf{y}) = \n","      \\frac{\\sum_{i=1}^N \\mathbf{x}_i \\cdot \\mathbf{y}_i}{|\\mathbf{x}| \\cdot |\\mathbf{y}|} $$\n","\n","This isn't a distance metric, but a similarity metric. For vectors of non-negative numbers, it ranges from 0 to 1, where 0 is maximally different and 1 is maximally similar. To turn it into a distance metric, then, we take the inverse cosine (to convert the cosine to an angle between $\\pi$ and 0) and divide by $\\pi$.\n","\n","$$ D_{cosine}(\\mathbf{x}, \\mathbf{y}) = \\frac{cos^{-1}(cos(\\mathbf{x}, \\mathbf{y}))}{\\pi} $$\n","\n","Since we're using `numpy`, some of these functions are already provided. See hints [here](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), [here](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html?highlight=norm#numpy.linalg.norm), and [here](https://numpy.org/doc/stable/reference/generated/numpy.arccos.html?highlight=arccos#numpy.arccos).\n","\n","(To avoid some math domain errors, we recommend that you use the function `safe_acos` that we've provided to compute the inverse cosine function instead of using `math.acos` directly.)"]},{"cell_type":"code","metadata":{"id":"s7IGdofL3gWI"},"source":["def safe_acos(x):\n","    \"\"\"Returns the arc cosine of `x`. Unlike `math.acos`, it \n","       does not raise an exception for values of `x` out of range, \n","       but rather clips `x` at -1..1, thereby avoiding math domain\n","       errors in the case of numerical errors.\"\"\"\n","    return math.acos(math.copysign(min(1.0, abs(x)), x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxESyN0x3gWx"},"source":["#TODO\n","def cosine_distance(v1, v2):\n","    \"\"\"Returns the cosine distance between two vectors\"\"\"\n","    v1, v2 = np.array(v1), np.array(v2)\n","    return safe_acos(np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2)))/math.pi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"wh4edWGm3gXB","executionInfo":{"status":"ok","timestamp":1604509012835,"user_tz":-120,"elapsed":781,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"12db5d1b-f568-46a9-9b1f-cc902ef9e3f6","colab":{"base_uri":"https://localhost:8080/","height":46}},"source":["grader.check(\"cosine_distance\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"metadata":{"tags":[]},"execution_count":159}]},{"cell_type":"code","metadata":{"id":"X8Fs7lSo3gYO","executionInfo":{"status":"ok","timestamp":1604509015382,"user_tz":-120,"elapsed":917,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"c797a396-d4ee-4a9e-fc13-6cb2e08891da","colab":{"base_uri":"https://localhost:8080/"}},"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{cosine_distance(sows[i], sows[j]):5.3f} \", end='')\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.000 0.250 0.378 0.414 \n","0.250 0.000 0.462 0.283 \n","0.378 0.462 0.000 0.414 \n","0.414 0.283 0.414 0.000 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K9uFxbL33gYw"},"source":["In the next lab, you'll use some of these distance metrics to automatically classify text using nearest neighbor classification."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"nl8zn76A3gYy"},"source":["<!-- BEGIN QUESTION -->\n","\n","## Lab debrief – for consensus submission only\n","\n","**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n","\n","* Was the lab too long or too short?\n","* Were the readings appropriate for the lab? \n","* Was it clear (at least after you completed the lab) what the points of the exercises were? \n","* Are there additions or changes you think would make the lab better?\n","\n","<!--\n","BEGIN QUESTION\n","name: open_response_debrief\n","manual: true\n","-->"]},{"cell_type":"markdown","metadata":{"id":"oIGwhELo3gY0"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","metadata":{"id":"wyxTxubF3gY1"},"source":["<!-- END QUESTION -->\n","\n","\n","\n","# End of lab 1-1"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"0JahdvrV3gY2"},"source":["---\n","\n","To double-check your work, the cell below will rerun all of the autograder tests."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"VhrGbSR83gY2","executionInfo":{"status":"ok","timestamp":1604522460416,"user_tz":-120,"elapsed":710,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"7556bb94-7b2d-4b9a-c86c-967ebed1a1c7","colab":{"base_uri":"https://localhost:8080/","height":805}},"source":["grader.check_all()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<p><strong>anywhere_1hot:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>anywhere_1hot_reverse:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>cosine_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>euclidean_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>example_bow:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>example_sow:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>hamming_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>jaccard_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>norm_tokens_punc:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>normalize_token:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>token_count:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>token_count_punc:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>token_count_whitespace:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>tokens_whitespace:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>tt_whitespace_tokenize_and_tt_normpunc_tokenize:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>type_count:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>type_count_norm_punc:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n"],"text/plain":["anywhere_1hot:\n","\n","    All tests passed!\n","    \n","\n","anywhere_1hot_reverse:\n","\n","    All tests passed!\n","    \n","\n","cosine_distance:\n","\n","    All tests passed!\n","    \n","\n","euclidean_distance:\n","\n","    All tests passed!\n","    \n","\n","example_bow:\n","\n","    All tests passed!\n","    \n","\n","example_sow:\n","\n","    All tests passed!\n","    \n","\n","hamming_distance:\n","\n","    All tests passed!\n","    \n","\n","jaccard_distance:\n","\n","    All tests passed!\n","    \n","\n","norm_tokens_punc:\n","\n","    All tests passed!\n","    \n","\n","normalize_token:\n","\n","    All tests passed!\n","    \n","\n","token_count:\n","\n","    All tests passed!\n","    \n","\n","token_count_punc:\n","\n","    All tests passed!\n","    \n","\n","token_count_whitespace:\n","\n","    All tests passed!\n","    \n","\n","tokens_whitespace:\n","\n","    All tests passed!\n","    \n","\n","tt_whitespace_tokenize_and_tt_normpunc_tokenize:\n","\n","    All tests passed!\n","    \n","\n","type_count:\n","\n","    All tests passed!\n","    \n","\n","type_count_norm_punc:\n","\n","    All tests passed!\n","    \n"]},"metadata":{"tags":[]},"execution_count":164}]}]}