{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "jupyter": {"outputs_hidden": true, "source_hidden": true}}, "outputs": [], "source": ["# Please do not change this cell because some hidden tests might depend on it.\n", "import os\n", "\n", "# Otter grader does not handle ! commands well, so we define and use our\n", "# own function to execute shell commands.\n", "def shell(commands, warn=True):\n", "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n", "     \n", "       Prints the result to stdout and returns the exit status. \n", "       Provides a printed warning on non-zero exit status unless `warn` \n", "       flag is unset.\n", "    \"\"\"\n", "    file = os.popen(commands)\n", "    print (file.read().rstrip('\\n'))\n", "    exit_status = file.close()\n", "    if warn and exit_status != None:\n", "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n", "    return exit_status\n", "\n", "shell(\"\"\"\n", "ls requirements.txt >/dev/null 2>&1\n", "if [ ! $? = 0 ]; then\n", " rm -rf .tmp\n", " git clone https://github.com/cs236299-2020/lab1-4.git .tmp\n", " mv .tmp/tests ./\n", " mv .tmp/requirements.txt ./\n", " rm -rf .tmp\n", "fi\n", "pip install -q -r requirements.txt\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook()"]}, {"cell_type": "raw", "metadata": {}, "source": ["%%latex\n", "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\newcommand{\\softmax}{\\operatorname{softmax}}\n", "\\newcommand{\\Prob}{\\Pr}\n", "\\newcommand{\\given}{\\,|\\,}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["$$\n", "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n", "\\renewcommand{\\Prob}{\\Pr}\n", "\\renewcommand{\\given}{\\,|\\,}\n", "$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Course 236299\n", "## Lab 1-4 \u2013 Discriminative methods for classification"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this lab, you'll apply a discriminative classification method to the _Federalist_ papers' authorship attribution problem, the logistic regression (perceptron) model. \n", "\n", "After this lab, you should be able to\n", "\n", "* Derive the basic equations for the logistic regression classification method;\n", "* Perform the forward computation to generate the class that a logistic regression model predicts for some input;\n", "* Calculate a loss for that prediction, the cross-entropy loss;\n", "* Update the parameters of a logistic regression model by stochastic gradient descent."]}, {"cell_type": "markdown", "metadata": {}, "source": ["New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n", "\n", "* `numpy.exp`"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preparation \u2013 Loading packages and data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Please do not change these imports because some hidden tests might depend on them.\n", "# You can add a cell below if you need to import anything else.\n", "import json\n", "import math\n", "import matplotlib\n", "import matplotlib.pyplot as plt\n", "matplotlib.style.use('tableau-colorblind10')\n", "import numpy as np\n", "import os\n", "\n", "from math import log2\n", "from pprint import pprint"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Download and read the Federalist data from the json file\n", "shell('wget -nv -N -P data https://github.com/nlp-236299/data/raw/master/Federalist/federalist_data.json')\n", "with open('data/federalist_data.json', 'r') as fin:\n", "    dataset = json.load(fin)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# As before, we extract the papers by either of Madison and Hamilton \n", "# to serve as training data.\n", "training = list(filter(lambda ex: ex['authors'] in ['Madison', 'Hamilton'],\n", "                       dataset))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Logistic regression\n", "\n", "You've read about logistic regression, a method for classification that is discriminative rather than generative. Like the generative Naive Bayes method, each example is characterized by a vector of features (the counts of word types for a text, say, as for the _Federalist_ example that we've been using). In logistic regression, each such feature is assigned a weight, and the score for an example is given by weighting each feature value by its weight and summing the result; that is, the score is the dot product of the feature vector and the weight vector. Let's take an example, one of the Federalist papers. We'll extract from the training data the counts for the first example in the training set. That's the feature vector for this example."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["training0_counts = training[0]['counts']\n", "training0_counts"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Suppose the weights for the features are as given by the following vector:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["weights = [-.1, .2, .3, -1]"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["What would the weighted sum of the counts with these weights be? Feel free to use `np.dot` to take the dot product for you.\n", "<!--\n", "BEGIN QUESTION\n", "name: wtd_sum\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO: Take the weighted sum of `training0_counts` with `weights`\n", "wtd_sum = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"wtd_sum\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** What is the range of possible values that such a weighted sum can take on?\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_possible_sums\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "In order to have a standard way of comparing these numbers, it helps to be able to place them on a fixed scale, from 0 to 1, say. This way, they can be interpreted as probabilities. We use the _logistic function_ ($\\sigma$) to carry out this conversion:\n", "\n", "$$ \\sigma(x) = \\frac{1}{1 + e^{-k x}}$$\n", "\n", "In addition to its argument $x$, the function takes an additional parameter $k$, which we will explore shortly.\n", "\n", "Define a function `sigma` implementing the logistic function. (We've established a default value for `k` of 1 in the header line below.)\n", "<!--\n", "BEGIN QUESTION\n", "name: sigma\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO: Implement the logistic function\n", "# x can be a number or a numpy array\n", "def sigma(x, k=1):\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"sigma\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To get a sense of the logistic function, we graph it for several values of $k$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sigma_plot():\n", "    x = np.linspace(-2, 2, 100)\n", "    fig, ax = plt.subplots()\n", "    for k in [0, 1, 2, 10]:\n", "        ax.plot(x, sigma(x, k), label = f\"k = {k}\")\n", "    plt.title(\"Logistic functions\")\n", "    plt.legend()\n", "    \n", "sigma_plot()"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** What does the $k$ parameter do to the logistic function?\n", "<!--\n", "BEGIN QUESTION\n", "manual: true\n", "name: open_response_k\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "The logistic function, when applied to the weighted average above is greater than 0.5."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sigma(wtd_sum)"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["In the _Federalist Papers_ classification problem, there are only two classes, so we can use this single value to determine the classification. For an input feature vector $\\vect{x}$ and weight vector $\\vect{w}$, we'll take the model to predict the probability of the author being Hamilton (say), as\n", "\n", "$$\\Prob(\\mathrm{Hamilton} \\given \\mathbf{x}) = \\sigma(\\mathbf{w} \\cdot \\mathbf{x})$$\n", "\n", "Therefore, \n", "\n", "$$\\Prob(\\mathrm{Madison} \\given \\mathbf{x}) = 1 - \\sigma(\\mathbf{w} \\cdot \\mathbf{x})$$\n", "\n", "since there are only two classes.\n", "\n", "> When there are more than two classes, we'd use a generalization of the sigmoid function, called [softmax](https://en.wikipedia.org/wiki/Softmax_function).\n", "\n", "Define a function `predict_lr` that calculates the probability of **Hamilton** being the author of an example text, given a weight vector and the feature vector for the example text.\n", "<!--\n", "BEGIN QUESTION\n", "name: predict_lr\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO: Calculate the probability of Hamilton being the author\n", "def predict_lr(weights, features):\n", "    ..."]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is the _forward computation_ for the logistic regression model, calculating its output prediction from some inputs. Next we turn to the _backward computation_, calculating the updates to the parameters based on any error in the predicted output, as measured by a loss function."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Using a logistic regression model to predict Federalist authorship"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Consider the following two training examples (examples 0 and 9) from the _Federalist_ training dataset:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for example in [0, 9]:\n", "    pprint(training[example])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As above, a logistic regression model is defined by a vector of weights $\\mathbf{w}$, like these:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["weights"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Calculate the predicted Hamilton probabilities for the two examples (examples 0 and 9) above.\n", "<!--\n", "BEGIN QUESTION\n", "name: prob_hamilton_examples\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "prob_hamilton_example0 = ...\n", "prob_hamilton_example9 = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"prob_hamilton_examples\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"example 0 prob of Hamilton: {prob_hamilton_example0:.3f}\\n\"\n", "      f\"example 9 prob of Hamilton: {prob_hamilton_example9:.3f}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** What does this model predict about the two training examples? Is it correct?\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_original_model_prediction\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "## Training a logistic regression model\n", "\n", "Of course, this is just one of an infinite number of models (weight vectors), since we could have set the weights in all kinds of ways. How should we come up with a _good_ model, i.e., a good setting of the weights? Ideally, we'd try to find a set of weights that predicts all of the training data well. This is the problem of _training_ a logistic regression model. Let's try another set of weights:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["weights_new = [0.1, 0.2, 0.3, -5]"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Calculate the probabilities generated by the model for these weights.\n", "<!--\n", "BEGIN QUESTION\n", "name: prob_hamilton_examples_new\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "prob_hamilton_example0_new = ...\n", "prob_hamilton_example9_new = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"prob_hamilton_examples_new\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"example 0 prob: {prob_hamilton_example0_new:.3f}\\n\" \n", "      f\"example 9 prob: {prob_hamilton_example9_new:.3f}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:**  Is this a better model, a worse model, or neither, at least as far as the two sample examples are concerned? \n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_new_model_prediction\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "An ideal model would give a probability of 1 to the Hamilton examples and a 0 to the Madison examples.\n", "For the two sample examples, you'll have noticed that the new weights generate not 1 and 0, respectively, but numbers quite a bit closer to 1 and 0.\n", "\n", "We could continue trying different weight values to try to improve the performance of the model on these training examples (and others) by trial and error, but a more systematic method is needed. We define a _loss function_, which specifies how bad a model is, and try to minimize the loss function by _stochastic gradient descent_.\n", "\n", "We'll do a few steps of the process here by hand. It's sufficiently tedious that it's far better to deploy computers on the task, which we'll do in the next lab.\n", "\n", "## Cross-entropy loss function\n", "\n", "We'll use the cross-entropy loss function. For an example $i$, we'll use $\\mathbf{x}^{(i)}$ for the feature vector, and $y^{(i)}$ for the actual (gold) label (1 or 0, 1 for Hamilton and 0 for Madison). The predicted probability of the label being 1 will be as per the logistic regression model $\\sigma(\\mathbf{w} \\cdot \\mathbf{x}^{(i)})$, which we will call $\\hat{y}^{(i)}$.\n", "\n", "The cross-entropy loss for example $i$ as per a model $\\mathbf{w}$ is\n", "$$L_{CE}(\\mathbf{w}) = -(y^{(i)} \\log \\hat{y}^{(i)} + (1-y^{(i)}) \\log (1-\\hat{y}^{(i)}))$$\n", "\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** What is the minimum possible value of the cross-entropy loss above? When is that achieved?\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_ce_loss\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "Define a function to compute the cross-entropy loss for a particular model (weight vector), example (feature vector), and gold label:\n", "<!--\n", "BEGIN QUESTION\n", "name: loss\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO: define the cross-entropy loss function according to the equation above. The output should be a scalar loss.\n", "def loss(weights, features, correct):\n", "    \"\"\"Returns the cross-entropy loss for a weight vector, a feature\n", "       vector and a gold label `correct`.\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"loss\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Use the `loss` function to determine the cross-entropy loss for example 0 for the original model that we used (`weights`), and for the new model (`weights_new`).\n", "<!--\n", "BEGIN QUESTION\n", "name: loss_example0_old_and_new\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO: Calculate loss for training[0] under `weights` and `weights_new`\n", "loss_example0_old = ...\n", "loss_example0_new = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"loss_example0_old_and_new\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Old model loss: {loss_example0_old:.3f}\\n\"\n", "      f\"New model loss: {loss_example0_new:.3f}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** Which of the models is better (at least on this example)?\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_loss_on_example\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "## Gradient of the loss function\n", "\n", "We want to find the weights that minimize the loss function. We use gradient descent:\n", "\n", "1. Find the gradient of the loss function, the direction in which it is increasing fastest.\n", "2. Take a step in the opposite direction.\n", "3. Repeat.\n", "\n", "For cross-entropy loss, recall that the partial derivative of the loss function with respect to a single weight $w_j$ is\n", "\n", "$$ \\frac{\\partial L_{CE}(\\mathbf{w})}{\\partial w_j} = (\\hat{y} - y) x_j $$\n", "\n", "> At the end of this subsection, we give a problem that explores how this gradient is derived, but you can just assume it for the time being.\n", "\n", "The gradient combines these partial derivatives for all of the weights.\n", "\n", "$$ \\nabla L_{CE}(\\mathbf{w}) = \\left[ \\begin{array}{c}\n", "    \\frac{\\partial L_{CE}(\\mathbf{w})}{\\partial w_1}\\\\\n", "    \\frac{\\partial L_{CE}(\\mathbf{w})}{\\partial w_2}\\\\\n", "    \\vdots \\\\\n", "    \\frac{\\partial L_{CE}(\\mathbf{w})}{\\partial w_m}\n", "    \\end{array} \\right] $$\n", "\n", "Let's work out an example, using example 0. The counts for example 0 are"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["training[0]['counts']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["and the original weights, recall, were"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["weights"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["What is the gradient vector for these `weights` and training example `training[0]`?\n", "<!--\n", "BEGIN QUESTION\n", "name: grad_vector\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["...\n", "grad_vector = ...\n", "grad_vector"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"grad_vector\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Adjusting weights against the gradient\n", "\n", "Step 2 is to adjust the weights in the _opposite_ direction of the gradient.\n", "In this case, we compute the new weight vector $w'$ by adding to the weight vector a fraction of the negative gradient:\n", "\n", "$$ \\mathbf{w}' = \\mathbf{w} - \\eta \\nabla L_{CE}(\\mathbf{w}) $$\n", "\n", "Here, $\\eta$ is the _learning rate_. The larger $\\eta$ is, the more we move in each step, but if $\\eta$ is too large we risk overshooting. We'll use a learning rate of 0.1 for now. (Setting good learning rates is one aspect of the black arts of machine learning.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["learning_rate = 0.1"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Calculate the new weight vector using `learning_rate` and `grad_vector`.\n", "<!--\n", "BEGIN QUESTION\n", "name: weights_updated\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["weights_updated = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"weights_updated\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pprint(weights_updated)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["How do these weights perform on the training example we've been using? Let's see."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loss_example0_updated = loss(weights_updated, training[0]['counts'], 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Old model loss: {loss_example0_old:.3f}\\n\"\n", "      f\"New model loss: {loss_example0_new:.3f}\\n\"\n", "      f\"Updated model loss: {loss_example0_updated:.3f}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["If you did this all right, the loss for the updated model, which was generated by taking a single step opposite the gradient from the old model is not only better than the old model, but better than the new model we used above as well. \n", "\n", "What about the loss on the other example we've been using (example 9)? Calculate the loss for example 9 with the old model (`weights`), the new model (`weights_new`), and the updated model (`weights_updated`):\n", "<!--\n", "BEGIN QUESTION\n", "name: weights_updated_9\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO\n", "loss_example9_old = ...\n", "loss_example9_new = ...\n", "loss_example9_updated = ..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"weights_updated_9\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Old model loss: {loss_example9_old:.3f}\\n\"\n", "      f\"New model loss: {loss_example9_new:.3f}\\n\"\n", "      f\"Updated model loss: {loss_example9_updated:.3f}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** Did the update to the model improve its performance on example 9 or make it worse? Why?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_updated_9\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "## Repeating the process\n", "\n", "Step 3 is to repeat the process for this and other training examples. We could recalculate the gradient and take another step to improve further, and take steps to improve the other training examples, and so on and so forth, eventually converging on a model that works well over the entire training set. But doing so manually in this way is too tedious. We need to be able to do these kinds of computations at scale. Fortunately, in the next lab we'll be turning to packages that allow specifying these larger-scale computations."]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "## Optional Questions\n", "The following questions are **optional**, you don't have to submit them.\n", "\n", "### Optional Question #1 ###\n", "In the figure of the logistic function, the logistic function looks to be radially symmetric. In particular, it appears that $\\sigma(x) = 1 - \\sigma(-x)$. (If so, we can use a simple thresholding for classificiation purposes, $\\sigma(x) > 0.5$ to capture $x > 0$.)\n", "\n", "Prove the identity $\\sigma(x) = 1 - \\sigma(-x)$.\n", "\n", "<!--\n", "BEGIN QUESTION\n", "manual: true\n", "name: open_response_sigma_identity\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "<!-- BEGIN QUESTION -->\n", "\n", "## Optional Question #2 - Deriving the gradient of the cross-entropy loss\n", "\n", "\n", "Jurafsky and Martin provide a derivation for the cross-entropy loss in their section 5.8. There, they note that the derivation relies on the salutary fact that the derivative of the sigmoid has an especially simple form:\n", "\n", "$$ \\frac{d \\sigma(z)}{dz} = \\sigma(z) \\cdot (1 - \\sigma(z)) $$\n", "\n", "**Question:** Prove this identity.\n", "\n", "> **Hint:** You may find some of the following standard formulas for the derivative of various functions \u2013 reviewed from your calculus course \u2013 useful. (In these schematic identities, $u$ and $v$ are metavariables over functions of $z$, and $a$ and $n$ are metavariables over constants.)\n", "\n", "\n", "\\begin{align*}\n", "\\frac{d}{dz} a &= 0 \\\\\n", "\\frac{d}{dz} (u + v) &= \\frac{du}{dz} + \\frac{dv}{dz} \\\\\n", "\\frac{d}{dz} (u\\,v) &= v\\frac{du}{dz} + u\\frac{dv}{dz} \\\\\n", "\\frac{d}{dz} \\left[\\frac{1}{u}\\right] &= - \\frac{1}{u^2} \\frac{du}{dz} \\\\\n", "\\frac{d}{dz} u^n &= n u^{n-1} \\frac{du}{dz} \\\\\n", "\\frac{d}{dz} e^u &= e^u \\frac{du}{dz} \\\\\n", "\\frac{d}{dz} \\log_a u &= (\\log_a e) \\frac{1}{u} \\frac{du}{dz} \n", "\\end{align*}\n", "\n", "\n", "<!--\n", "BEGIN QUESTION\n", "manual: true\n", "name: open_response_sigmoid_gradient\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "<!-- BEGIN QUESTION -->\n", "\n", "## Lab debrief \u2013 for consensus submission only\n", "\n", "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n", "\n", "* Was the lab too long or too short?\n", "* Were the readings appropriate for the lab? \n", "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n", "* Are there additions or changes you think would make the lab better?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_debrief\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "# End of lab 1-4"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["---\n", "\n", "To double-check your work, the cell below will rerun all of the autograder tests."]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check_all()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.3"}, "title": "CS187 Lab 1-4: Discriminative methods for classification"}, "nbformat": 4, "nbformat_minor": 4}