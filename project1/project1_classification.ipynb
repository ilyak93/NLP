{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of project1_classification.ipynb","provenance":[{"file_id":"https://github.com/cs236299-2020/project1/blob/master/project1_classification.ipynb","timestamp":1607006901423}],"collapsed_sections":["drbeoB66kJLd","5HCgGp4ACIvL","go2q9-vd6RO7","MDs3A17NqEZT","8lo119NGqEZV","OktPO1mQqEZV","VGLxXlAaqEZV","p7UkAap1qEZV"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"title":"CS187 -- Project Segment 1 // Text Classification"},"cells":[{"cell_type":"code","metadata":{"deletable":false,"editable":false,"jupyter":{"outputs_hidden":true,"source_hidden":true},"id":"V02Qt0p4qEZO","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607115339196,"user_tz":-120,"elapsed":3927,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"c0134fe6-9f92-43c1-939b-a9039ee9490a"},"source":["# Please do not change this cell because some hidden tests might depend on it.\n","import os\n","\n","# Otter grader does not handle ! commands well, so we define and use our\n","# own function to execute shell commands.\n","def shell(commands, warn=True):\n","    \"\"\"Executes the string `commands` as a sequence of shell commands.\n","     \n","       Prints the result to stdout and returns the exit status. \n","       Provides a printed warning on non-zero exit status unless `warn` \n","       flag is unset.\n","    \"\"\"\n","    file = os.popen(commands)\n","    print (file.read().rstrip('\\n'))\n","    exit_status = file.close()\n","    if warn and exit_status != None:\n","        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n","    return exit_status\n","\n","shell(\"\"\"\n","ls requirements.txt >/dev/null 2>&1\n","if [ ! $? = 0 ]; then\n"," rm -rf .tmp\n"," git clone https://github.com/cs236299-2020/project1.git .tmp\n"," mv .tmp/requirements.txt ./\n"," rm -rf .tmp\n","fi\n","pip install -q -r requirements.txt\n","\"\"\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"NsNDM7b8qEZQ","executionInfo":{"status":"ok","timestamp":1607115339199,"user_tz":-120,"elapsed":3914,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["# Initialize Otter\n","import otter\n","grader = otter.Notebook()"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sQeYA3AoqEZQ"},"source":["%%latex\n","\\newcommand{\\vect}[1]{\\mathbf{#1}}\n","\\newcommand{\\cnt}[1]{\\sharp(#1)}\n","\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n","\\newcommand{\\softmax}{\\operatorname{softmax}}\n","\\newcommand{\\Prob}{\\Pr}\n","\\newcommand{\\given}{\\,|\\,}"]},{"cell_type":"markdown","metadata":{"id":"ReEz7Xz7qEZQ"},"source":["$$\n","\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n","\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n","\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n","\\renewcommand{\\softmax}{\\operatorname{softmax}}\n","\\renewcommand{\\Prob}{\\Pr}\n","\\renewcommand{\\given}{\\,|\\,}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"JyMdovPAKAHU"},"source":["# Course 236299\n","\n","## Project segment 1: Text classification\n","\n","In this project segment you will build several varieties of text classifiers using PyTorch.\n","\n","1. A majority baseline.\n","2. A naive Bayes classifer.\n","3. A logistic regression classifier.\n","4. A multilayer perceptron classifier."]},{"cell_type":"markdown","metadata":{"id":"NWC8tn80qEZQ"},"source":["## Preparation"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"S7Zdp101qEZR","executionInfo":{"status":"ok","timestamp":1607115339626,"user_tz":-120,"elapsed":4333,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["import copy\n","import re\n","import torch\n","import torch.nn as nn\n","import torchtext as tt\n","\n","from collections import Counter\n","from torch import optim\n","from tqdm import tqdm"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"kdi-spgB0sEi","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607115339627,"user_tz":-120,"elapsed":4322,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"45db7a6f-f3b3-4129-cf84-f4cf8f42abc4"},"source":["# Random seed\n","random_seed = 1234\n","torch.manual_seed(random_seed)\n","\n","## GPU check\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["cpu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"drbeoB66kJLd"},"source":["## The task: answer types for ATIS queries\n","\n","For this and future project segments, you will be working with a standard natural-language-processing dataset, the [ATIS (Airline Travel Information System) dataset](https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk). This dataset is composed of queries about flights – their dates, times, locations, airlines, and the like. Over the years, the dataset has been annotated in all kinds of ways, with parts of speech, informational chunks, parse trees, and even corresponding SQL database queries. You'll use various of these annotations in future assignments. For this project segment, however, you'll pursue an easier classification task: **given a query, predict the answer type**.\n","\n","Below is an example taken from this dataset:\n","\n","_Query:_\n","\n","```\n","show me the afternoon flights from washington to boston\n","```\n","\n","_SQL:_\n","\n","```\n","SELECT DISTINCT flight_1.flight_id FROM flight flight_1 , airport_service airport_service_1 , city city_1 , airport_service airport_service_2 , city city_2 \n","   WHERE flight_1.departure_time BETWEEN 1200 AND 1800 \n","     AND ( flight_1.from_airport = airport_service_1.airport_code \n","           AND airport_service_1.city_code = city_1.city_code \n","           AND city_1.city_name = 'WASHINGTON' \n","           AND flight_1.to_airport = airport_service_2.airport_code \n","           AND airport_service_2.city_code = city_2.city_code \n","           AND city_2.city_name = 'BOSTON' )\n","```\n","\n","In this problem set, we will consider the answer type for a natural-language query to be the target field of the corresponding SQL query. For the above example, **the answer type would be *flight_id***."]},{"cell_type":"markdown","metadata":{"id":"l5NGDak-qEZS"},"source":["## Loading and preprocessing the data\n","\n","> Read over this section, executing the cells, and **making sure you understand what's going on before proceeding to the next parts.**\n","\n","First, let's download the dataset."]},{"cell_type":"code","metadata":{"id":"zGVWcvlk080Q","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607115342132,"user_tz":-120,"elapsed":6810,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"78a0e002-83e4-463e-a67a-752218d215ba"},"source":["data_dir = \"https://raw.githubusercontent.com/nlp-236299/data/master/ATIS/\"\n","for file in [\"train.nl\",\n","             \"train.sql\",\n","             \"dev.nl\",\n","             \"dev.sql\",\n","             \"test.nl\",\n","             \"test.sql\"]:\n","    shell(f\"wget -nv -N -P data {data_dir}{file}\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["\n","\n","\n","\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tPJ6ihGH1Oz8"},"source":["We use `torchtext` to prepare the data, as in lab 1-5. More information on `torchtext` can be found at https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/.\n","\n","To begin, `torchtext` requires that we define a mapping from the raw data to featurized indices, called a [`Field`](https://torchtext.readthedocs.io/en/latest/data.html#fields). We need one field for processing the question (`TEXT`), and another for processing the label (`LABEL`). These fields make it easy to map back and forth between readable data and lower-level representations like numbers."]},{"cell_type":"code","metadata":{"id":"n0XvgL1X6GiK","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607115342134,"user_tz":-120,"elapsed":6795,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"4ccc3a03-660c-4479-b64d-9e250c1946d1"},"source":["TEXT = tt.data.Field(lower=True,            # lowercase all tokens\n","                     sequential=True,       # sequential data\n","                     include_lengths=False, # do not include lengths\n","                     batch_first=True,      # batches will be batch_size X max_len\n","                     tokenize=tt.data.get_tokenizer(\"basic_english\")) \n","LABEL = tt.data.Field(batch_first=True, sequential=False, unk_token=None)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"fTkktPQa13yJ"},"source":["We provide an interface for loading ATIS data, built on top of [`torchtext.data.Dataset`](https://pytorch.org/text/data.html#torchtext.data.Dataset). "]},{"cell_type":"code","metadata":{"id":"qiZRD-ua1Jfm","executionInfo":{"status":"ok","timestamp":1607115342136,"user_tz":-120,"elapsed":6784,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["class ATIS(tt.data.Dataset):\n","  @staticmethod\n","  def sort_key(ex):\n","    return len(ex.text)\n","\n","  def __init__(self, path, text_field, label_field, **kwargs):\n","    \"\"\"Creates an ATIS dataset instance given a path and fields.\n","    Arguments:\n","        path: Path to the data file\n","        text_field: The field that will be used for text data.\n","        label_field: The field that will be used for label data.\n","        Remaining keyword arguments: Passed to the constructor of\n","            tt.data.Dataset.\n","    \"\"\"\n","    fields = [('text', text_field), ('label', label_field)]\n","    \n","    examples = []\n","    # Get text\n","    with open(path+'.nl', 'r') as f:\n","        for line in f:\n","            ex = tt.data.Example()\n","            ex.text = text_field.preprocess(line.strip()) \n","            examples.append(ex)\n","    \n","    # Get labels\n","    with open(path+'.sql', 'r') as f:\n","        for i, line in enumerate(f):\n","            label = self._get_label_from_query(line.strip())\n","            examples[i].label = label\n","            \n","    super(ATIS, self).__init__(examples, fields, **kwargs)\n","  \n","  def _get_label_from_query(self, query):\n","    \"\"\"Returns the answer type from `query` by dead reckoning.\n","    It's basically the second or third token in the SQL query.\n","    \"\"\"    \n","    match = re.match(r'\\s*SELECT\\s+(DISTINCT\\s*)?(\\w+\\.)?(?P<label>\\w+)', query)\n","    if match:\n","        label = match.group('label')\n","    else:\n","        raise RuntimeError(f'no label in query {query}')\n","    return label\n","\n","  @classmethod\n","  def splits(cls, text_field, label_field, path='./',\n","              train='train', validation='dev', test='test',\n","              **kwargs):\n","    \"\"\"Create dataset objects for splits of the ATIS dataset.\n","    \n","    Arguments:\n","        text_field: The field that will be used for the sentence.\n","        label_field: The field that will be used for label data.\n","        root: The root directory that the dataset's zip archive will be\n","            expanded into; therefore the directory in whose trees\n","            subdirectory the data files will be stored.\n","        train: The filename of the train data. Default: 'train.txt'.\n","        validation: The filename of the validation data, or None to not\n","            load the validation set. Default: 'dev.txt'.\n","        test: The filename of the test data, or None to not load the test\n","            set. Default: 'test.txt'.\n","        Remaining keyword arguments: Passed to the splits method of\n","            Dataset.\n","    \"\"\"\n","\n","    train_data = None if train is None else cls(\n","        os.path.join(path, train), text_field, label_field, **kwargs)\n","    val_data = None if validation is None else cls(\n","        os.path.join(path, validation), text_field, label_field, **kwargs)\n","    test_data = None if test is None else cls(\n","        os.path.join(path, test), text_field, label_field, **kwargs)\n","    return tuple(d for d in (train_data, val_data, test_data)\n","                  if d is not None)\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bGx_0Dk_d7fc"},"source":["We split the data into training, validation, and test corpora, and build the vocabularies from the training data."]},{"cell_type":"code","metadata":{"id":"aetkuF_1d1nb","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607119577470,"user_tz":-120,"elapsed":1696,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"eac438b3-44e4-449b-b230-219ba9754349"},"source":["# Make splits for data\n","train_data, val_data, test_data = ATIS.splits(TEXT, LABEL, path='./data/')\n","\n","# Build vocabulary for data fields\n","MIN_FREQ = 3 # words appearing less than 3 times are treated as 'unknown'\n","TEXT.build_vocab(train_data, min_freq=MIN_FREQ)\n","LABEL.build_vocab(train_data)\n","\n","# Compute size of vocabulary\n","vocab_size = len(TEXT.vocab.itos)\n","num_labels = len(LABEL.vocab.itos)\n","print(f\"Size of vocab: {vocab_size}\")\n","print(f\"Number of labels: {num_labels}\")"],"execution_count":130,"outputs":[{"output_type":"stream","text":["Size of vocab: 512\n","Number of labels: 30\n","['<unk>', '<pad>', 'to', 'from', 'flights', 'the', 'on', 'what', 'me', 'flight', 'boston', 'show', 'i', 'san', 'denver', 'a', 'in', 'francisco', 'atlanta', 'and', 'pittsburgh', 'is', 'dallas', 'baltimore', 'all', 'philadelphia', 'like', \"'\", 'are', 'list', 'airlines', 'of', 'that', 'between', 'washington', 'leaving', 'please', 'morning', 'would', 'fare', 'fly', 'for', 'first', 'oakland', 'after', 'there', 'wednesday', 'd', 'ground', 'cheapest', 'you', 'transportation', 'does', 'class', 'need', 'trip', 'city', 'arriving', 'round', 'available', 'have', 'before', 'with', 'afternoon', 'which', 'one', 'fares', 'way', 'american', 'new', 'leave', 'at', 'give', 'monday', 'want', 'dc', 'york', 'earliest', 'nonstop', 'thursday', 'arrive', 'united', 'go', 'information', 'tuesday', 'can', 'airport', 'find', 'how', '.', 'st', 'evening', 'twenty', 'newark', 'noon', 'miami', 'milwaukee', 'delta', 'sunday', 'any', 'august', 'vegas', 'charlotte', 'las', 's', 'continental', 'do', 'o', 'stop', 'chicago', 'clock', 'toronto', 'diego', 'july', 'orlando', 'seventh', 'airline', 'friday', 'fort', 'saturday', 'worth', 'next', 'us', 'air', 'early', 'phoenix', 'houston', 'indianapolis', 'seattle', 'tell', 'kansas', 'code', 'latest', 'tomorrow', 'aircraft', 'downtown', 'angeles', 'lake', 'los', 'salt', 'cleveland', 'cost', 'going', 'around', 'stopover', 'see', 'montreal', 'may', '5pm', 'june', 'about', 'petersburg', 'by', 'get', 'm', 'memphis', 'an', 'many', 'mean', 'ticket', 'twa', 'expensive', 'minneapolis', 'tampa', 'departing', 'leaves', 'long', 'type', 'jose', 'or', 'could', 'know', 'nashville', 'travel', 'daily', 'dollars', 'international', 'louis', 'much', 'okay', 'tacoma', 'than', '10am', '12', '6pm', 'beach', 'cincinnati', 'time', 'book', 'least', 'depart', 'into', 'meal', 'service', 'california', 'coach', 'detroit', 'economy', 'last', 'november', 'northwest', 'serve', '7pm', 'columbus', 'lowest', 'second', 'used', 'day', 'less', 'night', 'september', 'pm', 'flying', 'general', 'mitchell', 'paul', 'serves', 'kind', 'now', 'third', '5', 'am', 'arrives', 'breakfast', 'field', 'it', 'love', 'be', 'december', 'return', 'as', 'direct', 'make', 'out', 'possible', 'schedule', 'stops', 'times', 'april', 'burbank', 'car', 'la', 'airports', 'connecting', 'has', 'interested', 'late', 'looking', 'number', 'restriction', 'take', '1991', '8pm', 'business', 'dinner', 'fourth', 'goes', 'price', 'stopping', '2pm', '4pm', '6', 'eastern', 'eighth', 'fifth', 'ontario', 'will', 'dl', 'this', 'airfare', 'also', 'arrangements', 'most', 'plane', 'using', 'via', 'your', '1000', '8', 'fifteenth', 'prices', 'rental', 'twentieth', 'week', 'westchester', '4', '8am', '9am', 'display', 'ninth', 'returning', 'sixth', 'stand', 'thirtieth', 'through', 'county', 'flies', 'listing', 'today', 'ua', 'wednesdays', 'again', 'bwi', 'only', 'other', 'tenth', 'types', 'carolina', 'express', 'f', 'midwest', 'north', 'qx', 'smallest', 'tickets', 'where', '12pm', 'ap/57', 'back', 'classes', 'eleventh', 'florida', 'h', 'jersey', 'limousine', 'lunch', 'meals', 'served', 'should', 'twelfth', 'two', '10pm', 'capacity', 'cities', 'colorado', 'guardia', 'hi', 'makes', 'mco', 'october', 'right', 'same', 'seating', 'shortest', 'under', '9', 'dfw', 'either', 'explain', 'january', 'landings', 'layover', 'march', 'my', 'nineteenth', 'seventeenth', 'some', 'thank', 'then', 'transport', 'y', '3', '3pm', '466', '7', 'airplane', 'canadian', 'codes', 'departure', 'distance', 'ewr', 'fourteenth', 'hours', 'hp', 'if', 'logan', 'over', 'planes', 're', 'sixteenth', 'thirty', 'when', '1', '10', '1pm', '2', '281', '9pm', 'abbreviation', 'arrange', 'both', 'canada', 'each', 'february', 'let', 'no', 'people', 'q', 'serving', 'takeoffs', 'trying', 'use', 'uses', 'yn', '747', 'anywhere', 'arrival', 'booking', 'but', 'later', 'live', 'mornings', 'name', 'nationair', 'numbers', 'ohio', 'provided', 'qo', 'qw', 'rent', 'sfo', 'southwest', 'stopovers', 'their', 'thrift', 'traveling', 'yes', '1100', '1115am', '11am', '1245pm', 'ap57', 'area', 'arrivals', 'boeing', 'coming', 'departs', 'departures', 'destination', 'during', 'ea', 'eighteenth', 'far', 'ff', 'following', 'georgia', 'help', 'jfk', 'll', 'making', 'midnight', 'offer', 'passengers', 'pennsylvania', 'rates', 'requesting', 'these', 'total', 'tuesdays', '1291', '1992', '21', '2100', '296', '430pm', '630am', '718am', '7am', '825', '838', '934pm', 'another', 'ap80', 'cheap', 'choices', 'close', 'connect', 'costs', 'dc10', 'describe', 'fit', 'fn', 'great', 'hello', 'include', 'kinds', 'land', 'lufthansa', 'meaning', 'minnesota', 'more', 'near', 'noontime', 'nw', 'options', 'reservation', 'saturdays', 'seats', 'services', 'six', 'sometime', 'sorry', 'stands', 'sundays', 'taxi', 'tennessee', 'three', 'thursdays', 'trips', 'turboprop', 'we', 'weekday']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O6zHJlhQqEZT"},"source":["To get a sense of the kinds of things that are asked about in this dataset, here is the list of all of the answer types in the training data."]},{"cell_type":"code","metadata":{"id":"3kH1aoqrqEZT","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607115342623,"user_tz":-120,"elapsed":7240,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"73201dec-2f25-4385-a182-82678b40101e"},"source":["for i, label in enumerate(sorted(LABEL.vocab.itos)):\n","    print(f\"{i:2d} {label}\") "],"execution_count":9,"outputs":[{"output_type":"stream","text":[" 0 advance_purchase\n"," 1 aircraft_code\n"," 2 airline_code\n"," 3 airport_code\n"," 4 airport_location\n"," 5 arrival_time\n"," 6 basic_type\n"," 7 booking_class\n"," 8 city_code\n"," 9 city_name\n","10 count\n","11 day_name\n","12 departure_time\n","13 fare_basis_code\n","14 fare_id\n","15 flight_id\n","16 flight_number\n","17 ground_fare\n","18 meal_code\n","19 meal_description\n","20 miles_distant\n","21 minimum_connect_time\n","22 minutes_distant\n","23 restriction_code\n","24 state_code\n","25 stop_airport\n","26 stops\n","27 time_elapsed\n","28 time_zone_code\n","29 transport_type\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5HCgGp4ACIvL"},"source":["### Handling unknown words\n","\n","Note that we mapped words appearing fewer than 3 times to a special _unknown_ token (we're using the `torchtext` default, `<unk>`) for two reasons: \n","\n","1. Due to the scarcity of such rare words in training data, we might not be able to learn generalizable conclusions about them.\n","2. Introducing an unknown token allows us to deal with out-of-vocabulary words in the test data as well: we just map those words to `<unk>`."]},{"cell_type":"code","metadata":{"id":"Tr5Omf6yBTsI","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607115342623,"user_tz":-120,"elapsed":7229,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"1fceffb2-b980-4752-ba6a-566966d87660"},"source":["unk_token = TEXT.unk_token\n","print (f\"Unknown token: {unk_token}\")\n","unk_index = TEXT.vocab.stoi[unk_token]\n","print (f\"Unknown token id: {unk_index}\")\n","\n","# UNK example\n","example_unk_token = 'IAmAnUnknownWordForSure'\n","print (f\"An unknown token: {example_unk_token}\")\n","print (f\"Mapped back to word id: {TEXT.vocab.stoi[example_unk_token]}\")\n","print (f\"Mapped to <unk>?: {TEXT.vocab.stoi[example_unk_token] == unk_index}\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Unknown token: <unk>\n","Unknown token id: 0\n","An unknown token: IAmAnUnknownWordForSure\n","Mapped back to word id: 0\n","Mapped to <unk>?: True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"go2q9-vd6RO7"},"source":["### Batching the data\n","\n","To load data in batches, we use `data.BucketIterator`. This enables us to iterate over the dataset under a given `BATCH_SIZE` which specifies how many examples we want to process at a time."]},{"cell_type":"code","metadata":{"id":"QBVa2Krb5IcY","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607115342624,"user_tz":-120,"elapsed":7221,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"05757afa-c689-4120-aef1-9bad21514a69"},"source":["BATCH_SIZE = 32\n","train_iter = tt.data.BucketIterator(train_data, batch_size=BATCH_SIZE, device=device)\n","val_iter = tt.data.BucketIterator(val_data, batch_size=BATCH_SIZE, device=device)\n","test_iter = tt.data.Iterator(test_data, batch_size=BATCH_SIZE, sort=False, device=device)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"UBpXYa4h5g5I"},"source":["Let's look at a single batch from one of these iterators."]},{"cell_type":"code","metadata":{"id":"r6QHKuQ75bjd","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607115342626,"user_tz":-120,"elapsed":7215,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"4700e3d1-a955-4d52-8f0e-7081e15ed103"},"source":["batch = next(iter(train_iter))\n","text = batch.text\n","print (f\"Size of text batch: {text.size()}\")\n","print (f\"Third sentence in batch: {text[2]}\")\n","print (f\"Converted back to string: {' '.join([TEXT.vocab.itos[i] for i in text[2]])}\")\n","\n","label = batch.label\n","print (f\"Size of label batch: {label.size()}\")\n","print (f\"Third label in batch: {label[2]}\")\n","print (f\"Converted back to string: {LABEL.vocab.itos[label[2].item()]}\")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Size of text batch: torch.Size([32, 20])\n","Third sentence in batch: tensor([ 12,  38,  26,   2,  40,   3,  18,   2,  13,  17,  62,  15, 144,  16,\n","         22,   1,   1,   1,   1,   1])\n","Converted back to string: i would like to fly from atlanta to san francisco with a stopover in dallas <pad> <pad> <pad> <pad> <pad>\n","Size of label batch: torch.Size([32])\n","Third label in batch: 0\n","Converted back to string: flight_id\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"5iL-spuDoLDt"},"source":["You might notice some padding tokens `<pad>` when we convert word ids back to strings, or equivalently, padding ids `1` in the corresponding tensor. The reason why we need such padding is because the sentences in a batch might be of different lengths, and to save them in a 2D tensor for parallel processing, sentences that are shorter than the longest sentence need to be padded with some placeholder values. `torchtext` does all this for us automatically. Note that during training we need to make sure that the paddings do not affect the final results."]},{"cell_type":"code","metadata":{"id":"ifE_-aPo81x7","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607115342627,"user_tz":-120,"elapsed":7207,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"d7bc14eb-7cc3-41e2-a4a4-31912ac6f459"},"source":["padding_token = TEXT.pad_token\n","print (f\"Padding token: {padding_token}\")\n","\n","padding_id = TEXT.vocab.stoi[padding_token]\n","print (f\"Padding word id: {padding_id}\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Padding token: <pad>\n","Padding word id: 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H6rFmJcj-rgl"},"source":["Alternatively, we can also directly iterate over the individual examples in `train_data`, `val_data` and `test_data`. Here the returned values are the raw sentences and labels instead of their corresponding ids, and you might need to explicitly deal with the unknown words, unlike using bucket iterators which automatically map unknown words to an unknown word id."]},{"cell_type":"code","metadata":{"id":"Q2GGBhTF-5p0","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607115342629,"user_tz":-120,"elapsed":7200,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"cb39e3bb-3c08-4756-8b43-488c527f483e"},"source":["for example in train_iter.dataset[:5]: # train_iter.dataset is just train_data\n","  print(f\"{example.label:10} -- {' '.join(example.text)}\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["flight_id  -- list all the flights that arrive at general mitchell international from various cities\n","flight_id  -- give me the flights leaving denver august ninth coming back to boston\n","flight_id  -- what flights from tacoma to orlando on saturday\n","fare_id    -- what is the most expensive one way fare from boston to atlanta on american airlines\n","flight_id  -- what flights return from denver to philadelphia on a saturday\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MDs3A17NqEZT"},"source":["## Notations used\n","\n","In this project segment, we'll use the following notations.\n","\n","* Sequences of elements (vectors and the like) are written with angle brackets and commas ($\\langle w_1, \\ldots, w_M \\rangle$) or directly with no punctuation ($w_1 \\cdots w_M$).\n","* Sets are notated similarly but with braces, ($\\{ v_1, \\ldots, v_V \\}$).\n","* Maximum indices ($M$ and $V$ in the preceding examples) are written as uppercase italics.\n","* Variables over sequences and sets are written in boldface ($\\vect{w}$), typically with the same letter as the variables over their elements.\n","\n","In particular,\n","\n","* $\\vect{w} = w_1 \\cdots w_M$: A text to be classified, each element $w_j$ being a word token.\n","* $\\vect{v} = \\{ v_1, \\ldots, v_V\\}$: A vocabulary, each element $v_k$ being a word type.\n","* $\\vect{x} = \\langle x_1, \\ldots, x_X \\rangle$: Input features to a model.\n","* $\\vect{c} = \\{ c_1, \\ldots, c_N \\}$: The output classes of a model, each element $c_i$ being a class label.\n","* $\\vect{T} = \\langle \\vect{w}^{(1)}, \\ldots, \\vect{w}^{(T)} \\rangle$: The training corpus of texts.\n","* $\\vect{C} = \\langle c^{(1)}, \\ldots, c^{(T)} \\rangle$: The corresponding gold labels for the training examples in $T$."]},{"cell_type":"markdown","metadata":{"id":"CxDMbHJG9Qpg"},"source":["## Part 1: Establish a majority baseline\n","\n","A simple baseline for classification tasks is to always predict the most common class. \n","Given a training set of texts $\\vect{T}$ labeled by classes $\\vect{C}$, we classify an input text $\\vect{w} = w_1 \\cdots w_M$ as the class $c_i$ that occurs most frequently in the training data, that is, specified by\n","\n","$$ \\argmax{i} \\cnt{c_i} $$\n","\n","and thus ignoring the input entirely (!).\n","\n","**Implement the majority baseline and compute test accuracy using the starter code below.** Note that for this baseline, and the naive Bayes classifier later, we don't need to use the validation set since we don't tune any hyper-parameters."]},{"cell_type":"code","metadata":{"id":"Hd8XvBof6rVa","executionInfo":{"status":"ok","timestamp":1607115342630,"user_tz":-120,"elapsed":7193,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["# TODO\n","def majority_baseline_accuracy(train_iter, test_iter):\n","    \"\"\"Returns the most common label in the training set, and the accuracy of\n","       the majority baseline on the test set.\n","    \"\"\"\n","    \n","    most_common_label, _ = Counter(train_iter.dataset.label).most_common(1)[0]\n","    test_accuracy = sum([ex == most_common_label for ex in test_iter.dataset.label]) / len(test_iter.dataset)\n","    return most_common_label, test_accuracy"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t1X5W3rKqEZU"},"source":["How well does your classifier work? Let's see:"]},{"cell_type":"code","metadata":{"id":"vOC7A_34v1zB","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607115342631,"user_tz":-120,"elapsed":7182,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"b598835e-254f-4e16-8273-2fd1e5cb5ac5"},"source":["# Call the method to establish a baseline\n","most_common_label, test_accuracy = majority_baseline_accuracy(train_iter, test_iter)\n","\n","print(f'Most common label: {most_common_label}\\n'\n","      f'Test accuracy:     {test_accuracy:.3f}')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Most common label: flight_id\n","Test accuracy:     0.683\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dNbq_QvG_XGY"},"source":["## Part 2: Naive Bayes classifier\n","\n","\n","### Review of the naive Bayes method\n","\n","Recall from lab 1-3 that the Naive Bayes classification method classifies a text $\\vect{w} = \\langle w_1, w_2, \\ldots, w_M \\rangle$ as the class $c_i$ given by the following maximization:\n","\n","$$\n","\\argmax{i} \\Prob(c_i \\given \\vect{w}) \\approx \\argmax{i} \\Prob(c_i) \\cdot \\prod_{j=1}^m \\Prob(w_j \\given c_i)\n","$$\n","\n","or equivalently (since taking the log is monotonic)\n","\n","\\begin{align}\n","\\argmax{i} \\Prob(c_i \\given \\vect{w}) &= \\argmax{i} \\log\\Prob(c_i \\given \\vect{w}) \\\\\n","&\\approx \\argmax{i} \\left(\\log\\Prob(c_i) + \\sum_{j=1}^m \\log\\Prob(w_j \\given c_i)\\right)\n","\\end{align}\n","\n","All we need, then, to apply the Naive Bayes classification method is values for the various log probabilities: the priors $\\log\\Prob(c_i)$ and the likelihoods $\\log\\Prob(w_j \\given c_i)$, for each feature (word) $w_j$ and each class $c_i$.\n","\n","We can estimate the prior probabilities $\\Prob(c_i)$ by examining the empirical probability in the training set. That is, we estimate \n","\n","$$ \\Prob(c_i) \\approx \\frac{\\cnt{c_i}}{\\sum_j \\cnt{c_j}} $$\n","\n","We can estimate the likelihood probabilities $\\Prob(w_j \\given c_i)$ similarly by examining the empirical probability in the training set. That is, we estimate \n","\n","$$ \\Prob(w_j \\given c_i) \\approx \\frac{\\cnt{w_j, c_i}}{\\sum_{j'} \\cnt{w_{j'}, c_i}} $$\n","\n","To handle cases in which the count $\\cnt{w_j, c_i}$ is zero, we can adjust this estimate using add-$\\delta$ smoothing:\n","\n","$$ \\Prob(w_j \\given c_i) \\approx \\frac{\\cnt{w_j, c_i} + \\delta}{\\sum_{j'} \\cnt{w_{j'}, c_i} + \\delta \\cdot V} $$\n","\n","### Two conceptions of the naive Bayes method implementation\n","\n","We can store all of these parameters in different ways, leading to two different implementation conceptions. We review two conceptions of implementing the naive Bayes classification of a text $\\vect{w} = \\langle w_1, w_2, \\ldots, w_M \\rangle$, corresponding to using different representations of the input $\\vect{x}$ to the model: the index representation and the bag-of-words representation. \n","\n","Within each conception, the parameters of the model will be stored in one or more matrices. The conception dictates what operations will be performed with these matrices.\n","\n","#### Using the index representation\n","\n","In the first conception, we take the input elements $\\vect{x} = \\langle x_1, x_2, \\ldots, x_M \\rangle$ to be the _vocabulary indices_ of the words $\\vect{w} = w_1 \\cdots w_M$. That is, each word token $w_i$ is of the word type in the vocabulary $\\vect{v}$ at index $x_i$, so \n","\n","$$ v_{x_i} = w_i $$\n","\n","In this representation, the input vector has the same length as the word sequence.\n","\n","We think of the likelihood probabilities as forming a matrix, call it $\\vect{L}$, where the $i,j$-th element stores $\\log \\Prob(v_j \\given c_i)$. \n","\n","$$\\vect{L}_{ij} = \\log\\Prob(v_j \\given c_i)$$\n","\n","Similarly, for the priors, we'll have \n","\n","$$\\vect{P}_{i} = \\log\\Prob(c_i)$$\n","\n","Now the maximization can be implemented as \n","\n","\\begin{align}\n","\\argmax{i} \\log\\Prob(c_i) + \\sum_{j=1}^m \\log\\Prob(w_j \\given c_i)\n","&= \\argmax{i} \\vect{P}_i + \\sum_{j=1}^m \\vect{L}_{x_j i}\n","\\end{align}\n","\n","Implemented in this way, we see that the use of the inputs $x_i$ is as an _index_ into the likelihood matrix. \n","\n","#### Using the bag-of-words representation\n","\n","<img src=\"https://github.com/nlp-236299/data/raw/master/Resources/naive-bayes-figure.png\" width=400 align=right />\n","\n","Notice that since each word in the input is treated separately, the order of the words doesn't matter. Rather, all that matters is how frequently each word type occurs in a text. Consequently, we can use the bag-of-words representation introduced in lab 1-1.\n","\n","Recall that the bag-of-words representation of a text is just its frequency distribution over the vocabulary, which we will notate $bow(\\vect{w})$. Given a vocabulary of word types $\\vect{v} = \\langle v_1, v_2, \\ldots, v_V \\rangle$, the representation of a sentence $\\vect{w} = \\langle w_1, w_2, \\ldots, w_M \\rangle$ is a vector $\\vect{x}$ of size $V$, where \n","\n","$$\\begin{aligned}\n","bow(\\vect{w})_j &= \\sum_{i=1}^M 1[w_i = v_j] & \\mbox{for $1 \\leq j \\leq V$}\n","\\end{aligned}$$\n","\n","We write $1[w_i = v_j]$ to indicate 1 if $w_i = v_j$ and 0 otherwise. For convenience, we'll add an extra $(V+1)$-st element to the end of the bag-of-words vector, a single $1$ whose use will be clear shortly. That is,\n","\n","$$bow(\\vect{w})_{V+1} = 1$$\n","\n","Under this conception, then, we'll take the input $\\vect{x}$ to be $bow(\\vect{w})$. Instead of the input having the same length as the text, it has the same length as the vocabulary.\n","\n","As described in lecture, represented in this way, the quantity to be maximized in the naive Bayes method\n","\n","$$\\log\\Prob(c_i) + \\sum_{j=1}^M \\log\\Prob(w_j \\given c_i)$$\n","\n","can be calculated as \n","\n","$$\\log\\Prob(c_i) + \\sum_{j=1}^V x_j \\cdot \\log\\Prob(v_j \\given c_i)$$\n","\n","which is just $\\vect{U} \\vect{x}$ for a suitable choice of $N \\times (V+1)$ matrix $\\vect{U}$, namely\n","\n","$$ \\vect{U}_{ij} = \\left\\{\n","    \\begin{array}{ll}\n","        \\log \\Prob(v_j \\given c_i) & \\mbox{$1 \\leq i \\leq N$ and $1 \\leq j \\leq V$} \\\\\n","        \\log \\Prob(c_i) & \\mbox{$1 \\leq i \\leq N$ and $j = V+1$} \n","    \\end{array} \\right.\n","$$\n","\n","Under this implementation conception, we've reduced naive Bayes calculations to a single matrix operation. This conception is depicted in the figure at right.\n","\n","You are free to use either conception in your implementation of naive Bayes."]},{"cell_type":"markdown","metadata":{"id":"bJDEXnaESogl"},"source":["### Implement a naive Bayes classifier\n"," \n","For the implementation, we ask you to implement a Python class `NaiveBayes` that will have (at least) the following three methods:\n","\n","1. `__init__`: An initializer that takes two `torchtext` fields providing descriptions of the text and label aspects of examples.\n","\n","2. `train`: A method that takes a training data iterator and estimates all of the log probabilities $\\log\\Prob(c_i)$ and $\\log\\Prob(x_j \\given c_i)$ as described above. Perform add-$\\delta$ smoothing with $\\delta=1$. These parameters will be used by the `evaluate` method to evaluate a test dataset for accuracy, so you'll want to store them in some data structures in objects of the class.\n","\n","3. `evaluate`: A method that takes a test data iterator and evaluates the accuracy of the trained model on the test set.\n","\n","You can organize your code using either of the conceptions of Naive Bayes described above.\n","\n","You should expect to achieve about an **86% test accuracy** on the ATIS task."]},{"cell_type":"code","metadata":{"id":"SwSLwSEO2uyw","executionInfo":{"status":"ok","timestamp":1607115342632,"user_tz":-120,"elapsed":7170,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["class NaiveBayes():\n","  def __init__ (self, text, label):\n","    self.text = text\n","    self.label = label\n","    self.padding_id = text.vocab.stoi[text.pad_token]\n","    self.V = len(text.vocab.itos) # vocabulary size\n","    self.N = len(label.vocab.itos) # the number of classes\n","    # TODO: Add your code here\n","    self.U = torch.ones((self.N, self.V+1))\n","  \n","  def train(self, iterator):\n","    \"\"\"Calculates and stores log probabilities for training dataset `iterator`.\"\"\"\n","    word_to_index = self.text.vocab.stoi\n","    label_to_index = self.label.vocab.stoi\n","    for ex in iterator.dataset:\n","        for word in ex.text:\n","            self.U[label_to_index[ex.label], word_to_index[word]]+=1\n","    total_words = torch.sum(self.U)\n","    for i in range(self.N):\n","        self.U[i,-1] = torch.log2(torch.sum(self.U[i,:])/total_words)\n","        self.U[i,:-1] = torch.log2(self.U[i,:-1]/torch.sum(self.U[i,:-1]))\n","\n","\n","  def evaluate(self, iterator):\n","    \"\"\"Returns the model's performance on a given dataset `iterator`.\"\"\"\n","    correct = 0\n","    word_to_index = self.text.vocab.stoi\n","    labels = self.label.vocab.itos\n","    for index, ex in enumerate(iterator.dataset):\n","        x = torch.zeros(self.V+1)\n","        x[-1]=1\n","        counts = Counter(ex.text)\n","        for word,count in counts.items():\n","            index = word_to_index[word]\n","            x[index] = count\n","        y_hat = labels[torch.argmax(self.U@x)]\n","        correct+=y_hat==ex.label\n","    total = len(iterator.dataset)\n","    return correct/total "],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"WOO52qZMv1zS","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607116904532,"user_tz":-120,"elapsed":2490,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"76a38b08-f9d7-400c-df87-d2f3d262e6a0"},"source":["# Instantiate and train classifier\n","nb_classifier = NaiveBayes(TEXT, LABEL)\n","nb_classifier.train(train_iter)\n","\n","# Evaluate model performance\n","print(f'Training accuracy: {nb_classifier.evaluate(train_iter):.3f}\\n'\n","      f'Test accuracy:     {nb_classifier.evaluate(test_iter):.3f}')"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Training accuracy: 0.904\n","Test accuracy:     0.875\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-pH4ph3uHnvD"},"source":["## Part 3: Logistic regression classifier\n","\n","In this part, you'll complete a PyTorch implementation of a logistic regression (equivalently, a single layer perceptron) classifier. We review logistic regression here highlighting the similarities to the matrix-multiplication conception of naive Bayes. Thus, we take the input $\\vect{x}$ to be the bag-of-words representation $bow(\\vect{w})$. But as before you are free to use either implementation approach.\n","\n","### Review of logistic regression\n","\n","Similar to naive Bayes, in logistic regression, we assign a probability to a text $\\vect{x}$ by merely multiplying an $N \\times V$ matrix $\\vect{U}$ by it. However, we don't stipulate that the values in the matrix $\\vect{U}$ be estimated from the training corpus in the \"naive Bayes\" manner. Instead, we allow them to take on any value, using a training regime to select good values.\n","\n","In order to make sure that the output of the matrix multiplication $\\vect{U}\\vect{x}$ is mapped onto a probability distribution, we apply a nonlinear function to renormalize the values. We use the softmax function, a generalization of the sigmoid function from lab 1-4, defined by \n","\n","$$\\softmax(\\vect{z})_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{N} \\exp(z_j)}$$\n","\n","for each of the indices $i$ from $1$ to $N$.\n","\n","In summary, we model $\\Prob (c \\given \\vect{x})$ as\n","\n","$$ \\Prob(c_i \\given \\vect{x}) = \\softmax ( \\vect{U} \\vect{x} )_i $$\n","\n","<img src=\"https://github.com/nlp-236299/data/raw/master/Resources/logistic-regression-figure.png\" alt=\"logistic regression illustration\" width=\"400\"  align=right />\n","\n","The calculation of $\\Prob(c \\given \\vect{x})$ for each text $\\vect{x}$ is referred to as the _forward_ computation. In summary, the forward computation for logistic regression involves a linear calculation ($\\vect{U} \\vect{x}$) followed by a nonlinear calculation ($\\softmax$). We think of the perceptron (and more generally many of these neural network models) as transforming from one representation to another. A perceptron performs a linear transformation from the index or bag-of-words representation of the text to a representation as a vector, followed by a nonlinear transformation, a softmax or sigmoid, giving a representation as a probability distribution over the class labels. This single-layer perceptron thus involves two _sublayers_. (In the next part of the problem set, you'll experiment with a multilayer perceptron, with two perceptron layers, and hence four sublayers.)\n","\n","The loss function you'll use is the negative log probability $-\\log \\Prob (c \\given \\vect{x})$. The negative is used, since it is convention to minimize loss, whereas we want to maximize log likelihood. \n","\n","The forward and loss computations are illustrated in the figure at right. In practice, for numerical stability reasons, PyTorch absorbs the softmax operation into the loss function `nn.CrossEntropyLoss`. That is, the input to the `nn.CrossEntropyLoss` function is the vector of sums $\\vect{U} \\vect{x}$ (the last step in the box marked \"your job\" in the figure) rather than the vector of probabilities $\\Prob(c \\given \\vect{x})$. That makes things easier for you (!), since you're responsible only for the first sublayer.\n","\n","Given a forward computation, the weights can then be adjusted by taking a step opposite to the gradient of the loss function. Adjusting the weights in this way is referred to as the _backward_ computation. Fortunately, `torch` takes care of the backward computation for you, just as in lab 1-5.\n","\n","The optimization process of performing the forward computation, calculating the loss, and performing the backward computation to improve the weights is done repeatedly until the process converges on a (hopefully) good set of weights. You'll find this optimization process in the `train_all` method that we've provided. The trained weights can then be used to perform classification on a test set. See the `evaluate` method.\n","\n","You'll be responsible for implementing the forward computation as a method `forward`. We have provided code for performing the optimization and evaluation (though you should feel free to change them). "]},{"cell_type":"markdown","metadata":{"id":"KEgCwQVHrVw0"},"source":["### Implement a logistic regression classifier\n","\n","For the implementation, we ask you to implement a logistic regression classifier as a subclass of the [`torch.nn` module](https://pytorch.org/docs/stable/nn.html). You will be adding the following two methods:\n","\n","1. `__init__`: an initializer that takes two `torchtext` fields providing descriptions of the text and label aspects of examples.\n","\n","    During initialization, you'll want to define a [tensor](https://pytorch.org/docs/stable/tensors.html#torch-tensor) of weights, [initialized randomly](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_), which plays the role of $\\vect{U}$. The elements of this tensor are the [parameters](https://pytorch.org/docs/master/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter) of the `torch.nn` instance in the following special technical sense: It is the parameters of the module whose gradients will be calculated and whose values will be updated. Alternatively, you might find it easier to use the [`nn.Embedding` module](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) which is a wrapper to the weight tensor with a lookup implementation.\n","\n","2. `forward`: given a text batch of size `batch_size X max_length`, return a tensor of logits of size `batch_size X num_labels`. That is, for each text $\\vect{x}$ in the batch and each label $c$, you'll be calculating $\\vect{U}\\vect{x}$ as shown in the figure, returning a tensor of these values. Note that the softmax operation is absorbed into [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) so you won't need to deal with that.\n","\n","Some things to consider:\n","\n","1. The parameters of the model, the weights, need to be initialized properly. We suggest initializing them to some small random values. See [`torch.uniform_`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_).\n","\n","2. You'll want to make sure that padding tokens are handled properly. What should the weight be for the padding token?\n","\n","3. In extracting the proper weights to sum up, based on the word types in a sentence, we are essentially doing a lookup operation. You might find [`nn.Embedding`](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) or [`torch.gather`](https://pytorch.org/docs/stable/generated/torch.gather.html#torch-gather) useful.\n","\n","You should expect to achieve about **90%** accuracy on the ATIS classificiation task. "]},{"cell_type":"code","metadata":{"id":"dBdFcvg-PYBo","executionInfo":{"status":"ok","timestamp":1607121406995,"user_tz":-120,"elapsed":986,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["class LogisticRegression(nn.Module):\n","  def __init__ (self, text, label):\n","    super().__init__()\n","    self.text = text\n","    self.label = label\n","    self.padding_id = text.vocab.stoi[text.pad_token]\n","    # Keep the vocabulary sizes available\n","    self.N = len(label.vocab.itos) # num_classes\n","    self.V = len(text.vocab.itos)  # vocab_size\n","    # Specify cross-entropy loss for optimization\n","    self.criterion = nn.CrossEntropyLoss()\n","    # TODO: Create and initialize a tensor for the weights,\n","    #       or create an nn.Embedding module and initialize\n","    W = torch.rand((self.V+1, self.N), requires_grad=True)\n","    param = torch.nn.Parameter(W)\n","    self.register_parameter(name='W', param=param)\n","\n","\n","\n","\n","  def forward(self, text_batch):\n","    # TODO: Calculate the logits for the `text_batch`, \n","    #       returning a tensor of size batch_size x num_labels\n","    batch_size, _ = text_batch.shape\n","    bows_batch = torch.zeros(batch_size, self.V+1).long()\n","    for ex_ind, ex in enumerate(text_batch):\n","        counts = Counter(ex)\n","        bows_indices, bows_counts  = list(counts.keys()), list(counts.values())\n","        bows_batch[ex_ind, bows_indices] = torch.Tensor(bows_counts).long()\n","    bows_batch[:, -1:] = torch.Tensor(1).long()\n","    logits = bows_batch.float()@self.W\n","    return logits\n","\n","  def train_all(self, train_iter, val_iter, epochs=8, learning_rate=3e-3):\n","    # Switch the module to training mode\n","    self.train()\n","    # Use Adam to optimize the parameters\n","    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n","    best_validation_accuracy = -float('inf')\n","    best_model = None\n","    # Run the optimization for multiple epochs\n","    for epoch in range(epochs):\n","      c_num = 0\n","      total = 0\n","      running_loss = 0.0\n","      for batch in tqdm(train_iter):\n","        # Zero the parameter gradients\n","        optim.zero_grad()\n","\n","        # Input and target\n","        text = batch.text           # a tensor of shape (bsz, max_len)\n","        logits = self.forward(text) # perform the forward computation\n","        target = batch.label.long() # bsz\n","        batch_size = len(target)\n","\n","        # Compute the loss\n","        #print(target.shape)\n","        #print(target)\n","        #print()\n","        loss = self.criterion(logits, target)\n","\n","        # Perform backpropagation\n","        #print(list(self.parameters()))\n","        loss.backward()\n","        optim.step()\n","        #print(list(self.parameters()))\n","\n","        # Prepare to compute the accuracy\n","        predictions = torch.argmax(logits, dim=1)\n","        total += batch_size\n","        c_num += (predictions == target).float().sum().item()        \n","        running_loss += loss.item() * batch_size\n","\n","      # Evaluate and track improvements on the validation dataset\n","      validation_accuracy = self.evaluate(val_iter)\n","      if validation_accuracy > best_validation_accuracy:\n","        best_validation_accuracy = validation_accuracy\n","        self.best_model = copy.deepcopy(self.state_dict())\n","      epoch_loss = running_loss / total\n","      epoch_acc = c_num / total\n","      print (f'Epoch: {epoch} Loss: {epoch_loss:.4f} '\n","             f'Training accuracy: {epoch_acc:.4f} '\n","             f'Validation accuracy: {validation_accuracy:.4f}')\n","\n","  def evaluate(self, iterator):\n","    self.eval()   # switch the module to evaluation mode\n","    total = 0     # running total of example\n","    c_num = 0     # running total of correctly classified examples\n","    for batch in tqdm(iterator):\n","      text = batch.text\n","      logits = self.forward(text)                 # calculate forward probabilities\n","      target = batch.label.long()                 # extract gold labels\n","      predictions = torch.argmax(logits, dim=-1)  # calculate predicted labels\n","      total += len(target)\n","      c_num += (predictions == target).float().sum().item()\n","    return c_num / total"],"execution_count":168,"outputs":[]},{"cell_type":"code","metadata":{"id":"6wW6UcFqv1zp","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607121432502,"user_tz":-120,"elapsed":11973,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"4600a032-7b06-42f2-f307-ffbfd352d249"},"source":["# Instantiate the logistic regression classifier and run it\n","model = LogisticRegression(TEXT, LABEL).to(device) \n","model.train_all(train_iter, val_iter)\n","model.load_state_dict(model.best_model)\n","test_accuracy = model.evaluate(test_iter)\n","print (f'Test accuracy: {test_accuracy:.4f}')"],"execution_count":171,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/137 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","100%|██████████| 137/137 [00:00<00:00, 177.55it/s]\n","100%|██████████| 16/16 [00:00<00:00, 216.13it/s]\n"," 12%|█▏        | 17/137 [00:00<00:00, 168.12it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0 Loss: 1.7894 Training accuracy: 0.5638 Validation accuracy: 0.7251\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137/137 [00:00<00:00, 183.32it/s]\n","100%|██████████| 16/16 [00:00<00:00, 204.77it/s]\n"," 12%|█▏        | 17/137 [00:00<00:00, 163.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 Loss: 0.8345 Training accuracy: 0.8059 Validation accuracy: 0.8045\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137/137 [00:00<00:00, 169.83it/s]\n","100%|██████████| 16/16 [00:00<00:00, 208.36it/s]\n"," 12%|█▏        | 17/137 [00:00<00:00, 162.14it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 Loss: 0.6150 Training accuracy: 0.8513 Validation accuracy: 0.8371\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137/137 [00:00<00:00, 181.95it/s]\n","100%|██████████| 16/16 [00:00<00:00, 216.11it/s]\n"," 13%|█▎        | 18/137 [00:00<00:00, 176.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 3 Loss: 0.4914 Training accuracy: 0.8833 Validation accuracy: 0.8513\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137/137 [00:00<00:00, 181.34it/s]\n","100%|██████████| 16/16 [00:00<00:00, 206.40it/s]\n"," 13%|█▎        | 18/137 [00:00<00:00, 178.13it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 4 Loss: 0.4079 Training accuracy: 0.9007 Validation accuracy: 0.8615\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137/137 [00:00<00:00, 180.44it/s]\n","100%|██████████| 16/16 [00:00<00:00, 220.06it/s]\n"," 13%|█▎        | 18/137 [00:00<00:00, 171.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 5 Loss: 0.3458 Training accuracy: 0.9162 Validation accuracy: 0.8717\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137/137 [00:00<00:00, 180.85it/s]\n","100%|██████████| 16/16 [00:00<00:00, 210.66it/s]\n"," 14%|█▍        | 19/137 [00:00<00:00, 180.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 6 Loss: 0.2995 Training accuracy: 0.9281 Validation accuracy: 0.8880\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137/137 [00:00<00:00, 183.18it/s]\n","100%|██████████| 16/16 [00:00<00:00, 215.46it/s]\n","100%|██████████| 14/14 [00:00<00:00, 238.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 7 Loss: 0.2625 Training accuracy: 0.9381 Validation accuracy: 0.8961\n","Test accuracy: 0.9129\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Te35cWGOJlf9"},"source":["## Part 4: Multilayer perceptron\n","\n","### Review of multilayer perceptrons\n","\n","<img src=\"https://github.com/nlp-236299/data/raw/master/Resources/multilayer-perceptron-figure.png\" alt=\"multilayer perceptron illustration\" width=\"400\"  align=right />\n","\n","In the last part, you implemented a perceptron, a model that involved a linear calculation (the sum of weights) followed by a nonlinear calculation (the softmax, which converts the summed weight values to probabilities). In a multi-layer perceptron, we take the output of the first perceptron to be the input of a second perceptron (and of course, we could continue on with a third or even more).\n","\n","In this part, you'll implement the forward calculation of a two-layer perceptron, again letting PyTorch handle the backward calculation as well as the optimization of parameters. The first layer will involve a linear summation as before and a **sigmoid** as the nonlinear function. The second will involve a linear summation and a softmax (the latter absorbed, as before, into the loss function). Thus, the difference from the logistic regression implementation is simply the adding of the sigmoid and second linear calculations. See the figure for the structure of the computation. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"FsBnCFe0CnUv"},"source":["### Implement a multilayer perceptron classifier\n","\n","For the implementation, we ask you to implement a two layer perceptron classifier, again as a subclass of the [`torch.nn` module](https://pytorch.org/docs/stable/nn.html). You might reuse quite a lot of the code from logistic regression. As before, you will be adding the following two methods:\n","\n","1. `__init__`: An initializer that takes two `torchtext` fields providing descriptions of the text and label aspects of examples, and `hidden_size` specifying the size of the hidden layer (e.g., in the above illustration, `hidden_size` is `D`).\n","\n","    During initialization, you'll want to define two tensors of weights, which serve as the parameters of this model, one for each layer. You'll want to [initialize them randomly](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_). \n","    \n","    The weights in the first layer are a kind of lookup (as in the previous part), mapping words to a vector of size `hidden_size`. The [`nn.Embedding` module](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) is a good way to set up and make use of this weight tensor.\n","    \n","    The weights in the second layer define a linear mapping from vectors of size `hidden_size` to vectors of size `num_labels`. The [`nn.Linear` module](https://pytorch.org/docs/master/generated/torch.nn.Linear.html) or [`torch.mm`](https://pytorch.org/docs/master/generated/torch.mm.html) for matrix multiplication may be helpful here.\n","\n","2. `forward`: Given a text batch of size `batch_size X max_length`, the `forward` function returns a tensor of logits of size `batch_size X num_labels`. \n","\n","    That is, for each text $\\vect{x}$ in the batch and each label $c$, you'll be calculating $MLP(bow(\\vect{x}))$ as shown in the illustration above, returning a tensor of these values. Note that the softmax operation is absorbed into [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) so you don't need to worry about that.\n","    \n","    For the sigmoid sublayer, you might find [`nn.Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) useful.\n","\n","You should expect to achieve at least **90%** accuracy on the ATIS classificiation task. "]},{"cell_type":"code","metadata":{"id":"I5TVcZ879gI8","executionInfo":{"status":"ok","timestamp":1607121415381,"user_tz":-120,"elapsed":1070,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}}},"source":["class MultiLayerPerceptron(nn.Module):\n","  def __init__ (self, text, label, hidden_size=128): \n","    super().__init__ ()\n","    self.text = text\n","    self.label = label\n","    self.padding_id = text.vocab.stoi[text.pad_token]\n","    self.hidden_size = hidden_size\n","    # Keep the vocabulary sizes available\n","    self.N = len(label.vocab.itos) # num_classes\n","    self.V = len(text.vocab.itos)  # vocab_size\n","    # Specify cross-entropy loss for optimization\n","    self.criterion = nn.CrossEntropyLoss()\n","    # TODO: Create and initialize neural modules\n","    W1 = torch.rand((self.V, self.hidden_size), requires_grad=True)\n","    param1 = torch.nn.Parameter(W1)\n","    self.register_parameter(name='W1', param=param1)\n","    W2 = torch.rand((self.hidden_size, self.N), requires_grad=True)\n","    param2 = torch.nn.Parameter(W2)\n","    self.register_parameter(name='W2', param=param2)\n","\n","  def forward(self, text_batch):\n","    # TODO: Calculate the logits for the `text_batch`, \n","    #       returning a tensor of size batch_size x num_labels\n","    batch_size, _ = text_batch.shape\n","    bows_batch = torch.zeros(batch_size, self.V).long()\n","\n","    for ex_ind, ex in enumerate(text_batch):\n","        counts = Counter(ex)\n","        bows_indices, bows_counts  = list(counts.keys()), list(counts.values())\n","        bows_batch[ex_ind, bows_indices] = torch.Tensor(bows_counts).long()\n","    activation_value = torch.nn.Sigmoid()(bows_batch.float()@self.W1)\n","    logits=activation_value@self.W2\n","\n","    return logits\n","    \n","    \n","  \n","  def train_all(self, train_iter, val_iter, epochs=8, learning_rate=3e-3):\n","    # Switch the module to training mode\n","    self.train()\n","    # Use Adam to optimize the parameters\n","    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n","    best_validation_accuracy = -float('inf')\n","    best_model = None\n","    # Run the optimization for multiple epochs\n","    for epoch in range(epochs):\n","      c_num = 0\n","      total = 0\n","      running_loss = 0.0\n","      for batch in tqdm(train_iter):\n","        # Zero the parameter gradients\n","        optim.zero_grad()\n","\n","        # Input and target\n","        text = batch.text           # a tensor of shape (bsz, max_len)\n","        logits = self.forward(text) # perform the forward computation\n","        target = batch.label.long() # bsz\n","        batch_size = len(target)\n","\n","        # Compute the loss\n","        loss = self.criterion(logits, target)\n","\n","        # Perform backpropagation\n","        loss.backward()\n","        optim.step()\n","\n","        # Prepare to compute the accuracy\n","        predictions = torch.argmax(logits, dim=1)\n","        total += batch_size\n","        c_num += (predictions == target).float().sum().item()        \n","        running_loss += loss.item() * batch_size\n","\n","      # Evaluate and track improvements on the validation dataset\n","      validation_accuracy = self.evaluate(val_iter)\n","      if validation_accuracy > best_validation_accuracy:\n","        best_validation_accuracy = validation_accuracy\n","        self.best_model = copy.deepcopy(self.state_dict())\n","      epoch_loss = running_loss / total\n","      epoch_acc = c_num / total\n","      print (f'Epoch: {epoch} Loss: {epoch_loss:.4f} '\n","             f'Training accuracy: {epoch_acc:.4f} '\n","             f'Validation accuracy: {validation_accuracy:.4f}')\n","\n","  def evaluate(self, iterator):\n","    self.eval()   # switch the module to evaluation mode\n","    total = 0     # running total of example\n","    c_num = 0     # running total of correctly classified examples\n","    for batch in tqdm(iterator):\n","      text = batch.text\n","      logits = self.forward(text)                 # calculate forward probabilities\n","      target = batch.label.long()                 # extract gold labels\n","      predictions = torch.argmax(logits, dim=-1)  # calculate predicted labels\n","      total += len(target)\n","      c_num += (predictions == target).float().sum().item()\n","    return c_num / total"],"execution_count":169,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4Bs0f7Hv1z4","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1607121425539,"user_tz":-120,"elapsed":8607,"user":{"displayName":"Ilya Kotlov","photoUrl":"","userId":"00819922945080322606"}},"outputId":"85ed855f-75ff-4123-ddbf-667d7dbc5e88"},"source":["# Instantiate classifier and run it\n","model = MultiLayerPerceptron(TEXT, LABEL).to(device) \n","model.train_all(train_iter, val_iter)\n","model.load_state_dict(model.best_model)\n","test_accuracy = model.evaluate(test_iter)\n","print (f'Test accuracy: {test_accuracy:.4f}')"],"execution_count":170,"outputs":[{"output_type":"stream","text":["  0%|          | 0/137 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","100%|██████████| 137/137 [00:00<00:00, 151.00it/s]\n","100%|██████████| 16/16 [00:00<00:00, 195.74it/s]\n"," 10%|█         | 14/137 [00:00<00:00, 139.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0 Loss: 1.3073 Training accuracy: 0.7120 Validation accuracy: 0.6986\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137/137 [00:00<00:00, 160.61it/s]\n","100%|██████████| 16/16 [00:00<00:00, 208.89it/s]\n"," 12%|█▏        | 17/137 [00:00<00:00, 162.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 Loss: 0.8663 Training accuracy: 0.7769 Validation accuracy: 0.7963\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137/137 [00:00<00:00, 163.90it/s]\n","100%|██████████| 16/16 [00:00<00:00, 175.88it/s]\n"," 11%|█         | 15/137 [00:00<00:00, 148.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2 Loss: 0.4877 Training accuracy: 0.8776 Validation accuracy: 0.8839\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137/137 [00:00<00:00, 158.10it/s]\n","100%|██████████| 16/16 [00:00<00:00, 198.51it/s]\n"," 10%|█         | 14/137 [00:00<00:00, 135.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 3 Loss: 0.3075 Training accuracy: 0.9187 Validation accuracy: 0.9084\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137/137 [00:00<00:00, 160.66it/s]\n","100%|██████████| 16/16 [00:00<00:00, 212.31it/s]\n"," 12%|█▏        | 16/137 [00:00<00:00, 158.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 4 Loss: 0.2163 Training accuracy: 0.9466 Validation accuracy: 0.9226\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137/137 [00:00<00:00, 158.75it/s]\n","100%|██████████| 16/16 [00:00<00:00, 206.82it/s]\n"," 12%|█▏        | 16/137 [00:00<00:00, 155.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 5 Loss: 0.1603 Training accuracy: 0.9623 Validation accuracy: 0.9328\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137/137 [00:00<00:00, 162.01it/s]\n","100%|██████████| 16/16 [00:00<00:00, 172.59it/s]\n"," 11%|█         | 15/137 [00:00<00:00, 144.07it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 6 Loss: 0.1250 Training accuracy: 0.9719 Validation accuracy: 0.9348\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137/137 [00:00<00:00, 162.11it/s]\n","100%|██████████| 16/16 [00:00<00:00, 206.10it/s]\n","100%|██████████| 14/14 [00:00<00:00, 161.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 7 Loss: 0.1000 Training accuracy: 0.9767 Validation accuracy: 0.9430\n","Test accuracy: 0.9397\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"8lo119NGqEZV"},"source":["<!-- BEGIN QUESTION -->\n","\n","## Lesson learned\n","\n","Take a look at some of the examples that were classified correctly and incorrectly by your best method.\n","\n","**Question:** Do you notice anything about the incorrectly classified examples that might indicate _why_ they were classified incorrectly?\n","\n","<!--\n","BEGIN QUESTION\n","name: open_response_lessons\n","manual: true\n","-->"]},{"cell_type":"markdown","metadata":{"id":"vVmV0I_oqEZV"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"OktPO1mQqEZV"},"source":["<!-- END QUESTION -->\n","\n","<!-- BEGIN QUESTION -->\n","\n","## Debrief\n","\n","**Question:** We're interested in any thoughts you have about this project segment so that we can improve it for later years, and to inform later segments for this year. Please list any issues that arose or comments you have to improve the project segment. Useful things to comment on include the following: \n","\n","* Was the project segment clear or unclear? Which portions?\n","* Were the readings appropriate background for the project segment? \n","* Are there additions or changes you think would make the project segment better?\n","\n","<!--\n","BEGIN QUESTION\n","name: open_response_debrief\n","manual: true\n","-->"]},{"cell_type":"markdown","metadata":{"id":"Z8Taz2dfqEZV"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","metadata":{"id":"VGLxXlAaqEZV"},"source":["<!-- END QUESTION -->\n","\n","\n","\n","## Instructions for submission of the project segment\n","\n","This project segment should be submitted to Gradescope, which will be made available some time before the due date.\n","\n","Project segment notebooks are manually graded, not autograded using otter as labs are. (Otter is used within project segment notebooks to synchronize distribution and solution code however.) **We will not run your notebook before grading it.** Instead, we ask that you submit the already freshly run notebook. The best method is to \"restart kernel and run all cells\", allowing time for all cells to be run to completion.\n","\n","We also request that you **submit a PDF of the freshly run notebook**. The simplest method is to use \"Export notebook to PDF\", which will render the notebook to PDF via LaTeX. If that doesn't work, the method that seems to be most reliable is to export the notebook as HTML (if you are using Jupyter Notebook, you can do so using `File -> Print Preview`), open the HTML in a browser, and print it to a file. Then make sure to add the file to your git commit. Please name the file the same name as this notebook, but with a `.pdf` extension. (Conveniently, the methods just described will use that name by default.) You can then perform a git commit and push and submit the commit to Gradescope."]},{"cell_type":"markdown","metadata":{"id":"p7UkAap1qEZV"},"source":["# End of project segment 1"]}]}